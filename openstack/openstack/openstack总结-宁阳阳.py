root
596e1a60d94bb5c33f3e894c
admin
4d39d60402687927d789


dzc-op-storm02v	 192.168.33.13  sjhl-o-compute05
dzc-op-storm03v  192.168.33.14  sjhl-o-compute06	
dzc-op-storm04v  192.168.34.86  sjhl-o-compute06	
dzc-op-storm05v  192.168.34.87  sjhl-o-compute04	
dzc-op-storm06v	 192.168.33.15  sjhl-o-compute09
dzc-op-storm07v  192.168.33.16  sjhl-o-compute10
dzc-op-storm08v  192.168.33.17  sjhl-o-compute07
dzc-op-storm10v  192.168.33.20  sjhl-o-compute07

192.168.33.20

用户名ub_group 密码wuguo吗
220.181.149.223 
useradd  ub_group 
echo wuguo | passwd  ub_group --stdin 




dzc-op-storm02v	 192.168.33.13  sjhl-o-compute05
dzc-op-storm03v  192.168.33.14  sjhl-o-compute06	
dzc-op-storm04v  192.168.34.86  sjhl-o-compute06	
dzc-op-storm05v  192.168.34.87  sjhl-o-compute04	
dzc-op-storm06v	 192.168.33.15  sjhl-o-compute09
dzc-op-storm07v  192.168.33.16  sjhl-o-compute10
dzc-op-storm08v  192.168.33.17  sjhl-o-compute07
dzc-op-storm10v  192.168.33.20  sjhl-o-compute07

ssh  -p33777 webuser@192.168.33.14
                  
wget http://install.light.fang.com/zabbix_client.sh   
sed -i  's/jyw/dzc/g' zabbix_client.sh  
sh  zabbix_client.sh 
sed -i  's/zabbix01.light.fang.com/192.168.7.43/g'  /usr/local/zabbix/etc/zabbix_agentd.conf  
/etc/init.d/zabbix_agentd restart 


###
同宿主机同段         服务器板卡                           >wM
不同宿主机同段       去交换机过一下 还是二层              =wM 
同宿主机不同段       去交换机过一下 还是二层              =wM 
不同宿主机不同段      在三层交换机上走路由条目            <wM 


10.20.8.31	         220.181.149.153  dzc-o-control01v
10.20.8.32	         220.181.149.154  dzc-o-control02
10.20.8.33	         192.168.32.6     dzc-o-neutron01v
10.20.8.34	                          dzc-o-neutron02
10.20.8.37           192.168.32.9     dzc-o-compute01   centos7.1
10.20.8.36           192.168.32.8     dzc-o-compute02   centos7.1
10.20.8.38	                          sjhl-o-compute03
10.20.8.39                            sjhl-o-compute04
10.20.8.40                            sjhl-o-compute05
10.20.8.41                            sjhl-o-compute06
10.20.8.42                            sjhl-o-compute07
10.20.8.43                            sjhl-o-compute08
10.20.8.44                            sjhl-o-compute09
10.20.8.45                            sjhl-o-compute10
                                      

jyw-o-control01	    106.39.78.54	192.168.4.192	100G	32G	24	控制节点		80,443	主
jyw-o-network01	    106.39.78.53	192.168.4.191	300G	8G	8	网络节点		80,443	主
jyw-o-cinder01	    106.39.78.52	192.168.4.190	300G	32G	24	弹性块节点		80,443	

106.39.78.54
106.39.78.52


          (sjhl-o-compute07, sjhl-o-compute07) ram:61328 disk:394240 io_ops:0 instances:3,
          (sjhl-o-compute05, sjhl-o-compute05) ram:78992 disk:160768 io_ops:0 instances:5, 
		    (dzc-o-compute01, dzc-o-compute01) ram:31626 disk:229376 io_ops:0 instances:0, 
		  (sjhl-o-compute03, sjhl-o-compute03) ram:78224 disk:373760 io_ops:0 instances:4, 
		  (sjhl-o-compute09, sjhl-o-compute09) ram:70800 disk:205824 io_ops:0 instances:5, 
		  (sjhl-o-compute10, sjhl-o-compute10) ram:70032 disk:304128 io_ops:0 instances:4, 
		  (sjhl-o-compute04, sjhl-o-compute04) ram:70032 disk:304128 io_ops:0 instances:4, 
		  (sjhl-o-compute06, sjhl-o-compute06) ram:78736 disk:299008 io_ops:5 instances:18]


1、网桥和桥接
简单来说，桥接就是把一台机器上的若干个网络接口“连接”起来。其结果是，其中一个网口收到的报文会被复制给其他网口并发送出去。以使得网口之间的报文能够互相转发。
交换机就是这样一个设备，它有若干个网口，并且这些网口是桥接起来的。于是，与交换机相连的若干主机就能够通过交换机的报文转发而互相通信。
实际上，我们可以把逻辑网段192.168.1.0/24 看作使一个VLAN ，而br0 则是这个VLAN 的名称。
建立一个逻辑网段之后，我们还需要为这个网段分配特定的端口。在Linux 中，一个端口实际上就是一个物理网卡。而每个物理网卡的名称则分别为eth0 ，eth1 ，eth2 ，eth3 。我们需要把每个网卡一一和br0 这个网段联系起来，作为br0 中的一个端口。
网桥的每个物理网卡作为一个端口，运行于混杂模式，而且是在链路层工作，所以就不需要IP了。


brctl addbr br0
brctl stp br0 off
brctl addif br0 eth0
brctl addif br0 eth1
ifconfig eth0 down
ifconfig eth1 down
ifconfig eth0 0.0.0.0 up
ifconfig eth1 0.0.0.0 up
ifconfig br0 10.0.3.129 up
brctl delif ena eth1;
brctl delif ena eth0;
ifconfig ena down;
brctl delbr ena;

2、qemu kvm  libvirt
qemu
   是一个模拟器，它向guestos模拟cpu和其他硬件，Guest OS认为自己和硬件直接打交道，其实是同Qemu模拟出来的硬件打交道，Qemu将这些指令转译给真正的硬件。
   于所有的指令都要从Qemu里面过一手，因而性能较差。
kvm
    是linux内核的模块它需要CPU的支持，采用硬件辅助虚拟化技术Intel-VT，AMD-V，内存的相关如Intel的EPT和AMD的RVI技术，Guest OS的CPU指令不用再经过Qemu转译，直接运行，大大提高了速度，KVM通过/dev/kvm暴露接口，用户态程序可以通过ioctl函数来访问这个接口
    KVM内核模块本身只能提供CPU和内存的虚拟化，所以他必须整合qemu才能构成完整的虚拟化技术
qemu-kvm
  Qemu将KVM整合进来，kvm负责cpu和内存的虚拟化，但kvm不能模拟其它设备，qemu模拟IO设备 网卡 和磁盘等，
  kvm加上qemu就能实现真正意义上的虚拟化
libvirt 
   是目前使用最广泛的对KVM虚拟机进行管理的工具和APILibvirtd是一个daemon进程，可以被本地的virsh调用，也可以被远程的virsh调用，Libvirtd调用qemu-kvm操作虚拟机。   
    virsh  virsh-viewer  vitt  
	
3 、Open vSwitch是一个高质量的、多层虚拟交换机
它被设计位支持跨越多个物理服务器的分布式环境
跟传统的物理交换机相比，虚拟交换机同要具备很多有点：
  1.配置灵活，因为是软件实现的，一台物理服务器上可以配置数十太或者数百台虚拟交换机，而且端口数目可以灵活选择  
  2. 成本低廉，通过软件的方式可轻易达到10Gbps的交换速度。
  二、虚拟网络 
浅显的理解就是 使用虚拟交换机组成的网络，就是虚拟网络(和物理网络相比) ，如图所示： 绿色虚线内组成的就是一个虚拟网络了。其虚拟机之间的信息交换都通过虚拟交换机。

OVS组件：
1. ovsdb-sever:  OVS的数据库服务器，用来存储虚拟交换机的配置信息。它于manager和ovs-vswitchd交换信息使用了OVSDB(JSON-RPC)的方式。
2. ovs-vswitchd: OVS的核心部件，它和上层controller通信遵从OPENFLOW协议，它与ovsdb-server通信使用OVSDB协议，它和内核模块通过netlink通信，它支持多个独立的datapath（网桥），它通过更改flow table实现了绑定，和VLAN等功能。
3. ovs kernel module: OVS的内核模块，处理包交换和隧道，缓存flow，如果在内核的缓存中找到转发规则则转发，否则发向用户空间去处理。



openstack 网络

外部网络
公共网络，外部或Internet可以访问的网络
内部网络
私有网络，仅内部访问的网络
管理网络，用于openstack组件以及mysql DB Server，RabbitMQ messaging server之间的通信
存储网络，用于storage/volume节点与计算节点间的iSCSI volume traffic
服务网络，用于租户VLAN/subnets中的实例的固定IP地址


neutron  提供了对网络，子网，路由的模拟
neutron 是用来创建虚拟网络的，所谓虚拟网络，就是虚拟机启动时会有一块虚拟网卡
虚拟网卡会连接到虚拟switch 虚拟交换机会连接到虚拟路由上，虚拟路由器最终和物理网卡连通

vnic-->vswitch-->vrouter-->nic-->switch-->router
neutron 分布在控制节点，计算节点和网络节点上
1、控制节点:
  neutron-server 用户接受API请求创建网络，子网，路由器等，然而创建的这些东西仅仅是一些数据库中的数据结构
2、网络节点：
  neutron-l3-agent:用于创建和管理虚拟路由器的，当neutron将路由器的数据创建好时，neutron-l3-agent是做具体事情的，真正的调用命令行将虚拟路由器，路由表，namespace，iptables规则全部创建好。
  neutron-dhcp-agent:用于创建和管理DHCPserver,每一个网络都会有一个dhcpserver
  nobody   19367     1  0 Dec15 ?        00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=tapdb21d3d0-99 --except-interface=lo --pid-file=/var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/host --addn-hosts=/var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/opts --leasefile-ro --dhcp-authoritative --dhcp-range=set:tag0,192.168.34.0,static,86400s --dhcp-lease-max=256 --conf-file=/etc/neutron/dnsmasq-neutron.conf --domain=openstacklocal
  nobody   19369     1  0 Dec15 ?        00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=tap905ede5c-4d --except-interface=lo --pid-file=/var/lib/neutron/dhcp/ba31704e-c1b6-4a43-8eb6-6162d25e7b29/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/ba31704e-c1b6-4a43-8eb6-6162d25e7b29/host --addn-hosts=/var/lib/neutron/dhcp/ba31704e-c1b6-4a43-8eb6-6162d25e7b29/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/ba31704e-c1b6-4a43-8eb6-6162d25e7b29/opts --leasefile-ro --dhcp-authoritative --dhcp-range=set:tag0,192.168.33.0,static,86400s --dhcp-lease-max=256 --conf-file=/etc/neutron/dnsmasq-neutron.conf --domain=openstacklocal
/var/lib/neutron/dhcp/ba31704e-c1b6-4a43-8eb6-6162d25e7b29
-rw-r--r-- 1 neutron neutron 865 Dec 19 12:54 addn_hosts
-rw-r--r-- 1 neutron neutron 854 Dec 19 12:54 host
-rw-r--r-- 1 neutron neutron  14 Dec 15 19:04 interface
-rw-r--r-- 1 neutron neutron  86 Dec 19 12:54 opts
-rw-r--r-- 1 root    root      6 Dec 15 19:04 pid
[root@dzc-o-neutron01v d647355e-b019-4de0-999e-00107531edc0]# cat addn_hosts 
[root@dzc-o-neutron01v d647355e-b019-4de0-999e-00107531edc0]# cat addn_hosts 
192.168.34.14	host-192-168-34-14.openstacklocal host-192-168-34-14
192.168.34.5	host-192-168-34-5.openstacklocal host-192-168-34-5
192.168.34.11	host-192-168-34-11.openstacklocal host-192-168-34-11
192.168.34.16	host-192-168-34-16.openstacklocal host-192-168-34-16
192.168.34.19	host-192-168-34-19.openstacklocal host-192-168-34-19
192.168.34.12	host-192-168-34-12.openstacklocal host-192-168-34-12
192.168.34.15	host-192-168-34-15.openstacklocal host-192-168-34-15
192.168.34.3	host-192-168-34-3.openstacklocal host-192-168-34-3
192.168.34.51	host-192-168-34-51.openstacklocal host-192-168-34-51
192.168.34.54	host-192-168-34-54.openstacklocal host-192-168-34-54
192.168.34.57	host-192-168-34-57.openstacklocal host-192-168-34-57
192.168.34.86	host-192-168-34-86.openstacklocal host-192-168-34-86
192.168.34.87	host-192-168-34-87.openstacklocal host-192-168-34-87

neutron-openvswitch-plugin-agent,这个是用于创建L2的switch的，在Network节点上，Router和DHCP Server都会连接到二层的switch上。

3、compute节点：
neutron-openstackvswitch-plugin-agent，这个是用于创建L2层switch的，在compute节点上，虚拟机的网卡也是连接到二层的switch上。

############################################################
 1) TAP设备:TAP设备其实就是一个Linux内核虚拟化出来的一个网络接口
 2) qbr设备:是一个Linux Bridge
 3) OpenvSwitch不支持现在的OpenStack的实现方式，因为OpenStack是把iptables规则丢在TAP设备中，以此实现了安全组功能。没办法，
所以用了一个折衷的方式，在中间加一层，用Linux Bridge来实现吧，这样，就莫名其妙了多了一个qbr网桥。在qbr上面还存在另一个设备C，这也是一个TAP设备。C通常以qvb开头，C和br-int上的D连接在一起形成一个连接通道，使得qbr和br-int之间顺畅通信。
 4) br-int br-int是由OpenvSwitch虚拟化出来的网桥，但事实上它已经充当了一个虚拟交换机的功能了
 br-int的主要职责就是把它所在的计算节点的VM都连接到这个虚拟交换机上面
 5) br-tun 实现了不同的计算节点上的VM连接在同一个逻辑虚拟交换机上面的功能
    br-tun 只是用来充当一个通道层
 6) br-int openstack 大一体化网桥openstack环境中所有虚拟机都连接到了一个巨型的虚拟交换机上。
 7) 然而毕竟br-int 被物理的隔开了,需要有一种方式将他们串联起来，
     可以用GRE tunnel将不同机器的br-int连接起来，也可以通过VLAN将br-int连接起来，当然还可以使用vxlan。
 8) namespace：用来实现隔离的一套机制，不同 namespace 中的资源之间彼此不可见。
 由于 Neutron 的 legacy router 只会部署在网络节点上，因此会造成网络节点的流量过大，从而产生了两个问题，其一是网络节点将成为整个 Neutron 网络的瓶颈，其二是网络节点单点失败的问题。在这样的背景下，OpenStack 社区在 Juno 版本里正式引入了 DVR(Distributed Virtual Router)。DVR，顾名思义就是 Neutron 的 router 将不单单部署在网络节点上，所有启动了 Neutron L3 Agent 的节点，都会在必要时在节点上创建 Neutron router 对应的 namepsace，并更新与 DVR router 相关的 Openflow 规则，从而完成 DVR router 在该节点上的部署。在计算节点上部署了 DVR router 后，E-W 方向上的流量不再需要将数据包发送到网络节点后再转发，而是有本地的 DVR router 直接进行跨子网的转发；N-S 方向上，对于绑定了 floating IP 的虚机，其与外网通信时的数据包也将直接通过本地的 DVR router 进行转发。从而，Neutron 网络上的一些流量被分摊开，有效地减少了网络节点上的流量；通信不再必须通过网络节点，也提升了 Neutron 网络的抗单点失败的能力。 
 9) namespace可以理解为进程级别的网络设备

ip netns 
[root@dzc-o-neutron01v ~]# ip netns
qdhcp-d647355e-b019-4de0-999e-00107531edc0
qdhcp-ba31704e-c1b6-4a43-8eb6-6162d25e7b29
 
ip netns exec    qdhcp-d647355e-b019-4de0-999e-00107531edc0

[root@dzc-o-neutron01v ~]# ifconfig 
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 0  (Local Loopback)
        RX packets 229  bytes 68504 (66.8 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 229  bytes 68504 (66.8 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

tapdb21d3d0-99: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.34.3  netmask 255.255.255.0  broadcast 192.168.34.255
        inet6 fe80::f816:3eff:fe96:f0bf  prefixlen 64  scopeid 0x20<link>
        ether fa:16:3e:96:f0:bf  txqueuelen 0  (Ethernet)
        RX packets 697859  bytes 42818928 (40.8 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 1638  bytes 153964 (150.3 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

tcpdum -i  tapdb21d3d0-99   |grep 34.89
 
19:08:52.093496 IP dzc-o-neutron01v.bootps > 192.168.34.89.bootpc: BOOTP/DHCP, Reply, length 336
19:08:52.093614 IP dzc-o-neutron01v.bootps > 192.168.34.89.bootpc: BOOTP/DHCP, Reply, length 336
19:08:52.094277 IP dzc-o-neutron01v.bootps > 192.168.34.89.bootpc: BOOTP/DHCP, Reply, length 336
19:08:52.094386 IP dzc-o-neutron01v.bootps > 192.168.34.89.bootpc: BOOTP/DHCP, Reply, length 336
19:08:52.136237 ARP, Request who-has 192.168.34.89 (Broadcast) tell 0.0.0.0, length 46
19:08:52.136255 ARP, Request who-has 192.168.34.89 (Broadcast) tell 0.0.0.0, length 46
19:08:53.136098 ARP, Request who-has 192.168.34.89 (Broadcast) tell 0.0.0.0, length 46
19:08:53.136099 ARP, Request who-has 192.168.34.89 (Broadcast) tell 0.0.0.0, length 46
19:08:57.107470 ARP, Request who-has 192.168.34.89 tell dzc-o-neutron01v, length 28
19:08:57.108220 ARP, Reply 192.168.34.89 is-at fa:16:3e:a5:59:69 (oui Unknown), length 46


    Bridge br-int
        fail_mode: secure
		#两个qdhcp netns
        Port "tap905ede5c-4d"
            tag: 4
            Interface "tap905ede5c-4d"
                type: internal
        Port "tapdb21d3d0-99"
            tag: 3
            Interface "tapdb21d3d0-99"
                type: internal
        Port br-int
            Interface br-int
                type: internal
        Port "int-br-eth0"
            Interface "int-br-eth0"
                type: patch
                options: {peer="phy-br-eth0"}
        Port int-br-ex
            Interface int-br-ex
                type: patch
                options: {peer=phy-br-ex}
Neutron Tenant网络是为tenant中的虚机之间的通信。如果需要不同tenant内的虚机之间通信，需要在两个subnet之间增加Neutron路由。
路由是用来区别不同的tenant ？
ovs-vsctl： 查询和更新ovs-vswitchd的配置
ovs-ofctl： 查询和控制OpenFlow交换机和控制器

 <interface type="bridge">
      <mac address="fa:16:3e:8d:58:cd"/>
      <model type="virtio"/>
      <source bridge="qbr4faecb8e-f1"/>   虚机TAP设备所挂接的linux bridge
      <target dev="tap4faecb8e-f1"/>       虚机所连接的interface 
    </interface>

	    <interface type="bridge">
      <mac address="fa:16:3e:f6:68:4b"/>
      <model type="virtio"/>
      <source bridge="qbrf2066968-ba"/>
      <target dev="tapf2066968-ba"/>
    </interface>
[root@sjhl-o-compute07 ~]# iptables  -S |grep tapf2066968-ba
-A neutron-openvswi-FORWARD -m physdev --physdev-out tapf2066968-ba --physdev-is-bridged -j neutron-openvswi-sg-chain
-A neutron-openvswi-FORWARD -m physdev --physdev-in tapf2066968-ba --physdev-is-bridged -j neutron-openvswi-sg-chain
-A neutron-openvswi-INPUT -m physdev --physdev-in tapf2066968-ba --physdev-is-bridged -j neutron-openvswi-of2066968-b
-A neutron-openvswi-sg-chain -m physdev --physdev-out tapf2066968-ba --physdev-is-bridged -j neutron-openvswi-if2066968-b
-A neutron-openvswi-sg-chain -m physdev --physdev-in tapf2066968-ba --physdev-is-bridged -j neutron-openvswi-of2066968-b

[root@sjhl-o-compute07 ~]# iptables  -S |grep  if2066968-b
-N neutron-openvswi-if2066968-b
-A neutron-openvswi-if2066968-b -m state --state INVALID -j DROP
-A neutron-openvswi-if2066968-b -m state --state RELATED,ESTABLISHED -j RETURN
-A neutron-openvswi-if2066968-b -s 192.168.33.4/32 -p udp -m udp --sport 67 --dport 68 -j RETURN
-A neutron-openvswi-if2066968-b -p icmp -j RETURN
-A neutron-openvswi-if2066968-b -p tcp -m tcp -m multiport --dports 1:65535 -j RETURN
-A neutron-openvswi-if2066968-b -p udp -m udp -m multiport --dports 1:65535 -j RETURN
-A neutron-openvswi-if2066968-b -m set --match-set IPv4edfca931-a45a-4f22-9 src -j RETURN
-A neutron-openvswi-if2066968-b -j neutron-openvswi-sg-fallback
-A neutron-openvswi-sg-chain -m physdev --physdev-out tapf2066968-ba --physdev-is-bridged -j neutron-openvswi-if2066968-b
You have mail in /var/spool/mail/root
[root@sjhl-o-compute07 ~]# iptables  -S |grep  of2066968-b
-N neutron-openvswi-of2066968-b
-A neutron-openvswi-INPUT -m physdev --physdev-in tapf2066968-ba --physdev-is-bridged -j neutron-openvswi-of2066968-b
-A neutron-openvswi-of2066968-b -p udp -m udp --sport 68 --dport 67 -j RETURN
-A neutron-openvswi-of2066968-b -j neutron-openvswi-sf2066968-b
-A neutron-openvswi-of2066968-b -p udp -m udp --sport 67 --dport 68 -j DROP
-A neutron-openvswi-of2066968-b -m state --state INVALID -j DROP
-A neutron-openvswi-of2066968-b -m state --state RELATED,ESTABLISHED -j RETURN
-A neutron-openvswi-of2066968-b -p tcp -m tcp -m multiport --dports 1:65535 -j RETURN
-A neutron-openvswi-of2066968-b -j RETURN
-A neutron-openvswi-of2066968-b -p icmp -j RETURN
-A neutron-openvswi-of2066968-b -p udp -m udp -m multiport --dports 1:65535 -j RETURN
-A neutron-openvswi-of2066968-b -j neutron-openvswi-sg-fallback
-A neutron-openvswi-sg-chain -m physdev --physdev-in tapf2066968-ba --physdev-is-bridged -j neutron-openvswi-of2066968-b

############################################################  
[root@sjhl-o-compute07 neutron]# ovs-ofctl dump-flows  br-int
NXST_FLOW reply (xid=0x4):
 cookie=0x0, duration=587078.598s, table=0, n_packets=620971047, n_bytes=124787946068, idle_age=0, hard_age=65534, priority=1 actions=NORMAL
 cookie=0x0, duration=587065.223s, table=0, n_packets=753088134, n_bytes=167590415416, idle_age=0, hard_age=65534, priority=3,in_port=2,dl_vlan=501 actions=mod_vlan_vid:1,NORMAL
 cookie=0x0, duration=587077.628s, table=0, n_packets=41508937, n_bytes=69901695294, idle_age=0, hard_age=65534, priority=2,in_port=2 actions=drop
 cookie=0x0, duration=587078.536s, table=23, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=0 actions=drop
dl_vlan=501外部vlan  mod_vlan_vid 内部vlan
############################################################
openstack 中的存储
1. 块存储                                                    
     给虚拟机挂载使用,向虚拟机提供额外的磁盘空间
2. 对象存储
     向虚拟机提供备份、归档、数据保存空间、存放虚拟机镜像
三个主要组成部分
–cinder-api 组件负责向外提供Cinder REST API
–cinder-scheduler 组件负责分配存储资源
–cinder-volume 组件负责封装driver，不同的driver负责控制不同的后端存储
############################################################ 
1、块存储读写快，不利于共享，文件存储读写慢，利于共享
对象存储平衡了上面的缺点
2、首先一个文件包含了属性和内容
属性--> 元数据-->metadata-->文件的大小，修改时间，存储路径等
内容-->数据 --> data 
3、像FAT32这种文件系统，是直接将一份文件的data和metadata一起存储的
存储过程为先将文件按照文件系统的最小块来打散 （例如4M的文件，假设文件系统
要求一个块4K，那么就将文件打散为1000个小块）再写入硬盘里，过程中没有区分
data和metadata，而每个块最后会告诉下一个要读取的块的地址，然后一直这样顺序的
按图索骥，最后完成整份文件的所有块的读取。
这种情况下读写速率很慢，因为就算你有100个机械臂在读写，但是由于你只有
读取到第一个块，才知道下一个块在哪里，其实相当于只有一个1机械臂在读取
4、而对象存储将metadata独立了出来，控制节点叫做元数据服务器（服务器+对象存储管理软件）
主要负责存储对象的属性，对象的数据被打散存储在那几台分布式服务器中，
其它几台负责存储数据的分布式服务器叫做OSD。当用户访问对象时，会先访问元数据服务器，
元数据服务器只负责反馈对象存储在哪些OSD，假设反馈文件A存储在B、C、D这三台OSD上
那么用户就会再次访问3台OSD服务器去读取数据
这时候由于是3台OSD同时对外传输数据，所以传输速度加快了，当OSD服务器数量越多
这种读写速度的提升就越大
另一方面，对象存储原件是有专门的文件系统的，所以OSD服务器对外又相当于文件服务器，
那么就不存在文件共享方面的困难了
所以说对象存储的出现，很好的结合了块存储与文件存储的优点
5、为什么对象存储兼备块存储和文件存储的优点，还要使用块存储和文件存储呢？
  1、有一类应用是需要存储直接裸盘映射的，例如数据库。因为数据库需要存储裸盘映射给自己后，
     再根据自己的数据库文件系统来对裸盘进行格式化的
  2、 对象存储的成本比起普通的文件存储还是比较高的，需要购买专门的对象存储软件及大容量硬盘
     如果对数据了要求不是海量，只是为了做文件共享的时候，直接用文件存储就好了
############################################################
CEPH OSDS的功能是存储数据，处理数据的复制、恢复、回填再均衡，并通过检查其他OSD 守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当 Ceph 存储集群设定为有2个副本时，至少需要2个 OSD 守护进程，集群才能达到 active+clean 状态（ Ceph 默认有3个副本，但你可以调整副本数）。
Monitors: Ceph Monitor维护着展示集群状态的各种图表，包括监视器图、 OSD 图、归置组（ PG ）图、和 CRUSH 图。 Ceph 保存着发生在Monitors 、 OSD 和 PG上的每一次状态变更的历史信息（称为 epoch ）。
MDSs: Ceph 元数据服务器（ MDS ）为 Ceph 文件系统存储元数据（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）。元数据服务器使得 POSIX 文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 ls、find 等基本命令。

192.168.4.192    ceph-admin
192.168.4.234    ceph-osd
192.168.4.190    ceph-osd



192.168.4.192   jyw-o-control01   ceph-admin
192.168.4.234   jyw-o-network01  ceph-osd
192.168.4.190   jyw-o-cinder01   ceph-osd



[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-hammer/el7/noarch/
enabled=1

yum install ceph-deploy   ntp ntpdate ntp-doc -y
useradd -d  /home/cephtest -m cephtest
echo cephtest|passwd cephtest --stdin

echo "cephtest ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephtest
sudo chmod 0440 /etc/sudoers.d/cephtest
############################################################  

虚拟化是云计算的基础。简单的说，虚拟化使得在一台物理的服务器上可以跑多台虚拟机，虚拟机共享物理机的 CPU、内存、IO 硬件资源，但逻辑上虚拟机之间是相互隔离的。
物理机我们一般称为宿主机（Host），宿主机上面的虚拟机称为客户机（Guest）。

那么 Host 是如何将自己的硬件资源虚拟化，并提供给 Guest 使用的呢？
这个主要是通过一个叫做 Hypervisor 的程序实现的。

根据 Hypervisor 的实现方式和所处的位置，虚拟化又分为两种：
1型虚拟化和2型虚拟化

1型虚拟化
Hypervisor 直接安装在物理机上，多个虚拟机在 Hypervisor 上运行。Hypervisor 实现方式一般是一个特殊定制的 Linux 系统。Xen 和 VMWare 的 ESXi 都属于这个类型。
2型虚拟化
物理机上首先安装常规的操作系统，比如 Redhat、Ubuntu 和 Windows。Hypervisor 作为 OS 上的一个程序模块运行，并对管理虚拟机进行管理。KVM、VirtualBox 和 VMWare Workstation 都属于这个类型。
理论上讲：
1型虚拟化一般对硬件虚拟化功能进行了特别优化，性能上比2型要高；
2型虚拟化因为基于普通的操作系统，会比较灵活，比如支持虚拟机嵌套。嵌套意味着可以在KVM虚拟机中再运行KVM。
KVM 全称是 Kernel-Based Virtual Machine。也就是说 KVM 是基于 Linux 内核实现的。
KVM有一个内核模块叫 kvm.ko，只用于管理虚拟 CPU 和内存。
那 IO 的虚拟化，比如存储和网络设备由谁实现呢？
这个就交给 Linux 内核和Qemu来实现。
说白了，作为一个 Hypervisor，KVM 本身只关注虚拟机调度和内存管理这两个方面。IO 外设的任务交给 Linux 内核和 Qemu。
Libvirt 是啥？
简单说就是 KVM 的管理工具。
其实，Libvirt 除了能管理 KVM 这种 Hypervisor，还能管理 Xen，VirtualBox 等。
OpenStack 底层也使用 Libvirt，所以很有必要学习一下。
3、libvirt包含三个组件，后台daemon程序libvirtd API库和命令行工具virsh
    libvirtd是服务程序，接收和处理 API 请求；
	API 库使得其他人可以开发基于 Libvirt 的高级工具，比如 virt-manager，这是个图形化的 KVM 管理工具，后面我们也会介绍；
	virsh 是我们经常要用的 KVM 命令行工具，后面会有使用的示例。
	yum -y install libcanberra-gtk2 qemu-kvm.x86_64 qemu-kvm-tools.x86_64  \
	libvirt.x86_64 libvirt-cim.x86_64 libvirt-client.x86_64 libvirt-java.noarch \
	libvirt-python.x86_64 libiscsi-1.7.0-5.el6.x86_64  dbus-devel  virt-clone tunctl \
	virt-manager libvirt libvirt-python python-virtinst
	
	yum groupinstall "X Window System"
	yum install dejavu-lgc-sans-fonts  -y 
    yum groupinstall "Fonts" -y
	export NO_AT_BRIDGE=1
qemu-kvm 和 qemu-system 是 KVM 和 QEMU 的核心包，提供 CPU、内存和 IO 虚拟化功能
libvirt-bin 就是 libvirt，用于管理 KVM 等 Hypervisor

	
############################################################
一个 KVM 虚机在宿主机中其实是一个 qemu-kvm 进程，与其他 Linux 进程一样被调度。
虚机中的每一个虚拟 vCPU 则对应 qemu-kvm 进程中的一个线程。
KVM 的存储虚拟化是通过存储池（Storage Pool）和卷（Volume）来管理的。
Storage Pool 是宿主机上可以看到的一片存储空间，可以是多种类型，后面会详细讨论。Volume 是在 Storage Pool 中划分出的一块空间，宿主机将 Volume 分配给虚拟机，Volume 在虚拟机中看到的就是一块硬盘。
那 KVM 是怎么知道要把 /var/lib/libvirt/images 这个目录当做默认 Storage Pool 的呢？ 
实际上 KVM 所有可以使用的 Storage Pool 都定义在宿主机的 /etc/libvirt/storage 目录下，每个 Pool 一个 xml 文件，默认有一个 default.xml，其内容如下
在 /var/lib/libvirt/images/ 下多了一个 8G 的文件 kvm1.img
raw 是默认格式，即原始磁盘镜像格式，移植性好，性能好，但大小固定，不能节省磁盘空间。
qcow2 是推荐使用的格式，cow 表示 copy on write，能够节省磁盘空间，支持 AES 加密，支持 zlib 压缩，支持多快照，功能很多。
############################################################  
1.kvm 使用linux-bridge创建虚拟机网络过程  
yum install qemu-kvm libvirt virt-install virt-manager  -y 
systemctl  enable libvirtd.service
systemctl  start libvirtd.service
virt-manager 
2.启动libvirtd后可以看到在宿主机上面多了一块网卡
virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN 
 link/ether 52:54:00:d2:8b:9a brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever
创建一个新的cirros-0.3.3-x86_64-disk.img虚拟机后可以看到又多了一块网卡 vnet0 和virbr0-nic
vnet0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast master br0 state UNKNOWN qlen 500
    link/ether fe:54:00:8d:b8:5c brd ff:ff:ff:ff:ff:ff
    inet6 fe80::fc54:ff:fe8d:b85c/64 scope link 
       valid_lft forever preferred_lft forever
virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN qlen 500
    link/ether 52:54:00:d2:8b:9a brd ff:ff:ff:ff:ff:ff
3.virbr0 使用 dnsmasq 提供 DHCP 服务，可以在宿主机中查看该进程信息
4.virbr0 是 KVM 默认创建的一个 Bridge，其作用是为连接其上的虚机网卡提供 NAT 访问外网的功能。
5.virbr0 默认分配了一个IP 192.168.122.1，并为连接其上的其他虚拟网卡提供 DHCP 服务。

在 /var/lib/libvirt/dnsmasq/ 目录下有一个 virbr0.status文件，当 VM1 成功获得 DHCP 的 IP 后，可以在该文件中查看到相应的信息	
[
    {
        "ip-address": "192.168.122.162",
        "mac-address": "52:54:00:6e:76:89",
        "hostname": "cirros",
        "client-id": "01:52:54:00:6e:76:89",
        "expiry-time": 1482908439
    }
]
需要说明的是，使用 NAT 的虚机 VM1 可以访问外网，但外网无法直接访问 VM1。 
因为 VM1 发出的网络包源地址并不是 192.168.122.6，而是被 NAT 替换为宿主机的 IP 地址了。


############################################################
kvm虚拟化环境是如何实现vlan的
virt-install \   
--name nyy3 \
--os-type linux  \ 
--ram 512 \  
--vcpus 1 \
--disk path=/www/c2.img \  
--extra-args'console=ttyS0,115200n8 serial'
virt-install --name nyy4 --ram 200 --vcpu 1   --network bridge=brnyy --disk path=/www/c2.img --import    --extra-args'console=ttyS0,115200n8 serial'
virt-install --name nyy3 --ram 1000 --vcpu 2  --disk path=/www/IMGS/nyy4.img,size=40,sparse --cdrom /www/ISO/C8.iso

############################################################  
需求]:
在KVM主机上有若干台虚拟机，它们分别属与不同的vlan，如上图的vlan10,vlan20，我们现在想实现vlan之间互通.

[实验步骤如下]
添加KVM主机vlan

more /etc/rc.local
modprobe 8021q

vconfig add em2 10
vconfig add em2 20
ifconfig em2.10 up
ifconfig em2.20 up
brctl addbr br10
brctl addbr br20
ifconfig br10 up
ifconfig br20 up
brctl addif br10 em2.10
brctl addif br20 em2.20
启动虚拟机后将虚拟机生产的逻辑网口vnet0,vnet1分别加入到不同的vlan中．
brctl addif br10 vnet0
brctl addif br20 vnet1

虚拟机配置不同的网段，如:
Vm1:192.10.10.30
Vm2:192.20.20.30

此时:
需要配置交换机(3750)
Kvm-vlan(config)#vlan database
Kvm-vlan(vlan)#vlan 10
Kvm-vlan(vlan)#vlan 20
Kvm-vlan(vlan)#exit
Kvm-vlan(config)#int vlan 10
Kvm-vlan(config-if)#ip address 192.10.10.1 255.255.255.0

Kvm-vlan(config)#int vlan 20
Kvm-vlan(config-if)#ip address 192.20.20.1 255.255.255.0

Kvm-vlan(config-if)#int fa 1/0/3
Kvm-vlan(config-if)#switchport trunkencapsulation dot1q
Kvm-vlan(config-if)#switchport mode trunk
Kvm-vlan(config-if)#no shut

Kvm-vlan(config)#ip routing

此时再在虚拟机之间互ping，可通!

技巧:
1.删除vlan
ifconfig em2.10 down
vconfig rem em2.10
2.将vlan配置添加到配置文件中
[root@localhost  network-scripts]# more ifcfg-em2
DEVICE="em2"
HWADDR="D4:BE:D9:B5:BC:21"
ONBOOT="yes"
TYPE="Ethernet"
UUID="fc9a6ac1-7850-4ba6-aced-e8aa1813869e"
[root@localhost network-scripts]#  more ifcfg-em2.10
DEVICE="em2.10"
VLAN=yes
ONBOOT="yes"
TYPE="Ethernet"
BRIDGE=br10
[root@localhost network-scripts]#  more ifcfg-em2.20
DEVICE="em2.20"
VLAN=yes
ONBOOT="yes"
TYPE="Ethernet"
BRIDGE=br20
[root@localhost network-scripts]#  more ifcfg-br10
DEVICE=br10
TYPE=Bridge
BOOTPROTO=none
ONBOOT=yes
DELAY=0
[root@localhost network-scripts]#  more ifcfg-br20
DEVICE=br20
TYPE=Bridge
BOOTPROTO=none
ONBOOT=yes
DELAY=0

############################################################
devstack
[[local|localrc]]

MULTI_HOST=true
HOST_IP=192.168.4.235 # management & api network 
LOGFILE=/opt/stack/logs/stack.sh.log

# Credentials 
ADMIN_PASSWORD=admin 
MYSQL_PASSWORD=admin 
RABBIT_PASSWORD=admin 
SERVICE_PASSWORD=admin 
SERVICE_TOKEN=abcdefghijklmnopqrstuvwxyz

# enable neutron-ml2-vlan 
disable_service n-net 
enable_service q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron,q-lbaas,q-fwaas,q-vpn 
Q_AGENT=linuxbridge 
ENABLE_TENANT_VLANS=True 
TENANT_VLAN_RANGE=3001:4000 
PHYSICAL_NETWORK=default

LOG_COLOR=False 
LOGDIR=$DEST/logs 
SCREEN_LOGDIR=$LOGDIR/screen
GIT_BASE=http://git.trystack.cn 
NOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.git 
SPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git
############################################################ 
[[local|localrc]]

MULTI_HOST=true 
HOST_IP=192.168.4.180 # management & api network

# Credentials 
ADMIN_PASSWORD=admin 
MYSQL_PASSWORD=secret 
RABBIT_PASSWORD=secret 
SERVICE_PASSWORD=secret 
SERVICE_TOKEN=abcdefghijklmnopqrstuvwxyz

# Service information 
SERVICE_HOST=192.168.4.235
MYSQL_HOST=$SERVICE_HOST 
RABBIT_HOST=$SERVICE_HOST 
GLANCE_HOSTPORT=$SERVICE_HOST:9292 
Q_HOST=$SERVICE_HOST 
KEYSTONE_AUTH_HOST=$SERVICE_HOST 
KEYSTONE_SERVICE_HOST=$SERVICE_HOST

CEILOMETER_BACKEND=mongodb 
DATABASE_TYPE=mysql

ENABLED_SERVICES=n-cpu,q-agt,neutron 
Q_AGENT=linuxbridge 
ENABLE_TENANT_VLANS=True 
TENANT_VLAN_RANGE=3001:4000 
PHYSICAL_NETWORK=default

# vnc config 
NOVA_VNC_ENABLED=True 
NOVNCPROXY_URL="http://$SERVICE_HOST:6080/vnc_auto.html" 
VNCSERVER_LISTEN=$HOST_IP 
VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN

LOG_COLOR=False 
LOGDIR=$DEST/logs 
SCREEN_LOGDIR=$LOGDIR/screen 
############################################################
http://images.trystack.cn/
http://mp.weixin.qq.com/s?__biz=MjM5NjU1NzA4NA==&mid=2650918738&idx=1&sn=daf3eb361e4e18255a9141986564e28f#rd

############################################################  
keystone的基础概念
1、管理用户及其权限
2、维护openstack services 和Endpoint
3、Authentication 认证和Authorization鉴权
Authentication 认证是你是谁 ，Authorization鉴权是你可以干什么
Authentication是keystone验证user身份的过程
User访问Openstack时向Keystone提交用户名和密码的形式的Credentials，Keystone 验证通过后会给 User 签发一个 Token 作为后续访问的 Credential。
4、token是由数字和字母组成的字符串，User成功User 成功 Authentication 后由 Keystone 分配给 User。
    Token 用做访问 Service 的 Credential
    Service 会通过 Keystone 验证 Token 的有效性
    Token 的有效期默认是 24 小时
Project用于将openstack的资源(计算。存储和网络)进行分组和隔离
根据 OpenStack 服务的对象不同，Project 可以是一个客户（公有云，也叫租户）、部门或者项目组（私有云）。
5、可以理解成租户==project，可以给
 project 包含
      project information,定义
	       name ，description，enabled
      project Members 定义下面的用户，
	       glance，neutron，admin，nova，cinder
	  Quota:配额，可以分配
	        metadata items ,Vcpus,Instances,volume,RAM,net,subnet,snapshot,router
6、service 包括 compute ，block storage ，object storage， image service，network services
每个service都回提供若干的Endpoint，user通过Endpoint访问资源和执行操作
7、Endpoint 是一个网络上可访问的地址，通常是一个URl。Service 通过Endpoint暴露自己的API，Keystone负责管理和维护
每个service的Endpoint
[root@dzc-o-control01v ~]# keystone catalog
[root@dzc-o-control01v glance]# keystone user-list
+----------------------------------+---------+---------+-------------------------+
|                id                |   name  | enabled |          email          |
+----------------------------------+---------+---------+-------------------------+
| 7d5b5abc30ea463690567e5f8cc794f9 |  admin  |   True  | openstackadmin@fang.com |
| bef1232a773d48b1979313b72314ceab |  cinder |   True  |                         |
| 2a1a6dee31a0461c95a5ba8e7e1f30e1 |   fang  |   True  |     suqirui@fang.com    |
| 0472147413714634a041c33384148424 |  glance |   True  |                         |
| 142f626ff9994326ab1d967c6ea43ac0 | neutron |   True  |                         |
| a2164228f5134d959fa514366b236ea1 |   nova  |   True  |                         |
+----------------------------------+---------+---------+-------------------------+


Service: compute
+-------------+---------------------------------------------------------------------------+
|   Property  |                                   Value                                   |
+-------------+---------------------------------------------------------------------------+
|   adminURL  | http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3 |
|      id     |                      05754bb535f04d58b4f2a6819ea31a67                     |
| internalURL | http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3 |
|  publicURL  | http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3 |
|    region   |                                 regionOne                                 |
+-------------+---------------------------------------------------------------------------+
Service: network
+-------------+---------------------------------------+
|   Property  |                 Value                 |
+-------------+---------------------------------------+
|   adminURL  | http://controller.light.fang.com:9696 |
|      id     |    75d7d2b94fd14637a990056bf95ecec6   |
| internalURL | http://controller.light.fang.com:9696 |
|  publicURL  | http://controller.light.fang.com:9696 |
|    region   |               regionOne               |
+-------------+---------------------------------------+
Service: volumev2
+-------------+---------------------------------------------------------------------------+
|   Property  |                                   Value                                   |
+-------------+---------------------------------------------------------------------------+
|   adminURL  | http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3 |
|      id     |                      37eb9ae73b72467b8695247873083335                     |
| internalURL | http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3 |
|  publicURL  | http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3 |
|    region   |                                 regionOne                                 |
+-------------+---------------------------------------------------------------------------+
Service: image
+-------------+---------------------------------------+
|   Property  |                 Value                 |
+-------------+---------------------------------------+
|   adminURL  | http://controller.light.fang.com:9292 |
|      id     |    01461acd46ea449d93b38a730b721462   |
| internalURL | http://controller.light.fang.com:9292 |
|  publicURL  | http://controller.light.fang.com:9292 |
|    region   |               regionOne               |
+-------------+---------------------------------------+
Service: volume
+-------------+---------------------------------------------------------------------------+
|   Property  |                                   Value                                   |
+-------------+---------------------------------------------------------------------------+
|   adminURL  | http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3 |
|      id     |                      30d052b7f62344dabab8c29928d29f99                     |
| internalURL | http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3 |
|  publicURL  | http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3 |
|    region   |                                 regionOne                                 |
+-------------+---------------------------------------------------------------------------+
Service: identity
+-------------+---------------------------------------------+
|   Property  |                    Value                    |
+-------------+---------------------------------------------+
|   adminURL  | http://controller.light.fang.com:35357/v2.0 |
|      id     |       3e889bea96a34b55ad1ff2061f637851      |
| internalURL |  http://controller.light.fang.com:5000/v2.0 |
|  publicURL  |  http://controller.light.fang.com:5000/v2.0 |
|    region   |                  regionOne                  |
+-------------+---------------------------------------------+
8. Service决定每个Role能做什么事情，service通过各自的policy.json文件对Role进行访问控制
/etc/keystone/policy.json 
{
    "admin_required": "role:admin or is_admin:1",
    "service_role": "role:service",
    "service_or_admin": "rule:admin_required or rule:service_role",
    "owner" : "user_id:%(user_id)s",
    "admin_or_owner": "rule:admin_required or rule:owner",

    "default": "rule:admin_required",

    "identity:get_region": "",
    "identity:list_regions": "",
    "identity:create_region": "rule:admin_required",
    "identity:update_region": "rule:admin_required",
    "identity:delete_region": "rule:admin_required",

    "identity:get_service": "rule:admin_required",
    "identity:list_services": "rule:admin_required",
    "identity:create_service": "rule:admin_required",
    "identity:update_service": "rule:admin_required",
    "identity:delete_service": "rule:admin_required",
}
/etc/nova/policy.json
{
    "context_is_admin":  "role:admin",
    "admin_or_owner":  "is_admin:True or project_id:%(project_id)s",
    "default": "rule:admin_or_owner",

    "cells_scheduler_filter:TargetCellFilter": "is_admin:True",

    "compute:create": "",
    "compute:create:attach_network": "",
    "compute:create:attach_volume": "",
}
OpenStack 默认配置只区分 admin 和非 admin Role。 如果需要对特定的 Role 进行授权，可以修改 policy.json。
10 .当点击登录时发生了什么
admin -->带上credential请求登录--> Keystone通过Authentication通过这是token，token中包含了User的Role信息

########################################################################################################################
Glance服务
Image是一个模板，里面包含了基本的操作系统和其它软件
glance的架构

                           User
						    || 
                      |----------------|
					  |   glance-api   | 
					   ----------------
                        //            \\
                       //              \\
		      |---------------|	    |-------------|
			  |glance-registry| 	|backend-store| 
			   ----------------     ----------------
                     //                   \\
                    //                     \\
            |---------|                   |--------------------------------|
            |   DB   |                    |   swift   ,filesystem,http     |
             --------                      --------------------------------
glance-api 是系统后台运行的服务进程。 对外提供 REST API，响应 image 查询、获取和存储的调用
glance-api 不会真正的处理请求。如果是于image metadata相关的操作，glance-api会把请求转发给glance-registry
     如果是与image自身读取相关的操作，glance-api会把请求转给后端存储
1. 例如在页面上查询镜像时
/var/log/glance/glance-api.log 
2016-12-29 12:08:55.793 6988 INFO glance.wsgi.server [-] 10.20.8.32 - - [29/Dec/2016 12:08:55] "GET /v1/images/detail?sort_key=created_at&sort_dir=desc&limit=1000 HTTP/1.1" 200 5003 0.572073
/var/log/glance/glance-registry.log 
2016-12-29 12:08:55.789 7093 INFO glance.wsgi.server [98815fed-72aa-4ed5-97f1-06bbb571db3e 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] 127.0.0.1 - - [29/Dec/2016 12:08:55] "GET /images/detail?sort_key=created_at&sort_dir=desc&limit=1000 HTTP/1.1" 200 6554 0.316860	 
2. glance-registry负责处理和存取image的metadata，例如image的大小，类型
3. glance自己并不存储image，真正的image存放在后端，glance支持多种后端存储
       1. A directory on a local filesystem
	   2. GridFs
	   3. Ceph RBD
	   4. Amazon S3
       5. Sheepdog
       6. OpenStack Block Storage (Cinder)
       7. OpenStack Object Storage (Swift)
       8. VMware ESX	
4. 具体使用哪种backend是在/etc/glance/glance-api.conf 中配置的
[glance_store]
default_store=file
filesystem_store_datadir=/var/lib/glance/images/

-rw-r----- 1 glance glance 3.4G Dec 16 16:02 02d2fae8-6991-434e-8ee5-f3ef276fccd0
-rw-r----- 1 glance glance 1.3G Dec 16 15:36 17ed0c0a-55a6-4044-8ecf-8af8844b4199
-rw-r----- 1 glance glance 2.0G Dec 16 12:20 1ca0181d-24f4-4c13-938e-e05ade3d1ada
-rw-r----- 1 glance glance  16G Dec 29  2015 651a3d42-d699-45b8-92d3-2ca51a9d15df
-rw-r----- 1 glance glance  56G Jun  1  2016 a8da627c-dd8d-4541-9785-793222017537
-rw-r----- 1 glance glance  13M Dec 18  2015 db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5
-rw-r----- 1 glance glance 8.5G Dec 15 15:10 e274bc23-475e-4102-ac66-9012731690ec

[root@dzc-o-control01v glance]# glance image-list
+--------------------------------------+---------------------+-------------+------------------+-------------+--------+
| ID                                   | Name                | Disk Format | Container Format | Size        | Status |
+--------------------------------------+---------------------+-------------+------------------+-------------+--------+
| 1ca0181d-24f4-4c13-938e-e05ade3d1ada | centos6.5-new       | qcow2       | bare             | 2139029504  | active |
| 651a3d42-d699-45b8-92d3-2ca51a9d15df | CentOS6.5-x86_64    | qcow2       | bare             | 16108814336 | active |
| 17ed0c0a-55a6-4044-8ecf-8af8844b4199 | centos7.2_x64       | qcow2       | bare             | 1373306880  | active |
| db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5 | cirros-0.3.3-x86_64 | qcow2       | bare             | 13200896    | active |
| 02d2fae8-6991-434e-8ee5-f3ef276fccd0 | docker-test         | qcow2       | bare             | 3605397504  | active |
| e274bc23-475e-4102-ac66-9012731690ec | kafka快照测试       | qcow2       | bare             | 9059631104  | active |
| a8da627c-dd8d-4541-9785-793222017537 | Windows2008-x86_64  | qcow2       | bare             | 59065040896 | active |
+--------------------------------------+---------------------+-------------+------------------+-------------+--------+

5 .命令行创建一个image source /root/admin-openrc.sh 	   
glance image-create --name 1229cent8test --file ./CentOS-6-x86_64-GenericCloud-1608.qcow2 --disk-format qcow2 --container-format bare --progress  	   
[=============================>] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 236ac04fada7b251d5d8a30114a3f7f7     |
| container_format | bare                                 |
| created_at       | 2016-12-29T04:24:52                  |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | 4fb4ed33-1391-485b-9794-08a986eb1164 |
| is_public        | False                                |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | 1229cent8test                        |
| owner            | e599088c985f42e7948b12f601705cd3     |
| protected        | False                                |
| size             | 748290048                            |
| status           | active                               |
| updated_at       | 2016-12-29T04:24:58                  |
| virtual_size     | None                                 |
+------------------+--------------------------------------+
glance-registry.log 
2016-12-29 12:24:52.855 7098 INFO glance.registry.api.v1.images [b1534ef5-5f25-4519-ad89-6128d2e84338 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Successfully created image 4fb4ed33-1391-485b-9794-08a986eb1164
2016-12-29 12:24:52.856 7098 INFO glance.wsgi.server [b1534ef5-5f25-4519-ad89-6128d2e84338 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] 127.0.0.1 - - [29/Dec/2016 12:24:52] "POST /images HTTP/1.1" 200 686 0.241068
2016-12-29 12:24:52.976 7085 INFO glance.wsgi.server [04448175-0030-48de-b485-34fa86c7c93f 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] 127.0.0.1 - - [29/Dec/2016 12:24:52] "GET /images/4fb4ed33-1391-485b-9794-08a986eb1164 HTTP/1.1" 200 686 0.116424
2016-12-29 12:24:53.020 7085 INFO glance.registry.api.v1.images [11191ee4-ce1e-4cb1-ac47-5f3001454098 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Updating metadata for image 4fb4ed33-1391-485b-9794-08a986eb1164
2016-12-29 12:24:53.022 7085 INFO glance.wsgi.server [11191ee4-ce1e-4cb1-ac47-5f3001454098 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] 127.0.0.1 - - [29/Dec/2016 12:24:53] "PUT /images/4fb4ed33-1391-485b-9794-08a986eb1164 HTTP/1.1" 200 686 0.043416
2016-12-29 12:24:58.060 7101 INFO glance.registry.api.v1.images [cf6e6e6e-37d9-4bac-ad16-85a5ec7a47b4 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Updating metadata for image 4fb4ed33-1391-485b-9794-08a986eb1164
2016-12-29 12:24:58.061 7101 INFO glance.wsgi.server [cf6e6e6e-37d9-4bac-ad16-85a5ec7a47b4 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] 127.0.0.1 - - [29/Dec/2016 12:24:58] "PUT /images/4fb4ed33-1391-485b-9794-08a986eb1164 HTTP/1.1" 200 716 0.140181
2016-12-29 12:24:58.162 7101 INFO glance.registry.api.v1.images [3a08307b-d8d4-4373-86e7-63d49a00f7d5 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Updating metadata for image 4fb4ed33-1391-485b-9794-08a986eb1164
2016-12-29 12:24:58.163 7101 INFO glance.wsgi.server [3a08307b-d8d4-4373-86e7-63d49a00f7d5 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] 127.0.0.1 - - [29/Dec/2016 12:24:58] "PUT /images/4fb4ed33-1391-485b-9794-08a986eb1164 HTTP/1.1" 200 903 0.098581
2016-12-29 12:24:58.408 7080 INFO glance.wsgi.server [41e8c199-8ab4-4aaa-a42d-a262c1affc4b 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] 127.0.0.1 - - [29/Dec/2016 12:24:58] "GET /images/4fb4ed33-1391-485b-9794-08a986eb1164/members HTTP/1.1" 200 204 0.239410
glance-api.log
2016-12-29 12:24:58.063 7009 INFO glance.api.v1.images [-] Uploaded data of image 4fb4ed33-1391-485b-9794-08a986eb1164 from request payload successfully.
2016-12-29 12:24:58.411 7009 INFO glance.wsgi.server [-] 10.20.8.31 - - [29/Dec/2016 12:24:58] "POST /v1/images HTTP/1.1" 201 827 6.044210

########################################################################################################################  
nova架构
nova-api
   接收和响应客户的 API 调用。 除了提供 OpenStack 自己的API，nova-api 还支持 Amazon EC2 API。 也就是说，如果客户以前使用 Amazon EC2，并且用 EC2 的 API 开发了些工具来管理虚机，那么如果现在要换成 OpenStack，这些工具可以无缝迁移到 OpenStack，因为 nova-api 兼容 EC2 API，无需做任何修改。
nova-scheduler
   虚机调度服务，负责决定在哪个计算节点上运行虚机
nova-compute
   管理虚机的核心服务，通过调用 Hypervisor API 实现虚机生命周期管理
Hypervisor    通常是libvirt
   计算节点上跑的虚拟化管理程序，虚机管理最底层的程序。 不同虚拟化技术提供自己的 Hypervisor。 常用的 Hypervisor 有 KVM，Xen， VMWare 等
nova-conductor
    nova-compute 经常需要更新数据库，比如更新虚机的状态。 出于安全性和伸缩性的考虑，nova-compute 并不会直接访问数据库，而是将这个任务委托给 nova-conductor，这个我们在后面会详细讨论。
nova-console
    用户可以通过多种方式访问虚拟机的控制台，nova-novncproxy基于 Web 浏览器的 VNC 访问 nova-spicehtml5proxy，基于 HTML5 浏览器的 SPICE 访问 nova-xvpnvncproxy，基于 Java 客户端的 VNC 访问
nova-consoleauth
    负责对访问虚拟机控制台提供token认证
nova-cert
    提供X509证书支持
[root@dzc-o-control01v glance]# nova service-list
+----+------------------+------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host             | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-cert        | dzc-o-control01v | internal | enabled | up    | 2016-12-29T07:09:10.000000 | -               |
| 2  | nova-consoleauth | dzc-o-control01v | internal | enabled | up    | 2016-12-29T07:09:02.000000 | -               |
| 3  | nova-conductor   | dzc-o-control01v | internal | enabled | up    | 2016-12-29T07:09:08.000000 | -               |
| 5  | nova-scheduler   | dzc-o-control01v | internal | enabled | up    | 2016-12-29T07:09:01.000000 | -               |
| 13 | nova-consoleauth | dzc-o-control02  | internal | enabled | up    | 2016-12-29T07:09:06.000000 | -               |
| 14 | nova-conductor   | dzc-o-control02  | internal | enabled | up    | 2016-12-29T07:09:09.000000 | -               |
| 17 | nova-cert        | dzc-o-control02  | internal | enabled | up    | 2016-12-29T07:09:05.000000 | -               |
| 18 | nova-scheduler   | dzc-o-control02  | internal | enabled | up    | 2016-12-29T07:09:07.000000 | -               |
| 19 | nova-compute     | sjhl-o-compute03 | dzc      | enabled | up    | 2016-12-29T07:09:09.000000 | None            |
| 20 | nova-compute     | sjhl-o-compute04 | dzc      | enabled | up    | 2016-12-29T07:09:03.000000 | None            |
| 21 | nova-compute     | sjhl-o-compute05 | dzc      | enabled | up    | 2016-12-29T07:09:06.000000 | None            |
| 22 | nova-compute     | sjhl-o-compute06 | dzc      | enabled | up    | 2016-12-29T07:09:07.000000 | None            |
| 23 | nova-compute     | sjhl-o-compute08 | dzc      | enabled | up    | 2016-12-29T07:09:06.000000 | None            |
| 24 | nova-compute     | sjhl-o-compute09 | dzc      | enabled | up    | 2016-12-29T07:09:01.000000 | None            |
| 25 | nova-compute     | sjhl-o-compute10 | dzc      | enabled | up    | 2016-12-29T07:09:10.000000 | None            |
| 26 | nova-compute     | sjhl-o-compute07 | dzc      | enabled | up    | 2016-12-29T07:09:06.000000 | None            |
| 27 | nova-compute     | dzc-o-compute01  | dzc      | enabled | up    | 2016-12-29T07:09:01.000000 | -               |
| 28 | nova-compute     | dzc-o-compute02  | dzc      | enabled | down  | 2016-12-25T00:28:00.000000 | None            |
| 29 | nova-console     | dzc-o-control01v | internal | enabled | up    | 2016-12-29T07:09:10.000000 | -               |
+----+------------------+------------------+----------+---------+-------+----------------------------+-----------------+

从创建虚拟机看nova-*各种服务怎么协同工作的
						 user 
                          ||
	messaging	<----	nova-api 
       ||
       || ----->scheduler 
       || ----->compute 
	   || ----->conductor --->DB
	   
1. 客户（可以是 OpenStack 最终用户，也可以是其他程序）向 API（nova-api）发送请求：“帮我创建一个虚机”
2. API 对请求做一些必要处理后，向 Messaging（RabbitMQ）发送了一条消息：“让 Scheduler 创建一个虚机” 
3. Scheduler（nova-scheduler）从 Messaging 获取到 API 发给它的消息，然后执行调度算法，从若干计算节点中选出节点 A	   
4. Scheduler 向 Messaging 发送了一条消息：“在计算节点 A 上创建这个虚机”
5. 计算节点 A 的 Compute（nova-compute）从 Messaging 中获取到 Scheduler 发给它的消息，然后在本节点的 Hypervisor 上启动虚机。
6. 在虚机创建的过程中，Compute 如果需要查询或更新数据库信息，会通过 Messaging 向 Conductor（nova-conductor）发送消息，Conductor 负责数据库访问。
	   

openstack的driver框架OpenStack 的计算节点支持多种 Hypervisor。 包括 KVM, Hyper-V, VMWare, Xen, Docker, LXC 等。 
Nova-compute 为这些 Hypervisor 定义了统一的接口，hypervisor 只需要实现这些接口，就可以 driver 的形式即插即用到 OpenStack 中。 下面是 nova driver 的架构示意图 
在 nova-compute 的配置文件 /etc/nova/nova.conf 中由 compute_driver 配置项指定该计算节点使用哪种 Hypervisor 的 driver
compute_driver=libvirt.LibvirtDriver
instances_path=/var/lib/nova/instances

[root@dzc-o-control01v glance]# keystone endpoint-get  --service compute
+-------------------+---------------------------------------------------------------------------+
|      Property     |                                   Value                                   |
+-------------------+---------------------------------------------------------------------------+
| compute.publicURL | http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3 |
+-------------------+---------------------------------------------------------------------------+
Nova-api 对接收到的 HTTP API 请求会做如下处理
      1. 检查客户端传人的参数是否合法有效 
	  2. 调用 Nova 其他子服务的处理客户端 HTTP 请求 
	  3. 格式化 Nova 其他子服务返回的结果并返回给客户端
	   
#######||#################################################################################################################
launch instance  启动虚拟机 
flavor 虚拟机类型 主要定义了 Vcpus RAM disk  metadata
nova-scheduler会选择合适的计算节点
在/etc/nova/nova.conf中 ，nova通过scheduler_driver        这三个参数来配置scheduler
                                  scheduler_avaiable_filters
								  scheduler_default_filters 
Filter scheduler 是 nova-scheduler 默认的调度器，调度过程分为两步：
  1.通过过滤器（filter）选择满足条件的计算节点（运行 nova-compute）
  2.通过权重计算（weighting）选择在最优（权重值最大）的计算节点上创建 Instance。
  scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler  
  Nova.conf 中的 scheduler_available_filters 选项用于配置 scheduler 可用的 filter，默认是所有 nova 自带的 filter 都可以用于滤操作。
  scheduler_filter_classes=nova.cells.filters.all_filters
  另外还有一个选项 scheduler_default_filters，用于指定 scheduler 真正使用的 filter，默认值如下 
  scheduler_default_filters=RetryFilter,
							     RetryFilter 的作用是刷掉之前已经调度过的节点。 
							AvailabilityZoneFilter,
							     为提高容灾性和提供隔离服务，可以将计算节点划分到不同的Availability Zone中。 
								 例如把一个机架上的机器划分在一个 Availability Zone 中。 OpenStack 默认有一个命名为 “Nova” 的 Availability Zone，所有的计算节点初始都是放在 “Nova” 中。 用户可以根据需要创建自己的 Availability Zone。 
								 default_availability_zone=dzc
							RamFilter,
							     RamFilter 将不能满足 flavor 内存需求的计算节点过滤掉。 
								 对于内存有一点需要注意： 为了提高系统的资源使用率，OpenStack 在计算节点可用内存时允许 overcommit，也就是可以超过实际内存大小
								 ram_allocation_ratio=1.5
								 DiskFilter 将不能满足 flavor 磁盘需求的计算节点过滤掉。 
							ComputeFilter,
							     ComputeFilter 保证只有 nova-compute 服务正常工作的计算节点才能够被 nova-scheduler调度。
							ComputeCapabilitiesFilter,
							     ComputeCapabilitiesFilter 根据计算节点的特性来筛选。
							ImagePropertiesFilter,
							ServerGroupAntiAffinityFilter,
							     ServerGroupAntiAffinityFilter 可以尽量将 Instance 分散部署到不同的节点上。
							ServerGroupAffinityFilter
							     与 ServerGroupAntiAffinityFilter 的作用相反，ServerGroupAffinityFilter 会尽量将 instance 部署到同一个计算节点上。 方法类似
								 
经过前面一堆 filter 的过滤，nova-scheduler 选出了能够部署 instance 的计算节点。 如果有多个计算节点通过了过滤，那么最终选择哪个节点呢？
Scheduler 会对每个计算节点打分，得分最高的获胜。 打分的过程就是 weight，翻译过来就是计算权重值，那么 scheduler 是根据什么来计算权重值呢？
目前 nova-scheduler 的默认实现是根据计算节点空闲的内存量计算权重值： 空闲内存越多，权重越大，instance 将被部署到当前空闲内存最多的计算节点上。





nova-compute 的功能可以分为两类：
  1 .定时向openstack报告计算节点的状态
      前面我们看到 nova-scheduler 的很多 Filter 是根据算节点的资源使用情况进行过滤的。 比如 RamFilter 要检查计算节点当前可以的内存量；CoreFilter 检查可用的 vCPU 数量；DiskFilter 则会检查可用的磁盘空间。
	  那这里有个问题：OpenStack 是如何得知每个计算节点的这些信息呢？ 答案就是：nova-compute 会定期向 OpenStack 报告。
      从 nova-compute 的日志 /opt/stack/logs/n-cpu.log 可以发现： 每隔一段时间，nova-compute 就会报告当前计算节点的资源使用情况和 nova-compute 服务状态。
      nova调用hypervisor的driver拿到本机上instance所占用的资源信息

  2 .实现instance生命周期的管理

launch instance的过程
user-->nova-api-->rabbimtmq-->nova-scheduler算法选举-->rabbimtmq-->目标宿主机拿到信息创建虚拟机
在nova-compute节点上创建虚拟机的过程为
1.为虚拟机准备资源
2.创建虚拟机的镜像文件
3.准备xml文件
4.创建网络并启动虚拟机  
  
下面我们分析一下launch instance时scheduler和compute的日志
页面上创建虚拟机后
2016-12-29 16:08:29.851 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._expire_reservations run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 16:08:29.858 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 44.82 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-29 16:08:54.957 29308 WARNING nova.scheduler.host_manager [req-7404863a-6a27-416b-be0e-88a570d496cf None] Host has more disk space than database expected (362gb > 297gb)
2016-12-29 16:08:54.957 29308 WARNING nova.scheduler.host_manager [req-7404863a-6a27-416b-be0e-88a570d496cf None] Host has more disk space than database expected (284gb > 207gb)
2016-12-29 16:08:54.958 29308 WARNING nova.scheduler.host_manager [req-7404863a-6a27-416b-be0e-88a570d496cf None] Host has more disk space than database expected (514gb > 437gb)
2016-12-29 16:08:54.958 29308 WARNING nova.scheduler.host_manager [req-7404863a-6a27-416b-be0e-88a570d496cf None] Host has more disk space than database expected (374gb > 297gb)
在这里开始10台计算节点选举了
2016-12-29 16:08:54.959 29308 DEBUG nova.filters [req-7404863a-6a27-416b-be0e-88a570d496cf None] Starting with 10 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:70
sjhl-o-compute08首先被retry_filter刷掉了，因为它之前创建虚拟机失败了
2016-12-29 16:08:54.959 29308 DEBUG nova.scheduler.filters.retry_filter [req-7404863a-6a27-416b-be0e-88a570d496cf None] Host [u'sjhl-o-compute08', u'sjhl-o-compute08'] fails.  Previously tried hosts: [[u'sjhl-o-compute08', u'sjhl-o-compute08']] host_passes /usr/lib/python2.7/site-packages/nova/scheduler/filters/retry_filter.py:42
2016-12-29 16:08:54.960 29308 DEBUG nova.filters [req-7404863a-6a27-416b-be0e-88a570d496cf None] Filter RetryFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-29 16:08:55.018 29308 DEBUG nova.filters [req-7404863a-6a27-416b-be0e-88a570d496cf None] Filter AvailabilityZoneFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-29 16:08:55.019 29308 DEBUG nova.filters [req-7404863a-6a27-416b-be0e-88a570d496cf None] Filter RamFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-29 16:08:55.019 29308 DEBUG nova.servicegroup.drivers.db [req-7404863a-6a27-416b-be0e-88a570d496cf None] Seems service is down. Last heartbeat was 2016-12-25 00:28:00. Elapsed time is 373255.019553 is_up /usr/lib/python2.7/site-packages/nova/servicegroup/drivers/db.py:75
dzc-o-compute02被刷掉了，因为compute服务不可用，还剩8台
2016-12-29 16:08:55.019 29308 WARNING nova.scheduler.filters.compute_filter [req-7404863a-6a27-416b-be0e-88a570d496cf None] (dzc-o-compute02, dzc-o-compute02) ram:31626 disk:278528 io_ops:0 instances:0 has not been heard from in a while
2016-12-29 16:08:55.020 29308 DEBUG nova.filters [req-7404863a-6a27-416b-be0e-88a570d496cf None] Filter ComputeFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-29 16:08:55.020 29308 DEBUG nova.filters [req-7404863a-6a27-416b-be0e-88a570d496cf None] Filter ComputeCapabilitiesFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-29 16:08:55.020 29308 DEBUG nova.filters [req-7404863a-6a27-416b-be0e-88a570d496cf None] Filter ImagePropertiesFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-29 16:08:55.022 29308 DEBUG nova.filters [req-7404863a-6a27-416b-be0e-88a570d496cf None] Filter ServerGroupAntiAffinityFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-29 16:08:55.023 29308 DEBUG nova.filters [req-7404863a-6a27-416b-be0e-88a570d496cf None] Filter ServerGroupAffinityFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-29 16:08:55.024 29308 DEBUG nova.scheduler.filter_scheduler [req-7404863a-6a27-416b-be0e-88a570d496cf None] Filtered [(sjhl-o-compute07, sjhl-o-compute07) ram:61328 disk:394240 io_ops:0 instances:3, (sjhl-o-compute05, sjhl-o-compute05) ram:71184 disk:135168 io_ops:0 instances:5, (dzc-o-compute01, dzc-o-compute01) ram:31626 disk:229376 io_ops:0 instances:0, (sjhl-o-compute03, sjhl-o-compute03) ram:78096 disk:301056 io_ops:0 instances:5, (sjhl-o-compute09, sjhl-o-compute09) ram:70800 disk:205824 io_ops:0 instances:5, (sjhl-o-compute10, sjhl-o-compute10) ram:70032 disk:304128 io_ops:0 instances:4, (sjhl-o-compute04, sjhl-o-compute04) ram:70032 disk:304128 io_ops:0 instances:4, (sjhl-o-compute06, sjhl-o-compute06) ram:70416 disk:211968 io_ops:0 instances:4] _schedule /usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:281
有经过一堆filter一台也没刷掉，最后进入权重filter ，因为sjhl-o-compute03内存最高，权重为1 高票当选
2016-12-29 16:08:55.024 29308 DEBUG nova.scheduler.filter_scheduler [req-7404863a-6a27-416b-be0e-88a570d496cf None] Weighed [WeighedHost [host: (sjhl-o-compute03, sjhl-o-compute03) ram:78096 disk:301056 io_ops:0 instances:5, weight: 1.0], WeighedHost [host: (sjhl-o-compute05, sjhl-o-compute05) ram:71184 disk:135168 io_ops:0 instances:5, weight: 0.911493546404], WeighedHost [host: (sjhl-o-compute09, sjhl-o-compute09) ram:70800 disk:205824 io_ops:0 instances:5, weight: 0.906576521205], WeighedHost [host: (sjhl-o-compute06, sjhl-o-compute06) ram:70416 disk:211968 io_ops:0 instances:4, weight: 0.901659496005], WeighedHost [host: (sjhl-o-compute10, sjhl-o-compute10) ram:70032 disk:304128 io_ops:0 instances:4, weight: 0.896742470805], WeighedHost [host: (sjhl-o-compute04, sjhl-o-compute04) ram:70032 disk:304128 io_ops:0 instances:4, weight: 0.896742470805], WeighedHost [host: (sjhl-o-compute07, sjhl-o-compute07) ram:61328 disk:394240 io_ops:0 instances:3, weight: 0.785289899611], WeighedHost [host: (dzc-o-compute01, dzc-o-compute01) ram:31626 disk:229376 io_ops:0 instances:0, weight: 0.404963122311]] _schedule /usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:286
2016-12-29 16:09:14.683 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._run_periodic_tasks run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 16:09:14.684 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 16.16 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-29 16:09:30.853 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._expire_reservations run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 16:09:30.863 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 45.82 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132

然后看compute03中的nova日志
从rabbitmq中获取了创建实例的操作
2016-12-29 16:08:55.150 27161 AUDIT nova.compute.manager [req-7404863a-6a27-416b-be0e-88a570d496cf None] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Starting instance...
2016-12-29 16:08:55.243 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.243 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.243 27161 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock "instance_claim" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-29 16:08:55.243 27161 WARNING nova.compute.resource_tracker [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Host field should not be set on the instance until resources have been claimed.
2016-12-29 16:08:55.244 27161 WARNING nova.compute.resource_tracker [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Node field should not be set on the instance until resources have been claimed.
2016-12-29 16:08:55.244 27161 DEBUG nova.compute.resource_tracker [-] Memory overhead for 512 MB instance; 0 MB instance_claim /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:127
由于我选择的flavor是 tinny 就是512M内存 1vcpu  1G磁盘
  
2016-12-29 16:08:55.246 27161 AUDIT nova.compute.claims [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Attempting claim: memory 512 MB, disk 1 GB
2016-12-29 16:08:55.247 27161 AUDIT nova.compute.claims [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Total memory: 193424 MB, used: 115328.00 MB
2016-12-29 16:08:55.247 27161 AUDIT nova.compute.claims [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] memory limit: 290136.00 MB, free: 174808.00 MB
2016-12-29 16:08:55.247 27161 AUDIT nova.compute.claims [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Total disk: 547 GB, used: 241.00 GB
2016-12-29 16:08:55.248 27161 AUDIT nova.compute.claims [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] disk limit not specified, defaulting to unlimited
2016-12-29 16:08:55.262 27161 DEBUG nova.compute.resources.vcpu [-] Total CPUs: 24 VCPUs, used: 61.00 VCPUs test /usr/lib/python2.7/site-packages/nova/compute/resources/vcpu.py:51
2016-12-29 16:08:55.263 27161 DEBUG nova.compute.resources.vcpu [-] CPUs limit not specified, defaulting to unlimited test /usr/lib/python2.7/site-packages/nova/compute/resources/vcpu.py:55
2016-12-29 16:08:55.263 27161 AUDIT nova.compute.claims [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Claim successful

2016-12-29 16:08:55.377 27161 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute03', 'sjhl-o-compute03')
2016-12-29 16:08:55.378 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.378 27161 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released "instance_claim" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-29 16:08:55.378 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "refresh_cache-6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.378 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "refresh_cache-6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.379 27161 DEBUG nova.network.neutronv2.api [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
2016-12-29 16:08:55.379 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.379 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.379 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.380 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.380 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.380 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.380 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.381 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.381 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.381 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.381 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.381 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.382 27161 DEBUG neutronclient.client [-] 

这里很有意思，宿主机向network endpoint 请求ip
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b -X GET -H "X-Auth-Token: ce4ab34a19a44ca4a9c1ceed1e6c0322" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 16:08:55.412 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Thu, 29 Dec 2016 08:08:55 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '743', 'x-openstack-request-id': 'req-c6a08666-e076-4dfa-a501-5eaa6816559e'} {"ports": [{"status": "ACTIVE", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.97"}], "id": "e647790f-0ffe-44c6-b887-67635a8137a0", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:9c:bc:87"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 16:08:55.412 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.412 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.412 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.413 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.413 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.413 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.413 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.414 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.414 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.414 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.414 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.414 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.415 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.415 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.415 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.415 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.415 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.416 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.416 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.416 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.416 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.416 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.417 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.417 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.417 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/networks.json?id=d647355e-b019-4de0-999e-00107531edc0 -X GET -H "X-Auth-Token: ff7b3dc509c7430f88f195d84faad024" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 16:08:55.452 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Thu, 29 Dec 2016 08:08:55 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '269', 'x-openstack-request-id': 'req-8b8d8fcd-01b7-4c5c-b631-958e254ecbb0'} {"networks": [{"status": "ACTIVE", "subnets": ["b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"], "name": "DMZ_NET", "router:external": false, "tenant_id": "e599088c985f42e7948b12f601705cd3", "admin_state_up": true, "shared": true, "id": "d647355e-b019-4de0-999e-00107531edc0"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 16:08:55.452 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.452 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.453 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.453 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.453 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.453 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.453 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.454 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.454 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.454 27161 DEBUG neutronclient.client [-] 
宿主机向network endpoint 请求ip
REQ: curl -i http://controller.light.fang.com:9696/v2.0/floatingips.json?fixed_ip_address=192.168.34.97&port_id=e647790f-0ffe-44c6-b887-67635a8137a0 -X GET -H "X-Auth-Token: ce4ab34a19a44ca4a9c1ceed1e6c0322" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 16:08:55.479 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Thu, 29 Dec 2016 08:08:55 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '19', 'x-openstack-request-id': 'req-c776d7ad-1620-42cf-9eb7-fdbf8111f976'} {"floatingips": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 16:08:55.479 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.480 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.480 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.480 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.481 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.481 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.481 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.481 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.481 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.482 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.482 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.482 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.482 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.482 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.483 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.483 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.483 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.483 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.483 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.484 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.484 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.485 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.486 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.487 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.489 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/subnets.json?id=b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 -X GET -H "X-Auth-Token: ff7b3dc509c7430f88f195d84faad024" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 16:08:55.510 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Thu, 29 Dec 2016 08:08:55 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '519', 'x-openstack-request-id': 'req-a4d4c257-da31-41c2-a651-d6a00efa799d'} {"subnets": [{"name": "DMZ_SUBNET", "enable_dhcp": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "dns_nameservers": ["218.30.110.17", "8.8.8.8"], "gateway_ip": null, "ipv6_ra_mode": null, "allocation_pools": [{"start": "192.168.34.3", "end": "192.168.34.250"}], "host_routes": [{"nexthop": "192.168.34.254", "destination": "0.0.0.0/0"}], "ip_version": 4, "ipv6_address_mode": null, "cidr": "192.168.34.0/24", "id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 16:08:55.512 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?network_id=d647355e-b019-4de0-999e-00107531edc0&device_owner=network%3Adhcp -X GET -H "X-Auth-Token: ff7b3dc509c7430f88f195d84faad024" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 16:08:55.534 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Thu, 29 Dec 2016 08:08:55 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '13', 'x-openstack-request-id': 'req-9cc1a5e3-a527-49d3-a232-6f04c7dbb975'} {"ports": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 16:08:55.535 27161 DEBUG nova.network.base_api [-] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.97'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape647790f-0f', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:9c:bc:87', 'active': True, 'type': u'ovs', 'id': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2016-12-29 16:08:55.552 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "refresh_cache-6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.636 27161 DEBUG nova.block_device [req-480e56ed-4874-46e0-80f0-3cf94bc1fb8c None] block_device_list [] volume_in_mapping /usr/lib/python2.7/site-packages/nova/block_device.py:555
2016-12-29 16:08:55.802 27161 DEBUG nova.block_device [-] block_device_list [] volume_in_mapping /usr/lib/python2.7/site-packages/nova/block_device.py:555
创建 instance 的镜像文件
资源准备好之后，nova-compute 会为 instance 创建镜像文件。 OpenStack 启动一个 instance 时，会选择一个 image，这个 image 由 Glance 管理。 nova-compute会：
首先将该 image 下载到计算节点
然后将其作为 backing file 创建 instance 的镜像文件。
2016-12-29 16:08:55.803 27161 INFO nova.virt.libvirt.driver [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Creating image
2016-12-29 16:08:55.804 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk.info" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.804 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.804 27161 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-29 16:08:55.804 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:55.805 27161 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
首先判断本机文件系统中有没有这个img由此可知，如果计算节点上要运行多个相同 image 的 instance，只会在启动第一个 instance 的时候从 Glance 下载 image，后面的 instance 启动速度就大大加快了。 日志如下：


2016-12-29 16:08:55.805 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/_base/d2c8da7000ccecc9fff5da7b843148f0465b785a execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
结果是0 说明有                 
2016-12-29 16:08:55.877 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:55.877 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "d2c8da7000ccecc9fff5da7b843148f0465b785a" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:08:55.878 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "d2c8da7000ccecc9fff5da7b843148f0465b785a" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:08:55.878 27161 DEBUG nova.openstack.common.lockutils [-] Attempting to grab external lock "d2c8da7000ccecc9fff5da7b843148f0465b785a" external_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:178
2016-12-29 16:08:55.878 27161 DEBUG nova.openstack.common.lockutils [-] Got file lock "/var/lib/nova/instances/locks/nova-d2c8da7000ccecc9fff5da7b843148f0465b785a" acquire /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:93
2016-12-29 16:08:55.878 27161 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock "copy_qcow2_image" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-29 16:08:55.878 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/_base/d2c8da7000ccecc9fff5da7b843148f0465b785a execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:55.938 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195

还需要判断一下这个img的类型如果是 qcow2 还需要convert成raw格式的，因为qcow2不能作为backingfile
[root@sjhl-o-compute03 nova]# qemu-img info /var/lib/nova/instances/_base/d2c8da7000ccecc9fff5da7b843148f0465b785a
image: /var/lib/nova/instances/_base/d2c8da7000ccecc9fff5da7b843148f0465b785a
file format: raw
virtual size: 39M (41126400 bytes)
disk size: 17M

qemu-img create -f qcow2 -o backing_file=/var/lib/nova/instances/_base/d2c8da7000ccecc9fff5da7b843148f0465b785a /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk
image，指的是 Glance 上保存的镜像，作为 instance 运行的模板。 计算节点将下载的 image 存放在 /opt/stack/data/nova/instances/_base 目录下。
镜像文件，指的是 instance 启动盘所对应的文件
二者的关系是：image 是镜像文件 的 backing file。image 不会变，而镜像文件会发生变化。比如安装新的软件后，镜像文件会变大。

2016-12-29 16:08:55.938 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): qemu-img create -f qcow2 -o backing_file=/var/lib/nova/instances/_base/d2c8da7000ccecc9fff5da7b843148f0465b785a /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:56.038 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:56.038 27161 DEBUG nova.virt.disk.api [-] Checking if we can resize image /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk. size=1073741824 can_resize_image /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:192

2016-12-29 16:08:56.038 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:56.106 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195

2016-12-29 16:08:56.106 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): qemu-img resize /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk 1073741824 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:56.271 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:56.271 27161 DEBUG nova.virt.disk.api [-] Checking if we can resize filesystem inside /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk. CoW=True is_image_partitionless /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:206
2016-12-29 16:08:56.271 27161 DEBUG nova.virt.disk.vfs.api [-] Instance for image imgfile=/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk imgfmt=qcow2 partition=None instance_for_image /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/api.py:45
2016-12-29 16:08:56.272 27161 DEBUG nova.virt.disk.vfs.api [-] Using primary VFSGuestFS instance_for_image /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/api.py:49
2016-12-29 16:08:56.272 27161 DEBUG nova.virt.disk.vfs.guestfs [-] Setting up appliance for /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk qcow2 setup /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:137
2016-12-29 16:08:58.647 27161 DEBUG nova.virt.disk.vfs.guestfs [-] Mount guest OS image /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk partition None setup_os_static /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:83
2016-12-29 16:08:58.723 27161 DEBUG nova.virt.disk.vfs.guestfs [-] Tearing down appliance teardown /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:185
2016-12-29 16:08:58.725 27161 WARNING nova.virt.disk.vfs.guestfs [-] Failed to close augeas aug_close: do_aug_close: you must call 'aug-init' first to initialize Augeas
2016-12-29 16:08:58.766 27161 DEBUG nova.virt.disk.api [-] Unable to mount image /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk with error Error mounting /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk with libguestfs (mount_options: /dev/sda on / (options: ''): mount: /dev/sda is write-protected, mounting read-only
mount: unknown filesystem type '(null)'). Cannot resize. is_image_partitionless /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:218
2016-12-29 16:08:58.766 27161 DEBUG nova.openstack.common.lockutils [-] Released file lock "/var/lib/nova/instances/locks/nova-d2c8da7000ccecc9fff5da7b843148f0465b785a" release /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:115
2016-12-29 16:08:58.767 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "d2c8da7000ccecc9fff5da7b843148f0465b785a" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:08:58.767 27161 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released "copy_qcow2_image" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-29 16:08:58.767 27161 DEBUG nova.virt.libvirt.driver [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Start _get_guest_xml network_info=[VIF({'profile': {}, 'ovs_interfaceid': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.97'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape647790f-0f', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:9c:bc:87', 'active': True, 'type': u'ovs', 'id': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'qbg_params': None})] disk_info={'disk_bus': 'virtio', 'cdrom_bus': 'ide', 'mapping': {'disk': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': u'vda'}, 'root': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': u'vda'}}} image_meta={u'status': u'active', u'deleted': False, u'container_format': u'bare', u'min_ram': 0, u'updated_at': u'2015-12-18T08:55:33.000000', u'owner': u'e599088c985f42e7948b12f601705cd3', u'min_disk': 0, u'is_public': True, u'deleted_at': None, u'properties': {}, u'size': 13200896, u'name': u'cirros-0.3.3-x86_64', u'checksum': u'133eae9fb1c98f45894a4e60d8736619', u'created_at': u'2015-12-18T08:55:31.000000', u'disk_format': u'qcow2', u'id': u'db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5'} rescue=None block_device_info={'block_device_mapping': [], 'root_device_name': u'/dev/vda', 'ephemerals': [], 'swap': None} _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4156
2016-12-29 16:08:58.798 27161 DEBUG nova.virt.libvirt.driver [-] CPU mode 'host-model' model '' was chosen _get_guest_cpu_model_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:3362
2016-12-29 16:08:58.798 27161 DEBUG nova.virt.hardware [-] Getting desirable topologies for flavor Flavor(created_at=None,deleted=False,deleted_at=None,disabled=False,ephemeral_gb=0,extra_specs={},flavorid='1',id=2,is_public=True,memory_mb=512,name='m1.tiny',projects=<?>,root_gb=1,rxtx_factor=1.0,swap=0,updated_at=None,vcpu_weight=0,vcpus=1) and image_meta {u'status': u'active', u'deleted': False, u'container_format': u'bare', u'min_ram': 0, u'updated_at': u'2015-12-18T08:55:33.000000', u'owner': u'e599088c985f42e7948b12f601705cd3', u'min_disk': 0, u'is_public': True, u'deleted_at': None, u'properties': {}, u'size': 13200896, u'name': u'cirros-0.3.3-x86_64', u'checksum': u'133eae9fb1c98f45894a4e60d8736619', u'created_at': u'2015-12-18T08:55:31.000000', u'disk_format': u'qcow2', u'id': u'db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5'} get_desirable_configs /usr/lib/python2.7/site-packages/nova/virt/hardware.py:502
2016-12-29 16:08:58.799 27161 DEBUG nova.virt.hardware [-] Flavor limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:296
2016-12-29 16:08:58.799 27161 DEBUG nova.virt.hardware [-] Image limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:309
2016-12-29 16:08:58.799 27161 DEBUG nova.virt.hardware [-] Flavor pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:332
2016-12-29 16:08:58.799 27161 DEBUG nova.virt.hardware [-] Image pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:354
2016-12-29 16:08:58.800 27161 DEBUG nova.virt.hardware [-] Chosen -1:-1:-1 limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:383
2016-12-29 16:08:58.800 27161 DEBUG nova.virt.hardware [-] Build topologies for 1 vcpu(s) 1:1:1 get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:419
2016-12-29 16:08:58.800 27161 DEBUG nova.virt.hardware [-] Got 1 possible topologies get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:442
2016-12-29 16:08:58.802 27161 DEBUG nova.virt.libvirt.vif [-] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-29T08:08:24Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='asdasd',display_name='asdasd',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute03',hostname='asdasd',id=205,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=2,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=None,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=512,metadata={},node='sjhl-o-compute03',numa_topology=None,os_type=None,pci_devices=PciDeviceList,power_state=0,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-iu52nhwd',root_device_name='/dev/vda',root_gb=1,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='1',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='1',instance_type_id='2',instance_type_memory_mb='512',instance_type_name='m1.tiny',instance_type_root_gb='1',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1',network_allocated='True'},task_state='spawning',terminated_at=None,updated_at=2016-12-29T08:08:55Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b,vcpus=1,vm_mode=None,vm_state='building') vif=VIF({'profile': {}, 'ovs_interfaceid': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.97'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape647790f-0f', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:9c:bc:87', 'active': True, 'type': u'ovs', 'id': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'qbg_params': None}) virt_typekvm get_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:342
2016-12-29 16:08:58.804 27161 DEBUG nova.virt.libvirt.config [-] Generated XML ('<domain type="kvm">\n  <uuid>6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</uuid>\n  <name>instance-000000cd</name>\n  <memory>524288</memory>\n  <vcpu cpuset="0-5,12-17">1</vcpu>\n  <metadata>\n    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">\n      <nova:package version="2014.2.2-1.el7"/>\n      <nova:name>asdasd</nova:name>\n      <nova:creationTime>2016-12-29 08:08:58</nova:creationTime>\n      <nova:flavor name="m1.tiny">\n        <nova:memory>512</nova:memory>\n        <nova:disk>1</nova:disk>\n        <nova:swap>0</nova:swap>\n        <nova:ephemeral>0</nova:ephemeral>\n        <nova:vcpus>1</nova:vcpus>\n      </nova:flavor>\n      <nova:owner>\n        <nova:user uuid="2a1a6dee31a0461c95a5ba8e7e1f30e1">fang</nova:user>\n        <nova:project uuid="516953ba1481400fb69642e9ffe14b02">fang</nova:project>\n      </nova:owner>\n      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>\n    </nova:instance>\n  </metadata>\n  <sysinfo type="smbios">\n    <system>\n      <entry name="manufacturer">Fedora Project</entry>\n      <entry name="product">OpenStack Nova</entry>\n      <entry name="version">2014.2.2-1.el7</entry>\n      <entry name="serial">a6dae68f-0d87-4f34-9359-73beb2000e47</entry>\n      <entry name="uuid">6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</entry>\n    </system>\n  </sysinfo>\n  <os>\n    <type>hvm</type>\n    <boot dev="hd"/>\n    <smbios mode="sysinfo"/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n  </features>\n  <clock offset="utc">\n    <timer name="pit" tickpolicy="delay"/>\n    <timer name="rtc" tickpolicy="catchup"/>\n    <timer name="hpet" present="no"/>\n  </clock>\n  <cpu mode="host-model" match="exact">\n    <topology sockets="1" cores="1" threads="1"/>\n  </cpu>\n  <devices>\n    <disk type="file" device="disk">\n      <driver name="qemu" type="qcow2" cache="none"/>\n      <source file="/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk"/>\n      <target bus="virtio" dev="vda"/>\n    </disk>\n    <interface type="bridge">\n      <mac address="fa:16:3e:9c:bc:87"/>\n      <model type="virtio"/>\n      <source bridge="qbre647790f-0f"/>\n      <target dev="tape647790f-0f"/>\n    </interface>\n    <serial type="file">\n      <source path="/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/console.log"/>\n    </serial>\n    <serial type="pty"/>\n    <input type="tablet" bus="usb"/>\n    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>\n    <video>\n      <model type="cirrus"/>\n    </video>\n    <memballoon model="virtio">\n      <stats period="10"/>\n    </memballoon>\n  </devices>\n</domain>\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82
2016-12-29 16:08:58.805 27161 DEBUG nova.virt.libvirt.driver [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] End _get_guest_xml xml=<domain type="kvm">
  <uuid>6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</uuid>
  <name>instance-000000cd</name>
  <memory>524288</memory>
  <vcpu cpuset="0-5,12-17">1</vcpu>
  <metadata>
    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">
      <nova:package version="2014.2.2-1.el7"/>
      <nova:name>asdasd</nova:name>
      <nova:creationTime>2016-12-29 08:08:58</nova:creationTime>
      <nova:flavor name="m1.tiny">
        <nova:memory>512</nova:memory>
        <nova:disk>1</nova:disk>
        <nova:swap>0</nova:swap>
        <nova:ephemeral>0</nova:ephemeral>
        <nova:vcpus>1</nova:vcpus>
      </nova:flavor>
      <nova:owner>
        <nova:user uuid="2a1a6dee31a0461c95a5ba8e7e1f30e1">fang</nova:user>
        <nova:project uuid="516953ba1481400fb69642e9ffe14b02">fang</nova:project>
      </nova:owner>
      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>
    </nova:instance>
  </metadata>
  <sysinfo type="smbios">
    <system>
      <entry name="manufacturer">Fedora Project</entry>
      <entry name="product">OpenStack Nova</entry>
      <entry name="version">2014.2.2-1.el7</entry>
      <entry name="serial">a6dae68f-0d87-4f34-9359-73beb2000e47</entry>
      <entry name="uuid">6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev="hd"/>
    <smbios mode="sysinfo"/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset="utc">
    <timer name="pit" tickpolicy="delay"/>
    <timer name="rtc" tickpolicy="catchup"/>
    <timer name="hpet" present="no"/>
  </clock>
  <cpu mode="host-model" match="exact">
    <topology sockets="1" cores="1" threads="1"/>
  </cpu>
  <devices>
    <disk type="file" device="disk">
      <driver name="qemu" type="qcow2" cache="none"/>
      <source file="/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk"/>
      <target bus="virtio" dev="vda"/>
    </disk>
    <interface type="bridge">
      <mac address="fa:16:3e:9c:bc:87"/>
      <model type="virtio"/>
      <source bridge="qbre647790f-0f"/>
      <target dev="tape647790f-0f"/>
    </interface>
    <serial type="file">
      <source path="/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/console.log"/>
    </serial>
    <serial type="pty"/>
    <input type="tablet" bus="usb"/>
    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>
    <video>
      <model type="cirrus"/>
    </video>
    <memballoon model="virtio">
      <stats period="10"/>
    </memballoon>
  </devices>
</domain>
 _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4168
2016-12-29 16:08:58.805 27161 DEBUG nova.virt.libvirt.vif [-] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-29T08:08:24Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='asdasd',display_name='asdasd',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute03',hostname='asdasd',id=205,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=2,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=None,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=512,metadata={},node='sjhl-o-compute03',numa_topology=None,os_type=None,pci_devices=PciDeviceList,power_state=0,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-iu52nhwd',root_device_name='/dev/vda',root_gb=1,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='1',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='1',instance_type_id='2',instance_type_memory_mb='512',instance_type_name='m1.tiny',instance_type_root_gb='1',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1',network_allocated='True'},task_state='spawning',terminated_at=None,updated_at=2016-12-29T08:08:55Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b,vcpus=1,vm_mode=None,vm_state='building') vif=VIF({'profile': {}, 'ovs_interfaceid': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.97'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape647790f-0f', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:9c:bc:87', 'active': True, 'type': u'ovs', 'id': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'qbg_params': None}) plug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:531
2016-12-29 16:08:58.806 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl addbr qbre647790f-0f execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:58.874 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:58.875 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl setfd qbre647790f-0f 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:58.947 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:58.947 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl stp qbre647790f-0f off execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.019 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.020 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf tee /sys/class/net/qbre647790f-0f/bridge/multicast_snooping execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.088 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.089 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link add qvbe647790f-0f type veth peer name qvoe647790f-0f execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.162 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.163 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qvbe647790f-0f up execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.230 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.231 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qvbe647790f-0f promisc on execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.296 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.297 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qvoe647790f-0f up execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.372 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.373 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qvoe647790f-0f promisc on execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.438 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.438 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qbre647790f-0f up execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.510 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.511 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl addif qbre647790f-0f qvbe647790f-0f execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.581 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.582 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl --timeout=120 -- --if-exists del-port qvoe647790f-0f -- add-port br-int qvoe647790f-0f -- set Interface qvoe647790f-0f external-ids:iface-id=e647790f-0ffe-44c6-b887-67635a8137a0 external-ids:iface-status=active external-ids:attached-mac=fa:16:3e:9c:bc:87 external-ids:vm-uuid=6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:08:59.669 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:08:59.990 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1482998939.99, 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b => Resumed> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-29 16:08:59.990 27161 INFO nova.compute.manager [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] VM Resumed (Lifecycle Event)
2016-12-29 16:08:59.992 27161 DEBUG nova.virt.libvirt.driver [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Instance is running spawn /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:2623
2016-12-29 16:08:59.995 27161 INFO nova.virt.libvirt.driver [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Instance spawned successfully.
2016-12-29 16:08:59.995 27161 DEBUG nova.compute.manager [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2016-12-29 16:09:00.059 27161 DEBUG nova.compute.manager [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Synchronizing instance power state after lifecycle event "Resumed"; current vm_state: building, current task_state: spawning, current DB power_state: 0, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-29 16:09:00.118 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:09:00.119 27161 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released "_locked_do_build_and_run_instance" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-29 16:09:00.122 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1482998939.99, 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b => Started> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-29 16:09:00.122 27161 INFO nova.compute.manager [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] VM Started (Lifecycle Event)
2016-12-29 16:09:00.195 27161 DEBUG nova.compute.manager [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Synchronizing instance power state after lifecycle event "Started"; current vm_state: active, current task_state: None, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108



下面是shutoff instance  的nova compute日志

2016-12-29 17:23:39.176 27161 DEBUG nova.openstack.common.lockutils [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 ] Created new semaphore "0df36b9f-1f4d-4f9f-9418-b38535afdda5" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:23:39.176 27161 DEBUG nova.openstack.common.lockutils [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 ] Acquired semaphore "0df36b9f-1f4d-4f9f-9418-b38535afdda5" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:23:39.176 27161 DEBUG nova.openstack.common.lockutils [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 ] Got semaphore / lock "do_stop_instance" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-29 17:23:39.177 27161 DEBUG nova.compute.manager [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159

2016-12-29 17:23:39.179 27161 DEBUG nova.compute.manager [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Stopping instance; current vm_state: active, current task_state: powering-off, current DB power_state: 1, current VM power_state: 1 do_stop_instance /usr/lib/python2.7/site-packages/nova/compute/manager.py:2548
2016-12-29 17:23:39.181 27161 DEBUG nova.virt.libvirt.driver [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Shutting down instance from state 1 _clean_shutdown /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:2432
2016-12-29 17:23:41.568 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483003421.57, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Stopped> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-29 17:23:41.569 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Stopped (Lifecycle Event)
2016-12-29 17:23:41.621 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Stopped"; current vm_state: active, current task_state: powering-off, current DB power_state: 1, VM power_state: 4 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-29 17:23:41.692 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] During sync_power_state the instance has a pending task (powering-off). Skip.
2016-12-29 17:23:42.199 27161 INFO nova.virt.libvirt.driver [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Instance shutdown successfully after 3 seconds.
2016-12-29 17:23:42.202 27161 INFO nova.virt.libvirt.driver [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Instance destroyed successfully.
2016-12-29 17:23:42.203 27161 DEBUG nova.compute.manager [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2016-12-29 17:23:42.278 27161 DEBUG nova.openstack.common.lockutils [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 ] Releasing semaphore "0df36b9f-1f4d-4f9f-9418-b38535afdda5" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:23:42.278 27161 DEBUG nova.openstack.common.lockutils [req-d7b54882-73e9-498f-bc8b-a9e1b6520f50 ] Semaphore / lock released "do_stop_instance" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-29 17:23:52.814 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 17:23:52.815 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 6.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132


下面是start instance的日志
2016-12-29 17:26:57.818 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 17:26:57.818 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 3.99 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-29 17:26:58.371 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "refresh_cache-0df36b9f-1f4d-4f9f-9418-b38535afdda5" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.371 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "refresh_cache-0df36b9f-1f4d-4f9f-9418-b38535afdda5" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.371 27161 DEBUG nova.network.neutronv2.api [req-83141424-6003-405a-bb2c-d3a2f032532b None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
2016-12-29 17:26:58.372 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.372 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.372 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.373 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.373 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.373 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.373 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.374 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.374 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.374 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.374 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.375 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.375 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=0df36b9f-1f4d-4f9f-9418-b38535afdda5 -X GET -H "X-Auth-Token: 65032166337b48ba8913f16d37367fee" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 17:26:58.499 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] RESP:200 {'date': 'Thu, 29 Dec 2016 09:26:58 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-d3126231-89ed-4d86-8cc9-d06d3f333fb8'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.98"}], "id": "e9bb122e-5719-4614-bacb-5c4bd727f5e6", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "0df36b9f-1f4d-4f9f-9418-b38535afdda5", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:73:bc:a3"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 17:26:58.499 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.499 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.500 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.500 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.500 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.500 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.501 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.501 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.501 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.501 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.501 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.502 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.502 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.502 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.502 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.502 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.503 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.503 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.503 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.503 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.503 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.504 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.504 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.504 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.504 27161 DEBUG nova.objects.instance [req-83141424-6003-405a-bb2c-d3a2f032532b None] Lazy-loading `info_cache' on Instance uuid 0df36b9f-1f4d-4f9f-9418-b38535afdda5 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-29 17:26:58.567 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/networks.json?id=d647355e-b019-4de0-999e-00107531edc0 -X GET -H "X-Auth-Token: f4a9eec6299440d6bbeef43f294f11db" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 17:26:58.589 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] RESP:200 {'date': 'Thu, 29 Dec 2016 09:26:58 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '269', 'x-openstack-request-id': 'req-b70c83d3-3419-4c8b-a3dc-98e67fba509a'} {"networks": [{"status": "ACTIVE", "subnets": ["b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"], "name": "DMZ_NET", "router:external": false, "tenant_id": "e599088c985f42e7948b12f601705cd3", "admin_state_up": true, "shared": true, "id": "d647355e-b019-4de0-999e-00107531edc0"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 17:26:58.589 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.589 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.590 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.590 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.590 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.590 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.591 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.591 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.591 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.591 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/floatingips.json?fixed_ip_address=192.168.34.98&port_id=e9bb122e-5719-4614-bacb-5c4bd727f5e6 -X GET -H "X-Auth-Token: 65032166337b48ba8913f16d37367fee" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 17:26:58.601 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] RESP:200 {'date': 'Thu, 29 Dec 2016 09:26:58 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '19', 'x-openstack-request-id': 'req-2d5abf86-8cff-43e9-978c-3d47e23b9054'} {"floatingips": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 17:26:58.601 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.601 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.601 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.602 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.602 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.602 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.602 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.603 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.603 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.603 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.603 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.603 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.604 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.604 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.604 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.604 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.605 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.605 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.605 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.605 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.605 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.606 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:26:58.606 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:26:58.606 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.606 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/subnets.json?id=b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 -X GET -H "X-Auth-Token: f4a9eec6299440d6bbeef43f294f11db" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 17:26:58.627 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] RESP:200 {'date': 'Thu, 29 Dec 2016 09:26:58 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '519', 'x-openstack-request-id': 'req-0c596b56-f9cc-4534-b17b-9c1bc70b6f00'} {"subnets": [{"name": "DMZ_SUBNET", "enable_dhcp": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "dns_nameservers": ["218.30.110.17", "8.8.8.8"], "gateway_ip": null, "ipv6_ra_mode": null, "allocation_pools": [{"start": "192.168.34.3", "end": "192.168.34.250"}], "host_routes": [{"nexthop": "192.168.34.254", "destination": "0.0.0.0/0"}], "ip_version": 4, "ipv6_address_mode": null, "cidr": "192.168.34.0/24", "id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 17:26:58.627 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?network_id=d647355e-b019-4de0-999e-00107531edc0&device_owner=network%3Adhcp -X GET -H "X-Auth-Token: f4a9eec6299440d6bbeef43f294f11db" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 17:26:58.648 27161 DEBUG neutronclient.client [req-83141424-6003-405a-bb2c-d3a2f032532b ] RESP:200 {'date': 'Thu, 29 Dec 2016 09:26:58 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '13', 'x-openstack-request-id': 'req-851d544c-9047-49c1-90ea-2418d1bb41d6'} {"ports": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 17:26:58.649 27161 DEBUG nova.network.base_api [req-83141424-6003-405a-bb2c-d3a2f032532b None] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.98'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape9bb122e-57', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:73:bc:a3', 'active': False, 'type': u'ovs', 'id': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2016-12-29 17:26:58.664 27161 DEBUG nova.openstack.common.lockutils [req-83141424-6003-405a-bb2c-d3a2f032532b ] Releasing semaphore "refresh_cache-0df36b9f-1f4d-4f9f-9418-b38535afdda5" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:26:58.682 27161 INFO nova.virt.libvirt.driver [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Instance destroyed successfully.
2016-12-29 17:26:58.683 27161 DEBUG nova.block_device [req-83141424-6003-405a-bb2c-d3a2f032532b None] block_device_list [] volume_in_mapping /usr/lib/python2.7/site-packages/nova/block_device.py:555
2016-12-29 17:26:58.683 27161 DEBUG nova.virt.libvirt.driver [req-83141424-6003-405a-bb2c-d3a2f032532b None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Start _get_guest_xml network_info=[VIF({'profile': {}, 'ovs_interfaceid': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.98'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape9bb122e-57', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:73:bc:a3', 'active': False, 'type': u'ovs', 'id': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'qbg_params': None})] disk_info={'disk_bus': 'virtio', 'cdrom_bus': 'ide', 'mapping': {'disk': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': 'vda'}, 'root': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': 'vda'}}} image_meta={u'min_disk': u'1', u'container_format': u'bare', u'min_ram': u'0', u'disk_format': u'qcow2', 'properties': {u'instance_type_memory_mb': u'512', u'instance_type_swap': u'0', u'instance_type_root_gb': u'1', u'instance_type_name': u'm1.tiny', u'instance_type_id': u'2', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'instance_type_flavorid': u'1', u'instance_type_vcpus': u'1', u'base_image_ref': u'db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5'}} rescue=None block_device_info={'swap': None, 'ephemerals': [], 'block_device_mapping': []} _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4156
2016-12-29 17:26:58.711 27161 DEBUG nova.objects.instance [req-83141424-6003-405a-bb2c-d3a2f032532b None] Lazy-loading `numa_topology' on Instance uuid 0df36b9f-1f4d-4f9f-9418-b38535afdda5 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-29 17:26:58.723 27161 DEBUG nova.virt.libvirt.driver [req-83141424-6003-405a-bb2c-d3a2f032532b None] CPU mode 'host-model' model '' was chosen _get_guest_cpu_model_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:3362
2016-12-29 17:26:58.724 27161 DEBUG nova.virt.hardware [req-83141424-6003-405a-bb2c-d3a2f032532b None] Getting desirable topologies for flavor Flavor(created_at=None,deleted=False,deleted_at=None,disabled=False,ephemeral_gb=0,extra_specs={},flavorid='1',id=2,is_public=True,memory_mb=512,name='m1.tiny',projects=<?>,root_gb=1,rxtx_factor=1.0,swap=0,updated_at=None,vcpu_weight=0,vcpus=1) and image_meta {u'min_disk': u'1', u'container_format': u'bare', u'min_ram': u'0', u'disk_format': u'qcow2', 'properties': {u'instance_type_memory_mb': u'512', u'instance_type_swap': u'0', u'instance_type_root_gb': u'1', u'instance_type_name': u'm1.tiny', u'instance_type_id': u'2', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'instance_type_flavorid': u'1', u'instance_type_vcpus': u'1', u'base_image_ref': u'db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5'}} get_desirable_configs /usr/lib/python2.7/site-packages/nova/virt/hardware.py:502
2016-12-29 17:26:58.725 27161 DEBUG nova.virt.hardware [req-83141424-6003-405a-bb2c-d3a2f032532b None] Flavor limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:296
2016-12-29 17:26:58.725 27161 DEBUG nova.virt.hardware [req-83141424-6003-405a-bb2c-d3a2f032532b None] Image limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:309
2016-12-29 17:26:58.725 27161 DEBUG nova.virt.hardware [req-83141424-6003-405a-bb2c-d3a2f032532b None] Flavor pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:332
2016-12-29 17:26:58.726 27161 DEBUG nova.virt.hardware [req-83141424-6003-405a-bb2c-d3a2f032532b None] Image pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:354
2016-12-29 17:26:58.726 27161 DEBUG nova.virt.hardware [req-83141424-6003-405a-bb2c-d3a2f032532b None] Chosen -1:-1:-1 limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:383
2016-12-29 17:26:58.727 27161 DEBUG nova.virt.hardware [req-83141424-6003-405a-bb2c-d3a2f032532b None] Build topologies for 1 vcpu(s) 1:1:1 get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:419
2016-12-29 17:26:58.727 27161 DEBUG nova.virt.hardware [req-83141424-6003-405a-bb2c-d3a2f032532b None] Got 1 possible topologies get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:442
2016-12-29 17:26:58.730 27161 DEBUG nova.virt.libvirt.vif [req-83141424-6003-405a-bb2c-d3a2f032532b None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-29T08:12:39Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='asdsad',display_name='asdsad',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute03',hostname='asdsad',id=206,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=2,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2016-12-29T08:13:14Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=512,metadata={},node='sjhl-o-compute03',numa_topology=None,os_type=None,pci_devices=<?>,power_state=4,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-ljtrxt50',root_device_name='/dev/vda',root_gb=1,scheduled_at=None,security_groups=<?>,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='1',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='1',instance_type_id='2',instance_type_memory_mb='512',instance_type_name='m1.tiny',instance_type_root_gb='1',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1'},task_state='powering-on',terminated_at=None,updated_at=2016-12-29T09:26:58Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=0df36b9f-1f4d-4f9f-9418-b38535afdda5,vcpus=1,vm_mode=None,vm_state='stopped') vif=VIF({'profile': {}, 'ovs_interfaceid': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.98'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape9bb122e-57', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:73:bc:a3', 'active': False, 'type': u'ovs', 'id': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'qbg_params': None}) virt_typekvm get_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:342
2016-12-29 17:26:58.733 27161 DEBUG nova.objects.instance [req-83141424-6003-405a-bb2c-d3a2f032532b None] Lazy-loading `pci_devices' on Instance uuid 0df36b9f-1f4d-4f9f-9418-b38535afdda5 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-29 17:26:58.794 27161 DEBUG nova.virt.libvirt.config [req-83141424-6003-405a-bb2c-d3a2f032532b None] Generated XML ('<domain type="kvm">\n  <uuid>0df36b9f-1f4d-4f9f-9418-b38535afdda5</uuid>\n  <name>instance-000000ce</name>\n  <memory>524288</memory>\n  <vcpu cpuset="0-5,12-17">1</vcpu>\n  <metadata>\n    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">\n      <nova:package version="2014.2.2-1.el7"/>\n      <nova:name>asdsad</nova:name>\n      <nova:creationTime>2016-12-29 09:26:58</nova:creationTime>\n      <nova:flavor name="m1.tiny">\n        <nova:memory>512</nova:memory>\n        <nova:disk>1</nova:disk>\n        <nova:swap>0</nova:swap>\n        <nova:ephemeral>0</nova:ephemeral>\n        <nova:vcpus>1</nova:vcpus>\n      </nova:flavor>\n      <nova:owner>\n        <nova:user uuid="2a1a6dee31a0461c95a5ba8e7e1f30e1">fang</nova:user>\n        <nova:project uuid="516953ba1481400fb69642e9ffe14b02">fang</nova:project>\n      </nova:owner>\n      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>\n    </nova:instance>\n  </metadata>\n  <sysinfo type="smbios">\n    <system>\n      <entry name="manufacturer">Fedora Project</entry>\n      <entry name="product">OpenStack Nova</entry>\n      <entry name="version">2014.2.2-1.el7</entry>\n      <entry name="serial">a6dae68f-0d87-4f34-9359-73beb2000e47</entry>\n      <entry name="uuid">0df36b9f-1f4d-4f9f-9418-b38535afdda5</entry>\n    </system>\n  </sysinfo>\n  <os>\n    <type>hvm</type>\n    <boot dev="hd"/>\n    <smbios mode="sysinfo"/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n  </features>\n  <clock offset="utc">\n    <timer name="pit" tickpolicy="delay"/>\n    <timer name="rtc" tickpolicy="catchup"/>\n    <timer name="hpet" present="no"/>\n  </clock>\n  <cpu mode="host-model" match="exact">\n    <topology sockets="1" cores="1" threads="1"/>\n  </cpu>\n  <devices>\n    <disk type="file" device="disk">\n      <driver name="qemu" type="qcow2" cache="none"/>\n      <source file="/var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/disk"/>\n      <target bus="virtio" dev="vda"/>\n    </disk>\n    <interface type="bridge">\n      <mac address="fa:16:3e:73:bc:a3"/>\n      <model type="virtio"/>\n      <source bridge="qbre9bb122e-57"/>\n      <target dev="tape9bb122e-57"/>\n    </interface>\n    <serial type="file">\n      <source path="/var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/console.log"/>\n    </serial>\n    <serial type="pty"/>\n    <input type="tablet" bus="usb"/>\n    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>\n    <video>\n      <model type="cirrus"/>\n    </video>\n    <memballoon model="virtio">\n      <stats period="10"/>\n    </memballoon>\n  </devices>\n</domain>\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82
2016-12-29 17:26:58.794 27161 DEBUG nova.virt.libvirt.driver [req-83141424-6003-405a-bb2c-d3a2f032532b None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] End _get_guest_xml xml=<domain type="kvm">
  <uuid>0df36b9f-1f4d-4f9f-9418-b38535afdda5</uuid>
  <name>instance-000000ce</name>
  <memory>524288</memory>
  <vcpu cpuset="0-5,12-17">1</vcpu>
  <metadata>
    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">
      <nova:package version="2014.2.2-1.el7"/>
      <nova:name>asdsad</nova:name>
      <nova:creationTime>2016-12-29 09:26:58</nova:creationTime>
      <nova:flavor name="m1.tiny">
        <nova:memory>512</nova:memory>
        <nova:disk>1</nova:disk>
        <nova:swap>0</nova:swap>
        <nova:ephemeral>0</nova:ephemeral>
        <nova:vcpus>1</nova:vcpus>
      </nova:flavor>
      <nova:owner>
        <nova:user uuid="2a1a6dee31a0461c95a5ba8e7e1f30e1">fang</nova:user>
        <nova:project uuid="516953ba1481400fb69642e9ffe14b02">fang</nova:project>
      </nova:owner>
      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>
    </nova:instance>
  </metadata>
  <sysinfo type="smbios">
    <system>
      <entry name="manufacturer">Fedora Project</entry>
      <entry name="product">OpenStack Nova</entry>
      <entry name="version">2014.2.2-1.el7</entry>
      <entry name="serial">a6dae68f-0d87-4f34-9359-73beb2000e47</entry>
      <entry name="uuid">0df36b9f-1f4d-4f9f-9418-b38535afdda5</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev="hd"/>
    <smbios mode="sysinfo"/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset="utc">
    <timer name="pit" tickpolicy="delay"/>
    <timer name="rtc" tickpolicy="catchup"/>
    <timer name="hpet" present="no"/>
  </clock>
  <cpu mode="host-model" match="exact">
    <topology sockets="1" cores="1" threads="1"/>
  </cpu>
  <devices>
    <disk type="file" device="disk">
      <driver name="qemu" type="qcow2" cache="none"/>
      <source file="/var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/disk"/>
      <target bus="virtio" dev="vda"/>
    </disk>
    <interface type="bridge">
      <mac address="fa:16:3e:73:bc:a3"/>
      <model type="virtio"/>
      <source bridge="qbre9bb122e-57"/>
      <target dev="tape9bb122e-57"/>
    </interface>
    <serial type="file">
      <source path="/var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/console.log"/>
    </serial>
    <serial type="pty"/>
    <input type="tablet" bus="usb"/>
    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>
    <video>
      <model type="cirrus"/>
    </video>
    <memballoon model="virtio">
      <stats period="10"/>
    </memballoon>
  </devices>
</domain>
 _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4168
2016-12-29 17:26:58.795 27161 DEBUG nova.openstack.common.processutils [req-83141424-6003-405a-bb2c-d3a2f032532b None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 17:26:58.863 27161 DEBUG nova.openstack.common.processutils [req-83141424-6003-405a-bb2c-d3a2f032532b None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 17:26:58.864 27161 DEBUG nova.openstack.common.processutils [req-83141424-6003-405a-bb2c-d3a2f032532b None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 17:26:58.924 27161 DEBUG nova.openstack.common.processutils [req-83141424-6003-405a-bb2c-d3a2f032532b None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 17:26:58.925 27161 DEBUG nova.virt.libvirt.vif [req-83141424-6003-405a-bb2c-d3a2f032532b None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-29T08:12:39Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='asdsad',display_name='asdsad',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute03',hostname='asdsad',id=206,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=2,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2016-12-29T08:13:14Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=512,metadata={},node='sjhl-o-compute03',numa_topology=None,os_type=None,pci_devices=PciDeviceList,power_state=4,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-ljtrxt50',root_device_name='/dev/vda',root_gb=1,scheduled_at=None,security_groups=<?>,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='1',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='1',instance_type_id='2',instance_type_memory_mb='512',instance_type_name='m1.tiny',instance_type_root_gb='1',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1'},task_state='powering-on',terminated_at=None,updated_at=2016-12-29T09:26:58Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=0df36b9f-1f4d-4f9f-9418-b38535afdda5,vcpus=1,vm_mode=None,vm_state='stopped') vif=VIF({'profile': {}, 'ovs_interfaceid': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.98'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape9bb122e-57', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:73:bc:a3', 'active': False, 'type': u'ovs', 'id': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'qbg_params': None}) plug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:531
2016-12-29 17:26:59.274 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483003619.27, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Resumed> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-29 17:26:59.275 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Resumed (Lifecycle Event)
2016-12-29 17:26:59.279 27161 INFO nova.virt.libvirt.driver [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Instance rebooted successfully.
2016-12-29 17:26:59.279 27161 DEBUG nova.compute.manager [req-83141424-6003-405a-bb2c-d3a2f032532b None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2016-12-29 17:26:59.327 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Resumed"; current vm_state: stopped, current task_state: powering-on, current DB power_state: 4, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-29 17:26:59.389 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483003619.28, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Started> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-29 17:26:59.389 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Started (Lifecycle Event)
2016-12-29 17:26:59.445 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Started"; current vm_state: active, current task_state: None, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-29 17:27:01.813 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._reclaim_queued_deletes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 17:27:01.814 27161 DEBUG nova.compute.manager [-] CONF.reclaim_instance_interval <= 0, skipping... _reclaim_queued_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:5932
2016-12-29 17:27:01.814 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 1.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-29 17:27:02.814 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._heal_instance_info_cache run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 17:27:02.814 27161 DEBUG nova.compute.manager [-] Starting heal instance info cache _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5289
2016-12-29 17:27:02.896 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "refresh_cache-5d2bbe60-9400-459e-982d-ef6fb8971748" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.897 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "refresh_cache-5d2bbe60-9400-459e-982d-ef6fb8971748" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.897 27161 DEBUG nova.network.neutronv2.api [-] [instance: 5d2bbe60-9400-459e-982d-ef6fb8971748] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
2016-12-29 17:27:02.897 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.897 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.898 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.898 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.898 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.899 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.899 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.899 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.899 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.900 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.900 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.900 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.901 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=5d2bbe60-9400-459e-982d-ef6fb8971748 -X GET -H "X-Auth-Token: 65032166337b48ba8913f16d37367fee" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 17:27:02.926 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Thu, 29 Dec 2016 09:27:02 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '743', 'x-openstack-request-id': 'req-5a4e250d-44a1-49fa-8c7c-c7adbb3f167e'} {"ports": [{"status": "ACTIVE", "binding:host_id": "sjhl-o-compute03", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.54"}], "id": "2dc0110f-3162-40da-8026-8a451716c21f", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "5d2bbe60-9400-459e-982d-ef6fb8971748", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:1a:98:12"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 17:27:02.926 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.926 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.927 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.927 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.927 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.927 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.927 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.928 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.928 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.928 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.928 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.928 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.929 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.929 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.929 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.929 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.929 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.930 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.930 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.930 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.930 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.930 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.931 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.931 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.931 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.931 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.931 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.932 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.932 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.932 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.932 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.932 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.933 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.933 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.933 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.933 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.933 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/networks.json?id=d647355e-b019-4de0-999e-00107531edc0 -X GET -H "X-Auth-Token: 65032166337b48ba8913f16d37367fee" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 17:27:02.955 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Thu, 29 Dec 2016 09:27:02 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '376', 'x-openstack-request-id': 'req-e3a7a029-dd92-45c4-a38e-5828db105e65'} {"networks": [{"status": "ACTIVE", "subnets": ["b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"], "name": "DMZ_NET", "provider:physical_network": "physnet1", "admin_state_up": true, "tenant_id": "e599088c985f42e7948b12f601705cd3", "provider:network_type": "vlan", "router:external": false, "shared": true, "id": "d647355e-b019-4de0-999e-00107531edc0", "provider:segmentation_id": 502}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 17:27:02.955 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.955 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.955 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.956 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.956 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.956 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.957 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.957 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.957 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.957 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.957 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.958 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.958 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.958 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.958 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.958 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.959 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.959 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.959 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.959 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.959 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.960 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.960 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.960 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.960 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.960 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.961 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.961 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.961 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.961 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.961 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 17:27:02.962 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 17:27:02.962 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 17:27:02.962 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/floatingips.json?fixed_ip_address=192.168.34.54&port_id=2dc0110f-3162-40da-8026-8a451716c21f -X GET -H "X-Auth-Token: 65032166337b48ba8913f16d37367fee" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140



下面是teminate instance  的nova compute日志

2016-12-29 16:00:01.506 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Created new semaphore "39e88c01-79d8-4cdf-98d9-04e246004d5b" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:00:01.507 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Acquired semaphore "39e88c01-79d8-4cdf-98d9-04e246004d5b" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:00:01.507 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Got semaphore / lock "do_terminate_instance" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-29 16:00:01.507 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Created new semaphore "39e88c01-79d8-4cdf-98d9-04e246004d5b-events" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:00:01.507 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Acquired semaphore "39e88c01-79d8-4cdf-98d9-04e246004d5b-events" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:00:01.508 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Got semaphore / lock "_clear_events" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-29 16:00:01.508 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Releasing semaphore "39e88c01-79d8-4cdf-98d9-04e246004d5b-events" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:00:01.508 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Semaphore / lock released "_clear_events" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-29 16:00:01.523 27161 AUDIT nova.compute.manager [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] Terminating instance
2016-12-29 16:00:01.729 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1482998401.73, 39e88c01-79d8-4cdf-98d9-04e246004d5b => Stopped> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-29 16:00:01.730 27161 INFO nova.compute.manager [-] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] VM Stopped (Lifecycle Event)
2016-12-29 16:00:01.733 27161 INFO nova.virt.libvirt.driver [-] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] Instance destroyed successfully.
2016-12-29 16:00:01.734 27161 DEBUG nova.virt.libvirt.vif [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-29T07:51:06Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='12229',display_name='12229',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute03',hostname='12229',id=203,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=2,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2016-12-29T07:51:42Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=512,metadata={},node='sjhl-o-compute03',numa_topology=<?>,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='e599088c985f42e7948b12f601705cd3',ramdisk_id='',reservation_id='r-85dxvvw0',root_device_name='/dev/vda',root_gb=1,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='1',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='1',instance_type_id='2',instance_type_memory_mb='512',instance_type_name='m1.tiny',instance_type_root_gb='1',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1'},task_state='deleting',terminated_at=None,updated_at=2016-12-29T08:00:01Z,user_data=None,user_id='7d5b5abc30ea463690567e5f8cc794f9',uuid=39e88c01-79d8-4cdf-98d9-04e246004d5b,vcpus=1,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'9a94a24e-1918-4c7c-82bd-04be42c96da0', 'network': Network({'bridge': u'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': u'fixed', 'floating_ips': [], 'address': u'192.168.34.95'})], 'version': 4, 'meta': {u'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': u'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': u'gateway', 'address': None})})], 'meta': {u'injected': False, u'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tap9a94a24e-19', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:2f:4f:ce', 'active': True, 'type': u'ovs', 'id': u'9a94a24e-1918-4c7c-82bd-04be42c96da0', 'qbg_params': None}) unplug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:678
2016-12-29 16:00:01.736 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delif qbr9a94a24e-19 qvb9a94a24e-19 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:01.790 27161 DEBUG nova.compute.manager [req-204dd003-cee9-4419-aa04-49824db2e755 None] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] Synchronizing instance power state after lifecycle event "Stopped"; current vm_state: active, current task_state: deleting, current DB power_state: 1, VM power_state: 4 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-29 16:00:01.801 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:01.802 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qbr9a94a24e-19 down execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:01.857 27161 INFO nova.compute.manager [req-204dd003-cee9-4419-aa04-49824db2e755 None] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] During sync_power_state the instance has a pending task (deleting). Skip.
2016-12-29 16:00:01.892 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:01.892 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delbr qbr9a94a24e-19 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:01.984 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:01.985 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl --timeout=120 -- --if-exists del-port br-int qvo9a94a24e-19 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:02.065 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:02.066 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link delete qvo9a94a24e-19 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:02.148 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:02.148 27161 DEBUG nova.network.linux_net [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Net device removed: 'qvo9a94a24e-19' delete_net_dev /usr/lib/python2.7/site-packages/nova/network/linux_net.py:1380
2016-12-29 16:00:02.204 27161 DEBUG nova.objects.instance [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Lazy-loading `system_metadata' on Instance uuid 39e88c01-79d8-4cdf-98d9-04e246004d5b obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-29 16:00:02.265 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Running cmd (subprocess): mv /var/lib/nova/instances/39e88c01-79d8-4cdf-98d9-04e246004d5b /var/lib/nova/instances/39e88c01-79d8-4cdf-98d9-04e246004d5b_del execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:02.274 27161 DEBUG nova.openstack.common.processutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:02.275 27161 INFO nova.virt.libvirt.driver [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] Deleting instance files /var/lib/nova/instances/39e88c01-79d8-4cdf-98d9-04e246004d5b_del
2016-12-29 16:00:02.275 27161 INFO nova.virt.libvirt.driver [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] Deletion of /var/lib/nova/instances/39e88c01-79d8-4cdf-98d9-04e246004d5b_del complete
2016-12-29 16:00:02.353 27161 DEBUG nova.compute.manager [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] Deallocating network for instance _deallocate_network /usr/lib/python2.7/site-packages/nova/compute/manager.py:1919
2016-12-29 16:00:02.354 27161 DEBUG nova.network.neutronv2.api [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] deallocate_for_instance() deallocate_for_instance /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:528
2016-12-29 16:00:02.354 27161 DEBUG neutronclient.client [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?device_id=39e88c01-79d8-4cdf-98d9-04e246004d5b -X GET -H "X-Auth-Token: bbca34fdb7a4477882d29b19bdb48ab1" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 16:00:02.390 27161 DEBUG neutronclient.client [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] RESP:200 {'date': 'Thu, 29 Dec 2016 08:00:02 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-52915b21-e7b2-4e3b-b6af-470ff297bf3b'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.95"}], "id": "9a94a24e-1918-4c7c-82bd-04be42c96da0", "security_groups": ["b20cac25-ec58-4830-b7f3-888a54ce1391"], "device_id": "39e88c01-79d8-4cdf-98d9-04e246004d5b", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:2f:4f:ce"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 16:00:02.391 27161 DEBUG neutronclient.client [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports/9a94a24e-1918-4c7c-82bd-04be42c96da0.json -X DELETE -H "X-Auth-Token: bbca34fdb7a4477882d29b19bdb48ab1" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-29 16:00:02.577 27161 DEBUG neutronclient.client [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] RESP:204 {'date': 'Thu, 29 Dec 2016 08:00:02 GMT', 'connection': 'keep-alive', 'content-length': '0', 'x-openstack-request-id': 'req-aeb21a18-3834-41a6-bc22-05c11add9b46'} 
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-29 16:00:02.578 27161 DEBUG nova.network.base_api [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Updating cache with info: [] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2016-12-29 16:00:02.592 27161 DEBUG nova.compute.manager [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] [instance: 39e88c01-79d8-4cdf-98d9-04e246004d5b] terminating bdm BlockDeviceMapping(boot_index=0,connection_info=None,created_at=2016-12-29T07:51:06Z,delete_on_termination=True,deleted=False,deleted_at=None,destination_type='local',device_name='/dev/vda',device_type='disk',disk_bus=None,guest_format=None,id=283,image_id='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',instance=<?>,instance_uuid=39e88c01-79d8-4cdf-98d9-04e246004d5b,no_device=False,snapshot_id=None,source_type='image',updated_at=2016-12-29T07:51:07Z,volume_id=None,volume_size=None) _cleanup_volumes /usr/lib/python2.7/site-packages/nova/compute/manager.py:2422
2016-12-29 16:00:02.670 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:00:02.671 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:00:02.671 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Got semaphore / lock "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-29 16:00:02.671 27161 DEBUG nova.objects.instance [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Lazy-loading `numa_topology' on Instance uuid 39e88c01-79d8-4cdf-98d9-04e246004d5b obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-29 16:00:02.715 27161 INFO nova.scheduler.client.report [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 None] Compute_service record updated for ('sjhl-o-compute03', 'sjhl-o-compute03')
2016-12-29 16:00:02.715 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:00:02.716 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Semaphore / lock released "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-29 16:00:02.873 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Releasing semaphore "39e88c01-79d8-4cdf-98d9-04e246004d5b" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:00:02.874 27161 DEBUG nova.openstack.common.lockutils [req-bdf48c97-e90c-45d7-b519-f476f17e92f8 ] Semaphore / lock released "do_terminate_instance" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-29 16:00:02.941 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/456c88aa-407b-4ea4-81f8-41d0f72a5395/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:03.004 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:03.005 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/456c88aa-407b-4ea4-81f8-41d0f72a5395/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:03.067 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:03.068 27161 DEBUG nova.virt.libvirt.driver [-] skipping disk for instance-00000054 as it does not have a path _get_instance_disk_info /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:5689
2016-12-29 16:00:03.070 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/5d2bbe60-9400-459e-982d-ef6fb8971748/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:03.143 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:03.144 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/5d2bbe60-9400-459e-982d-ef6fb8971748/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:03.205 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:03.206 27161 DEBUG nova.virt.libvirt.driver [-] skipping disk for instance-0000009b as it does not have a path _get_instance_disk_info /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:5689
2016-12-29 16:00:03.208 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d2ed6f28-8e5a-428b-bea6-d9ec89e5da1c/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:03.287 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:03.288 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d2ed6f28-8e5a-428b-bea6-d9ec89e5da1c/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:03.352 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:03.355 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:03.429 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:03.430 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-29 16:00:03.492 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-29 16:00:03.493 27161 DEBUG nova.virt.libvirt.driver [-] skipping disk for instance-000000c6 as it does not have a path _get_instance_disk_info /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:5689
2016-12-29 16:00:03.601 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: free ram (MB): 123672 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:454
2016-12-29 16:00:03.602 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: free disk (GB): 524 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:455
2016-12-29 16:00:03.602 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: free VCPUs: -37 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:460
2016-12-29 16:00:03.602 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: assignable PCI devices: [] _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:467
2016-12-29 16:00:03.602 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-29 16:00:03.602 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-29 16:00:03.603 27161 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-29 16:00:03.756 27161 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 114816
2016-12-29 16:00:03.757 27161 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 307
2016-12-29 16:00:03.757 27161 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 61
2016-12-29 16:00:03.757 27161 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2016-12-29 16:00:03.778 27161 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute03', 'sjhl-o-compute03')
2016-12-29 16:00:03.778 27161 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute03:sjhl-o-compute03
2016-12-29 16:00:03.778 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-29 16:00:03.779 27161 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-29 16:00:03.840 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rebooting_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 16:00:03.841 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_volume_usage run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 16:00:03.841 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._reclaim_queued_deletes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 16:00:03.841 27161 DEBUG nova.compute.manager [-] CONF.reclaim_instance_interval <= 0, skipping... _reclaim_queued_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:5932
2016-12-29 16:00:03.842 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_unconfirmed_resizes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 16:00:03.842 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rescued_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-29 16:00:03.843 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 39.97 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132


Pause/Resume暂停恢复操作   pause==中止实例
Pause短时间停止虚拟机，将虚拟机的状态保存到宿主机内存中。当需要恢复的时候执行Resume操作，从内存中读回instance的状态	

2016-12-30 09:47:36.236 27161 DEBUG nova.compute.manager [-] [instance: 456c88aa-407b-4ea4-81f8-41d0f72a5395] Updated the network info_cache for instance _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5350
2016-12-30 09:47:36.237 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 23.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 09:47:52.860 27161 AUDIT nova.compute.manager [req-8c1210f6-d482-4e46-94a7-f5c5647ca527 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Pausing
2016-12-30 09:47:52.867 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483062472.87, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Paused> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 09:47:52.867 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Paused (Lifecycle Event)
2016-12-30 09:47:52.868 27161 DEBUG nova.compute.manager [req-8c1210f6-d482-4e46-94a7-f5c5647ca527 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2016-12-30 09:47:52.933 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Paused"; current vm_state: active, current task_state: pausing, current DB power_state: 1, VM power_state: 3 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108

2016-12-30 09:47:53.001 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483062472.87, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Paused> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 09:47:53.001 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Paused (Lifecycle Event)
2016-12-30 09:47:53.073 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Paused"; current vm_state: paused, current task_state: None, current DB power_state: 3, VM power_state: 3 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108


Resume
2016-12-30 09:50:33.815 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rebooting_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 09:50:33.815 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 8.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 09:50:38.875 27161 AUDIT nova.compute.manager [req-869eccfa-a981-4708-b94f-a77521e844b4 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Unpausing
2016-12-30 09:50:38.907 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483062638.91, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Resumed> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 09:50:38.908 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Resumed (Lifecycle Event)
2016-12-30 09:50:38.909 27161 DEBUG nova.compute.manager [req-869eccfa-a981-4708-b94f-a77521e844b4 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2016-12-30 09:50:38.968 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Resumed"; current vm_state: paused, current task_state: unpausing, current DB power_state: 3, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 09:50:39.035 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483062638.91, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Resumed> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 09:50:39.035 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Resumed (Lifecycle Event)
2016-12-30 09:50:39.089 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Resumed"; current vm_state: active, current task_state: None, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 09:50:41.813 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._heal_instance_info_cache run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 09:50:41.813 27161 DEBUG nova.compute.manager [-] Starting heal instance info cache _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5289


Suspend instance 
有时需要长时间暂停 instance，可以通过 Suspend 操作将 instance 的状态保存到宿主机的磁盘上。当需要恢复的时候，执行 Resume 操作，从磁盘读回 instance 的状态，使之继续运行。					 
相同点
1 两者都是暂停 instance 的运行，并保存当前状态，之后可以通过 Resume 操作恢复

不同点
1. Suspend 将 instance 的状态保存在磁盘上；Pause 是保存在内存中，所以 Resume 被 Pause 的 instance 要比 Suspend 快。 2. Suspend 之后的 instance，其状态是 Shut Down；而被 Pause 的 instance 状态是Paused。 3. 虽然都是通过 Resume 操作恢复，Pause 对应的 Resume 在 OpenStack 内部被叫作 “Unpause”；Suspend 对应的 Resume 才是真正的 “Resume”。这个在日志中能体现出来。
2016-12-30 09:55:52.433 27161 DEBUG nova.objects.instance [req-10a7b40c-69fe-495c-9e24-827a1d2cd760 None] Lazy-loading `pci_devices' on Instance uuid 0df36b9f-1f4d-4f9f-9418-b38535afdda5 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 09:55:52.549 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483062952.55, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Paused> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 09:55:52.549 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Paused (Lifecycle Event)
2016-12-30 09:55:52.628 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Paused"; current vm_state: active, current task_state: suspending, current DB power_state: 1, VM power_state: 3 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 09:55:52.717 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] During sync_power_state the instance has a pending task (suspending). Skip.
2016-12-30 09:55:53.354 27161 DEBUG nova.compute.manager [req-10a7b40c-69fe-495c-9e24-827a1d2cd760 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2016-12-30 09:55:53.355 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483062953.35, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Stopped> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 09:55:53.355 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Stopped (Lifecycle Event)
2016-12-30 09:55:53.427 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Stopped"; current vm_state: active, current task_state: suspending, current DB power_state: 1, VM power_state: 4 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 09:55:53.511 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] During sync_power_state the instance has a pending task (suspending). Skip.

resumed
f36b9f-1f4d-4f9f-9418-b38535afdda5 => Started> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 09:57:33.631 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Started (Lifecycle Event)
2016-12-30 09:57:33.647 27161 DEBUG nova.objects.instance [req-d0d321d4-f136-4338-ad86-627326b5e8e6 None] Lazy-loading `pci_devices' on Instance uuid 0df36b9f-1f4d-4f9f-9418-b38535afdda5 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 09:57:33.695 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Started"; current vm_state: suspended, current task_state: resuming, current DB power_state: 4, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 09:57:33.722 27161 DEBUG nova.compute.manager [req-d0d321d4-f136-4338-ad86-627326b5e8e6 None] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2016-12-30 09:57:33.756 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] During sync_power_state the instance has a pending task (resuming). Skip.
2016-12-30 09:57:33.756 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483063053.63, 0df36b9f-1f4d-4f9f-9418-b38535afdda5 => Resumed> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 09:57:33.757 27161 INFO nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] VM Resumed (Lifecycle Event)
2016-12-30 09:57:33.810 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Synchronizing instance power state after lifecycle event "Resumed"; current vm_state: suspended, current task_state: resuming, current DB power_state: 4, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 09:57:35.839 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._reclaim_queued_deletes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 09:57:35.839 27161 DEBUG nova.compute.manager [-] CONF.reclaim_instance_interval <= 0, skipping... _reclaim_queued_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:5932
2016-12-30 09:57:35.839 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 4.97 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132

Rescue/Unrescue
有时候由于误操作或者突然断电，操作系统重启后却起不来了。 为了最大限度挽救数据，我们通常会使用一张系统盘将系统引导起来，然后在尝试恢复。 问题如果不太严重，完全可以通过这种方式让系统重新正常工作。 比如某个系统文件意外删除， root 密码遗忘等



[root@dzc-o-control01v rabbitmq]# nova rescue  3e11d9ce-7e64-4029-a5cd-84afd39a5cda
+-----------+--------------+
| Property  | Value        |
+-----------+--------------+
| adminPass | w7jq6ME8j2Hz |
+-----------+--------------+


2016-12-30 10:10:10.105 27161 AUDIT nova.compute.manager [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Rescuing
2016-12-30 10:10:10.105 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "refresh_cache-3e11d9ce-7e64-4029-a5cd-84afd39a5cda" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.105 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "refresh_cache-3e11d9ce-7e64-4029-a5cd-84afd39a5cda" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.106 27161 DEBUG nova.network.neutronv2.api [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
2016-12-30 10:10:10.106 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.106 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.106 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.107 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.107 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.107 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.108 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.108 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.108 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.109 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.109 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.109 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.109 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=3e11d9ce-7e64-4029-a5cd-84afd39a5cda -X GET -H "X-Auth-Token: 4f53ee00bd384eba8c376428dae462e5" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:10:10.135 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] RESP:200 {'date': 'Fri, 30 Dec 2016 02:10:10 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-808261cf-5767-4208-a5f0-f99943e1d6a2'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.90"}], "id": "ee77baef-a63f-490e-9c61-a9217b2ecce7", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "3e11d9ce-7e64-4029-a5cd-84afd39a5cda", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:37:81:0f"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:10:10.135 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.136 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.136 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.136 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.136 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.137 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.137 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.137 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.137 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.138 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.138 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.138 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.138 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.139 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.139 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.139 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.139 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.139 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.140 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.140 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.140 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.141 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.141 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.141 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.141 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/networks.json?id=d647355e-b019-4de0-999e-00107531edc0 -X GET -H "X-Auth-Token: 9c200ebc386d4e3cbec0af76f8e21f15" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:10:10.165 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] RESP:200 {'date': 'Fri, 30 Dec 2016 02:10:10 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '376', 'x-openstack-request-id': 'req-061c37b2-e002-4da2-b884-a84e32060215'} {"networks": [{"status": "ACTIVE", "subnets": ["b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"], "name": "DMZ_NET", "provider:physical_network": "physnet1", "admin_state_up": true, "tenant_id": "e599088c985f42e7948b12f601705cd3", "provider:network_type": "vlan", "router:external": false, "shared": true, "id": "d647355e-b019-4de0-999e-00107531edc0", "provider:segmentation_id": 502}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:10:10.165 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.166 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.166 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.166 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.166 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.167 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.167 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.167 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.167 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.168 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/floatingips.json?fixed_ip_address=192.168.34.90&port_id=ee77baef-a63f-490e-9c61-a9217b2ecce7 -X GET -H "X-Auth-Token: 4f53ee00bd384eba8c376428dae462e5" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:10:10.177 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] RESP:200 {'date': 'Fri, 30 Dec 2016 02:10:10 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '19', 'x-openstack-request-id': 'req-8121cf0d-956c-438b-a111-abc06082a974'} {"floatingips": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:10:10.177 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.177 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.177 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.178 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.178 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.178 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.178 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.179 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.179 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.179 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.179 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.179 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.180 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.180 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.180 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.180 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.180 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.181 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.181 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.181 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.181 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.182 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:10.182 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:10.182 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.182 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/subnets.json?id=b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 -X GET -H "X-Auth-Token: 9c200ebc386d4e3cbec0af76f8e21f15" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:10:10.205 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] RESP:200 {'date': 'Fri, 30 Dec 2016 02:10:10 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '519', 'x-openstack-request-id': 'req-25ae4160-8664-4da5-9179-d8056cd9becd'} {"subnets": [{"name": "DMZ_SUBNET", "enable_dhcp": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "dns_nameservers": ["218.30.110.17", "8.8.8.8"], "gateway_ip": null, "ipv6_ra_mode": null, "allocation_pools": [{"start": "192.168.34.3", "end": "192.168.34.250"}], "host_routes": [{"nexthop": "192.168.34.254", "destination": "0.0.0.0/0"}], "ip_version": 4, "ipv6_address_mode": null, "cidr": "192.168.34.0/24", "id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:10:10.206 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?network_id=d647355e-b019-4de0-999e-00107531edc0&device_owner=network%3Adhcp -X GET -H "X-Auth-Token: 9c200ebc386d4e3cbec0af76f8e21f15" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:10:10.239 27161 DEBUG neutronclient.client [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] RESP:200 {'date': 'Fri, 30 Dec 2016 02:10:10 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '746', 'x-openstack-request-id': 'req-14b4bbab-3524-48c9-b126-2aa5f47f12a9'} {"ports": [{"status": "ACTIVE", "binding:host_id": "dzc-o-neutron01v", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "network:dhcp", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.3"}], "id": "db21d3d0-9960-43c7-a811-53c63050a6ce", "security_groups": [], "device_id": "dhcp95fc7688-e5d9-5615-b6e9-fc077300258a-d647355e-b019-4de0-999e-00107531edc0", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:96:f0:bf"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:10:10.240 27161 DEBUG nova.network.base_api [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.90'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tapee77baef-a6', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:37:81:0f', 'active': False, 'type': u'ovs', 'id': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2016-12-30 10:10:10.259 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "refresh_cache-3e11d9ce-7e64-4029-a5cd-84afd39a5cda" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:10.316 27161 DEBUG glanceclient.common.http [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] curl -i -X HEAD -H 'X-Service-Catalog: [{"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "id": "37eb9ae73b72467b8695247873083335", "internalURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3"}], "type": "volumev2", "name": "cinderv2"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "region": "regionOne", "id": "30d052b7f62344dabab8c29928d29f99", "serviceName": "cinder", "internalURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "publicURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3"}], "type": "volume", "name": "cinder"}]' -H 'X-Auth-Token: {SHA1}4cd10124ead4c1096fbfda564a95e213c22db9d9' -H 'Accept-Encoding: gzip, deflate' -H 'Connection: keep-alive' -H 'Accept: */*' -H 'X-Roles: admin' -H 'User-Agent: python-glanceclient' -H 'X-Tenant-Id: e599088c985f42e7948b12f601705cd3' -H 'X-User-Id: 7d5b5abc30ea463690567e5f8cc794f9' -H 'X-Identity-Status: Confirmed' -H 'Content-Type: application/octet-stream' http://controller.light.fang.com:9292/v1/images/17ed0c0a-55a6-4044-8ecf-8af8844b4199 log_curl_request /usr/lib/python2.7/site-packages/glanceclient/common/http.py:122
2016-12-30 10:10:10.771 27161 DEBUG glanceclient.common.http [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] 
HTTP/1.1 200 OK
content-length: 0
x-image-meta-id: 17ed0c0a-55a6-4044-8ecf-8af8844b4199
date: Fri, 30 Dec 2016 02:10:10 GMT
x-image-meta-deleted: False
x-image-meta-container_format: bare
connection: keep-alive
x-image-meta-checksum: ad11feb5af04f08d74ff6f4cb14527fa
x-image-meta-protected: False
x-image-meta-min_disk: 0
x-image-meta-created_at: 2016-12-16T07:36:04
x-image-meta-size: 1373306880
x-image-meta-status: active
etag: ad11feb5af04f08d74ff6f4cb14527fa
x-image-meta-is_public: True
x-image-meta-min_ram: 0
x-image-meta-owner: e599088c985f42e7948b12f601705cd3
x-image-meta-updated_at: 2016-12-16T07:36:20
content-type: text/html; charset=UTF-8
x-openstack-request-id: req-1c4ea325-0f0f-4f00-ae80-f17b70b6b19f
x-image-meta-disk_format: qcow2
x-image-meta-name: centos7.2_x64
 log_http_response /usr/lib/python2.7/site-packages/glanceclient/common/http.py:135
2016-12-30 10:10:10.776 27161 DEBUG nova.virt.libvirt.driver [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Shutting down instance from state 1 _clean_shutdown /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:2432
2016-12-30 10:10:15.789 27161 INFO nova.virt.libvirt.driver [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Instance shutdown successfully after 5 seconds.
2016-12-30 10:10:16.398 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483063816.4, 3e11d9ce-7e64-4029-a5cd-84afd39a5cda => Stopped> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 10:10:16.399 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] VM Stopped (Lifecycle Event)
2016-12-30 10:10:16.402 27161 INFO nova.virt.libvirt.driver [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Instance destroyed successfully.
2016-12-30 10:10:16.404 27161 INFO nova.virt.libvirt.driver [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Creating image
2016-12-30 10:10:16.405 27161 DEBUG nova.openstack.common.processutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf chown 162 /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/console.log execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:10:16.474 27161 DEBUG nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Synchronizing instance power state after lifecycle event "Stopped"; current vm_state: active, current task_state: rescuing, current DB power_state: 1, VM power_state: 4 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 10:10:16.488 27161 DEBUG nova.openstack.common.processutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:10:16.490 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk.info" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:16.491 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:16.491 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Got semaphore / lock "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 10:10:16.491 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:16.492 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Semaphore / lock released "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 10:10:16.492 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Created new semaphore "fbde4392872102bb2e6b40f009c1f5d3fa12cb34" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:10:16.492 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Acquired semaphore "fbde4392872102bb2e6b40f009c1f5d3fa12cb34" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:10:16.492 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Attempting to grab external lock "fbde4392872102bb2e6b40f009c1f5d3fa12cb34" external_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:178
2016-12-30 10:10:16.493 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Got file lock "/var/lib/nova/instances/locks/nova-fbde4392872102bb2e6b40f009c1f5d3fa12cb34" acquire /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:93
2016-12-30 10:10:16.493 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Got semaphore / lock "copy_qcow2_image" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 10:10:16.493 27161 DEBUG nova.openstack.common.processutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/_base/fbde4392872102bb2e6b40f009c1f5d3fa12cb34 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:10:16.562 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] During sync_power_state the instance has a pending task (rescuing). Skip.
2016-12-30 10:10:16.570 27161 DEBUG nova.openstack.common.processutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195

可以看到nova将instance启动时的imge创建 磁盘 然后用这个磁盘引导系统
2016-12-30 10:10:16.570 27161 DEBUG nova.openstack.common.processutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Running cmd (subprocess): qemu-img create -f qcow2 -o backing_file=/var/lib/nova/instances/_base/fbde4392872102bb2e6b40f009c1f5d3fa12cb34 /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk.rescue execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:10:16.668 27161 DEBUG nova.openstack.common.processutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:10:16.668 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Released file lock "/var/lib/nova/instances/locks/nova-fbde4392872102bb2e6b40f009c1f5d3fa12cb34" release /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:115
2016-12-30 10:10:16.669 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Releasing semaphore "fbde4392872102bb2e6b40f009c1f5d3fa12cb34" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:10:16.669 27161 DEBUG nova.openstack.common.lockutils [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f ] Semaphore / lock released "copy_qcow2_image" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 10:10:16.670 27161 DEBUG nova.virt.libvirt.driver [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Start _get_guest_xml network_info=[VIF({'profile': {}, 'ovs_interfaceid': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:37:81:0f', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.90', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tapee77baef-a6', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:37:81:0f', 'active': False, 'type': u'ovs', 'id': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'qbg_params': None})] disk_info={'disk_bus': 'virtio', 'cdrom_bus': 'ide', 'mapping': {'disk': {'bus': 'virtio', 'type': 'disk', 'dev': 'vdb'}, 'disk.rescue': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': 'vda'}, 'root': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': 'vda'}}} image_meta={u'container_format': u'bare', u'min_ram': 0, u'disk_format': u'qcow2', 'id': u'17ed0c0a-55a6-4044-8ecf-8af8844b4199', u'min_disk': 100, 'properties': {u'instance_type_memory_mb': u'16000', u'instance_type_swap': u'0', u'instance_type_root_gb': u'100', u'instance_type_name': u'fang-kafka', u'instance_type_id': u'22', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'instance_type_flavorid': u'621313b5-1e0b-4819-a33b-68990cbb93ca', u'instance_type_vcpus': u'16', u'base_image_ref': u'17ed0c0a-55a6-4044-8ecf-8af8844b4199'}} rescue={'kernel_id': u'', 'image_id': u'17ed0c0a-55a6-4044-8ecf-8af8844b4199', 'ramdisk_id': u''} block_device_info=None _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4156
2016-12-30 10:10:16.700 27161 DEBUG nova.objects.instance [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Lazy-loading `numa_topology' on Instance uuid 3e11d9ce-7e64-4029-a5cd-84afd39a5cda obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 10:10:16.714 27161 DEBUG nova.virt.libvirt.driver [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] CPU mode 'host-model' model '' was chosen _get_guest_cpu_model_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:3362
2016-12-30 10:10:16.714 27161 DEBUG nova.virt.hardware [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Getting desirable topologies for flavor Flavor(created_at=2016-08-31T10:42:03Z,deleted=False,deleted_at=None,disabled=False,ephemeral_gb=0,extra_specs={},flavorid='621313b5-1e0b-4819-a33b-68990cbb93ca',id=22,is_public=False,memory_mb=16000,name='fang-kafka',projects=<?>,root_gb=100,rxtx_factor=1.0,swap=0,updated_at=None,vcpu_weight=0,vcpus=16) and image_meta {u'container_format': u'bare', u'min_ram': 0, u'disk_format': u'qcow2', 'id': u'17ed0c0a-55a6-4044-8ecf-8af8844b4199', u'min_disk': 100, 'properties': {u'instance_type_memory_mb': u'16000', u'instance_type_swap': u'0', u'instance_type_root_gb': u'100', u'instance_type_name': u'fang-kafka', u'instance_type_id': u'22', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'instance_type_flavorid': u'621313b5-1e0b-4819-a33b-68990cbb93ca', u'instance_type_vcpus': u'16', u'base_image_ref': u'17ed0c0a-55a6-4044-8ecf-8af8844b4199'}} get_desirable_configs /usr/lib/python2.7/site-packages/nova/virt/hardware.py:502
2016-12-30 10:10:16.715 27161 DEBUG nova.virt.hardware [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Flavor limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:296
2016-12-30 10:10:16.715 27161 DEBUG nova.virt.hardware [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Image limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:309
2016-12-30 10:10:16.715 27161 DEBUG nova.virt.hardware [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Flavor pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:332
2016-12-30 10:10:16.715 27161 DEBUG nova.virt.hardware [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Image pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:354
2016-12-30 10:10:16.716 27161 DEBUG nova.virt.hardware [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Chosen -1:-1:-1 limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:383
2016-12-30 10:10:16.716 27161 DEBUG nova.virt.hardware [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Build topologies for 16 vcpu(s) 16:16:16 get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:419
2016-12-30 10:10:16.717 27161 DEBUG nova.virt.hardware [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Got 15 possible topologies get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:442
2016-12-30 10:10:16.719 27161 DEBUG nova.virt.libvirt.vif [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-26T07:41:49Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='trestle',display_name='ceph-01',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute03',hostname='trestle',id=198,image_ref='17ed0c0a-55a6-4044-8ecf-8af8844b4199',info_cache=InstanceInfoCache,instance_type_id=22,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2016-12-26T07:43:11Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=16000,metadata={},node='sjhl-o-compute03',numa_topology=None,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-h8jb081h',root_device_name='/dev/vda',root_gb=100,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='17ed0c0a-55a6-4044-8ecf-8af8844b4199',image_container_format='bare',image_disk_format='qcow2',image_min_disk='100',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='621313b5-1e0b-4819-a33b-68990cbb93ca',instance_type_id='22',instance_type_memory_mb='16000',instance_type_name='fang-kafka',instance_type_root_gb='100',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='16'},task_state='rescuing',terminated_at=None,updated_at=2016-12-30T02:10:10Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=3e11d9ce-7e64-4029-a5cd-84afd39a5cda,vcpus=16,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:37:81:0f', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.90', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tapee77baef-a6', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:37:81:0f', 'active': False, 'type': u'ovs', 'id': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'qbg_params': None}) virt_typekvm get_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:342
2016-12-30 10:10:16.722 27161 DEBUG nova.objects.instance [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Lazy-loading `pci_devices' on Instance uuid 3e11d9ce-7e64-4029-a5cd-84afd39a5cda obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 10:10:16.792 27161 DEBUG nova.virt.libvirt.config [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] Generated XML ('<domain type="kvm">\n  <uuid>3e11d9ce-7e64-4029-a5cd-84afd39a5cda</uuid>\n  <name>instance-000000c6</name>\n  <memory>16384000</memory>\n  <vcpu>16</vcpu>\n  <metadata>\n    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">\n      <nova:package version="2014.2.2-1.el7"/>\n      <nova:name>ceph-01</nova:name>\n      <nova:creationTime>2016-12-30 02:10:16</nova:creationTime>\n      <nova:flavor name="fang-kafka">\n        <nova:memory>16000</nova:memory>\n        <nova:disk>100</nova:disk>\n        <nova:swap>0</nova:swap>\n        <nova:ephemeral>0</nova:ephemeral>\n        <nova:vcpus>16</nova:vcpus>\n      </nova:flavor>\n      <nova:owner>\n        <nova:user uuid="7d5b5abc30ea463690567e5f8cc794f9">admin</nova:user>\n        <nova:project uuid="e599088c985f42e7948b12f601705cd3">admin</nova:project>\n      </nova:owner>\n      <nova:root type="image" uuid="17ed0c0a-55a6-4044-8ecf-8af8844b4199"/>\n    </nova:instance>\n  </metadata>\n  <sysinfo type="smbios">\n    <system>\n      <entry name="manufacturer">Fedora Project</entry>\n      <entry name="product">OpenStack Nova</entry>\n      <entry name="version">2014.2.2-1.el7</entry>\n      <entry name="serial">a6dae68f-0d87-4f34-9359-73beb2000e47</entry>\n      <entry name="uuid">3e11d9ce-7e64-4029-a5cd-84afd39a5cda</entry>\n    </system>\n  </sysinfo>\n  <os>\n    <type>hvm</type>\n    <smbios mode="sysinfo"/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n  </features>\n  <clock offset="utc">\n    <timer name="pit" tickpolicy="delay"/>\n    <timer name="rtc" tickpolicy="catchup"/>\n    <timer name="hpet" present="no"/>\n  </clock>\n  <cpu mode="host-model" match="exact">\n    <topology sockets="16" cores="1" threads="1"/>\n  </cpu>\n  <devices>\n    <disk type="file" device="disk">\n      <driver name="qemu" type="qcow2" cache="none"/>\n      <source file="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk.rescue"/>\n      <target bus="virtio" dev="vda"/>\n    </disk>\n    <disk type="file" device="disk">\n      <driver name="qemu" type="qcow2" cache="none"/>\n      <source file="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk"/>\n      <target bus="virtio" dev="vdb"/>\n    </disk>\n    <interface type="bridge">\n      <mac address="fa:16:3e:37:81:0f"/>\n      <model type="virtio"/>\n      <source bridge="qbree77baef-a6"/>\n      <target dev="tapee77baef-a6"/>\n    </interface>\n    <serial type="file">\n      <source path="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/console.log"/>\n    </serial>\n    <serial type="pty"/>\n    <input type="tablet" bus="usb"/>\n    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>\n    <video>\n      <model type="cirrus"/>\n    </video>\n    <memballoon model="virtio">\n      <stats period="10"/>\n    </memballoon>\n  </devices>\n</domain>\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82
2016-12-30 10:10:16.793 27161 DEBUG nova.virt.libvirt.driver [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] End _get_guest_xml xml=<domain type="kvm">
  <uuid>3e11d9ce-7e64-4029-a5cd-84afd39a5cda</uuid>
  <name>instance-000000c6</name>
  <memory>16384000</memory>
  <vcpu>16</vcpu>
  <metadata>
    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">
      <nova:package version="2014.2.2-1.el7"/>
      <nova:name>ceph-01</nova:name>
      <nova:creationTime>2016-12-30 02:10:16</nova:creationTime>
      <nova:flavor name="fang-kafka">
        <nova:memory>16000</nova:memory>
        <nova:disk>100</nova:disk>
        <nova:swap>0</nova:swap>
        <nova:ephemeral>0</nova:ephemeral>
        <nova:vcpus>16</nova:vcpus>
      </nova:flavor>
      <nova:owner>
        <nova:user uuid="7d5b5abc30ea463690567e5f8cc794f9">admin</nova:user>
        <nova:project uuid="e599088c985f42e7948b12f601705cd3">admin</nova:project>
      </nova:owner>
      <nova:root type="image" uuid="17ed0c0a-55a6-4044-8ecf-8af8844b4199"/>
    </nova:instance>
  </metadata>
  <sysinfo type="smbios">
    <system>
      <entry name="manufacturer">Fedora Project</entry>
      <entry name="product">OpenStack Nova</entry>
      <entry name="version">2014.2.2-1.el7</entry>
      <entry name="serial">a6dae68f-0d87-4f34-9359-73beb2000e47</entry>
      <entry name="uuid">3e11d9ce-7e64-4029-a5cd-84afd39a5cda</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <smbios mode="sysinfo"/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset="utc">
    <timer name="pit" tickpolicy="delay"/>
    <timer name="rtc" tickpolicy="catchup"/>
    <timer name="hpet" present="no"/>
  </clock>
  <cpu mode="host-model" match="exact">
    <topology sockets="16" cores="1" threads="1"/>
  </cpu>
  <devices>
    <disk type="file" device="disk">
      <driver name="qemu" type="qcow2" cache="none"/>
      <source file="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk.rescue"/>
      <target bus="virtio" dev="vda"/>
    </disk>
    <disk type="file" device="disk">
      <driver name="qemu" type="qcow2" cache="none"/>
      <source file="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk"/>
      <target bus="virtio" dev="vdb"/>
    </disk>
    <interface type="bridge">
      <mac address="fa:16:3e:37:81:0f"/>
      <model type="virtio"/>
      <source bridge="qbree77baef-a6"/>
      <target dev="tapee77baef-a6"/>
    </interface>
    <serial type="file">
      <source path="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/console.log"/>
    </serial>
    <serial type="pty"/>
    <input type="tablet" bus="usb"/>
    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>
    <video>
      <model type="cirrus"/>
    </video>
    <memballoon model="virtio">
      <stats period="10"/>
    </memballoon>
  </devices>
</domain>
 _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4168
2016-12-30 10:10:16.796 27161 INFO nova.virt.libvirt.driver [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Instance destroyed successfully.
2016-12-30 10:10:17.268 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483063817.27, 3e11d9ce-7e64-4029-a5cd-84afd39a5cda => Resumed> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 10:10:17.268 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] VM Resumed (Lifecycle Event)
2016-12-30 10:10:17.290 27161 DEBUG nova.compute.manager [req-52fc8aaf-784c-4def-91d5-60ea2f23bc0f None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2016-12-30 10:10:17.334 27161 DEBUG nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Synchronizing instance power state after lifecycle event "Resumed"; current vm_state: active, current task_state: rescuing, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 10:10:17.417 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] During sync_power_state the instance has a pending task (rescuing). Skip.
2016-12-30 10:10:17.417 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483063817.27, 3e11d9ce-7e64-4029-a5cd-84afd39a5cda => Started> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 10:10:17.418 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] VM Started (Lifecycle Event)
2016-12-30 10:10:17.480 27161 DEBUG nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Synchronizing instance power state after lifecycle event "Started"; current vm_state: rescued, current task_state: None, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 10:10:19.096 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager.update_available_resource run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:10:19.097 27161 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2016-12-30 10:10:19.097 27161 DEBUG nova.virt.libvirt.driver [-] Updating host stats update_status /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:6399

[root@dzc-o-control01v rabbitmq]# nova unrescue  3e11d9ce-7e64-4029-a5cd-84afd39a5cda


Nova 备份的操作叫 Snapshot
其工作原理是对 instance 的镜像文件（系统盘）进行全量备份，生成一个类型为 snapshot 的 image，然后将其保存到 Glance 上。
从备份恢复的操作叫 Rebuild




2016-12-30 10:29:16.588 27161 DEBUG nova.compute.manager [req-b14539de-0aee-4a0a-b9cb-12e6124af779 None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159

2016-12-30 10:29:16.674 27161 AUDIT nova.compute.manager [req-b14539de-0aee-4a0a-b9cb-12e6124af779 None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] instance snapshotting
2016-12-30 10:29:16.678 27161 DEBUG glanceclient.common.http [req-b14539de-0aee-4a0a-b9cb-12e6124af779 ] curl -i -X HEAD -H 'X-Service-Catalog: [{"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "id": "37eb9ae73b72467b8695247873083335", "internalURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02"}], "type": "volumev2", "name": "cinderv2"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "id": "30d052b7f62344dabab8c29928d29f99", "internalURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02"}], "type": "volume", "name": "cinder"}]' -H 'X-Auth-Token: {SHA1}ef83b2f864d1bd45a47f8cd0e19ce35908c4bbc3' -H 'Accept-Encoding: gzip, deflate' -H 'Connection: keep-alive' -H 'Accept: */*' -H 'X-Roles: _member_,admin' -H 'User-Agent: python-glanceclient' -H 'X-Tenant-Id: 516953ba1481400fb69642e9ffe14b02' -H 'X-User-Id: 2a1a6dee31a0461c95a5ba8e7e1f30e1' -H 'X-Identity-Status: Confirmed' -H 'Content-Type: application/octet-stream' http://controller.light.fang.com:9292/v1/images/17ed0c0a-55a6-4044-8ecf-8af8844b4199 log_curl_request /usr/lib/python2.7/site-packages/glanceclient/common/http.py:122
2016-12-30 10:29:16.813 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._heal_instance_info_cache run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:29:16.813 27161 DEBUG nova.compute.manager [-] Starting heal instance info cache _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5289
2016-12-30 10:29:16.876 27161 DEBUG glanceclient.common.http [req-b14539de-0aee-4a0a-b9cb-12e6124af779 ] 
HTTP/1.1 200 OK
content-length: 0
x-image-meta-id: 17ed0c0a-55a6-4044-8ecf-8af8844b4199
date: Fri, 30 Dec 2016 02:29:16 GMT
x-image-meta-deleted: False
x-image-meta-container_format: bare
connection: keep-alive
x-image-meta-checksum: ad11feb5af04f08d74ff6f4cb14527fa
x-image-meta-protected: False
x-image-meta-min_disk: 0
x-image-meta-created_at: 2016-12-16T07:36:04
x-image-meta-size: 1373306880
x-image-meta-status: active
etag: ad11feb5af04f08d74ff6f4cb14527fa
x-image-meta-is_public: True
x-image-meta-min_ram: 0
x-image-meta-owner: e599088c985f42e7948b12f601705cd3
x-image-meta-updated_at: 2016-12-16T07:36:20
content-type: text/html; charset=UTF-8
x-openstack-request-id: req-1276e70c-b763-4aa9-ae02-2e15da8f36c5
x-image-meta-disk_format: qcow2
x-image-meta-name: centos7.2_x64
 log_http_response /usr/lib/python2.7/site-packages/glanceclient/common/http.py:135
2016-12-30 10:29:16.878 27161 DEBUG glanceclient.common.http [req-b14539de-0aee-4a0a-b9cb-12e6124af779 ] curl -i -X HEAD -H 'X-Service-Catalog: [{"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "id": "37eb9ae73b72467b8695247873083335", "internalURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02"}], "type": "volumev2", "name": "cinderv2"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "id": "30d052b7f62344dabab8c29928d29f99", "internalURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02"}], "type": "volume", "name": "cinder"}]' -H 'X-Auth-Token: {SHA1}ef83b2f864d1bd45a47f8cd0e19ce35908c4bbc3' -H 'Accept-Encoding: gzip, deflate' -H 'Connection: keep-alive' -H 'Accept: */*' -H 'X-Roles: _member_,admin' -H 'User-Agent: python-glanceclient' -H 'X-Tenant-Id: 516953ba1481400fb69642e9ffe14b02' -H 'X-User-Id: 2a1a6dee31a0461c95a5ba8e7e1f30e1' -H 'X-Identity-Status: Confirmed' -H 'Content-Type: application/octet-stream' http://controller.light.fang.com:9292/v1/images/8231f02f-2ab0-4762-8b80-437ddb72b7cd log_curl_request /usr/lib/python2.7/site-packages/glanceclient/common/http.py:122
2016-12-30 10:29:16.895 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=0df36b9f-1f4d-4f9f-9418-b38535afdda5 -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:29:16.921 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:29:16 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-3181a0ee-8f6a-4e8a-bf12-adb3c3a66420'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.98"}], "id": "e9bb122e-5719-4614-bacb-5c4bd727f5e6", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "0df36b9f-1f4d-4f9f-9418-b38535afdda5", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:73:bc:a3"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:29:16.927 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/networks.json?id=d647355e-b019-4de0-999e-00107531edc0 -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:29:16.946 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:29:16 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '376', 'x-openstack-request-id': 'req-f8b0bf0e-bdc7-4623-82bd-e08581657fae'} {"networks": [{"status": "ACTIVE", "subnets": ["b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"], "name": "DMZ_NET", "provider:physical_network": "physnet1", "admin_state_up": true, "tenant_id": "e599088c985f42e7948b12f601705cd3", "provider:network_type": "vlan", "router:external": false, "shared": true, "id": "d647355e-b019-4de0-999e-00107531edc0", "provider:segmentation_id": 502}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149

2016-12-30 10:29:16.952 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/floatingips.json?fixed_ip_address=192.168.34.98&port_id=e9bb122e-5719-4614-bacb-5c4bd727f5e6 -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:29:16.962 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:29:16 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '19', 'x-openstack-request-id': 'req-e9cbf50c-aca9-4318-85dd-4d4f3ff0eb02'} {"floatingips": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:29:16.968 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/subnets.json?id=b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:29:16.984 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:29:16 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '519', 'x-openstack-request-id': 'req-c50eba17-b2dc-46ea-8cd2-3a9666631840'} {"subnets": [{"name": "DMZ_SUBNET", "enable_dhcp": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "dns_nameservers": ["218.30.110.17", "8.8.8.8"], "gateway_ip": null, "ipv6_ra_mode": null, "allocation_pools": [{"start": "192.168.34.3", "end": "192.168.34.250"}], "host_routes": [{"nexthop": "192.168.34.254", "destination": "0.0.0.0/0"}], "ip_version": 4, "ipv6_address_mode": null, "cidr": "192.168.34.0/24", "id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:29:16.991 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?network_id=d647355e-b019-4de0-999e-00107531edc0&device_owner=network%3Adhcp -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:29:17.014 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:29:17 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '746', 'x-openstack-request-id': 'req-10aa524f-6aa3-4ee6-a800-00f2b5d0ff54'} {"ports": [{"status": "ACTIVE", "binding:host_id": "dzc-o-neutron01v", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "network:dhcp", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.3"}], "id": "db21d3d0-9960-43c7-a811-53c63050a6ce", "security_groups": [], "device_id": "dhcp95fc7688-e5d9-5615-b6e9-fc077300258a-d647355e-b019-4de0-999e-00107531edc0", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:96:f0:bf"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:29:17.019 27161 DEBUG nova.network.base_api [-] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.98'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape9bb122e-57', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:73:bc:a3', 'active': False, 'type': u'ovs', 'id': u'e9bb122e-5719-4614-bacb-5c4bd727f5e6', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2016-12-30 10:29:17.033 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "refresh_cache-0df36b9f-1f4d-4f9f-9418-b38535afdda5" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:29:17.033 27161 DEBUG nova.compute.manager [-] [instance: 0df36b9f-1f4d-4f9f-9418-b38535afdda5] Updated the network info_cache for instance _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5350
2016-12-30 10:29:17.033 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 21.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 10:29:17.122 27161 DEBUG glanceclient.common.http [req-b14539de-0aee-4a0a-b9cb-12e6124af779 ] 
HTTP/1.1 200 OK
content-length: 0
x-image-meta-property-user_id: 2a1a6dee31a0461c95a5ba8e7e1f30e1
x-image-meta-property-instance_type_memory_mb: 16000
x-image-meta-status: queued
x-image-meta-owner: 516953ba1481400fb69642e9ffe14b02
x-image-meta-name: ceph-01
x-image-meta-property-instance_type_ephemeral_gb: 0
x-image-meta-container_format: bare
connection: keep-alive
x-image-meta-property-image_type: snapshot
x-image-meta-created_at: 2016-12-30T02:29:16
x-image-meta-property-instance_type_root_gb: 100
x-image-meta-property-instance_type_vcpus: 16
x-image-meta-updated_at: 2016-12-30T02:29:16
x-image-meta-property-instance_type_rxtx_factor: 1.0
x-image-meta-id: 8231f02f-2ab0-4762-8b80-437ddb72b7cd
x-image-meta-property-instance_type_id: 22
x-image-meta-property-instance_uuid: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda
x-image-meta-min_ram: 0
x-image-meta-property-instance_type_name: fang-kafka
date: Fri, 30 Dec 2016 02:29:17 GMT
x-image-meta-property-instance_type_flavorid: 621313b5-1e0b-4819-a33b-68990cbb93ca
x-openstack-request-id: req-d0432cc7-4c0e-403b-aa7a-8e8e24023ef0
x-image-meta-deleted: False
x-image-meta-property-instance_type_swap: 0
x-image-meta-protected: False
x-image-meta-min_disk: 100
x-image-meta-size: 0
x-image-meta-property-base_image_ref: 17ed0c0a-55a6-4044-8ecf-8af8844b4199
x-image-meta-is_public: False
content-type: text/html; charset=UTF-8
x-image-meta-disk_format: qcow2
 log_http_response /usr/lib/python2.7/site-packages/glanceclient/common/http.py:135
2016-12-30 10:29:17.125 27161 DEBUG nova.openstack.common.processutils [req-b14539de-0aee-4a0a-b9cb-12e6124af779 None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:17.186 27161 DEBUG nova.openstack.common.processutils [req-b14539de-0aee-4a0a-b9cb-12e6124af779 None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:17.189 27161 DEBUG nova.objects.instance [req-b14539de-0aee-4a0a-b9cb-12e6124af779 None] Lazy-loading `pci_devices' on Instance uuid 3e11d9ce-7e64-4029-a5cd-84afd39a5cda obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 10:29:17.284 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483064957.28, 3e11d9ce-7e64-4029-a5cd-84afd39a5cda => Paused> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 10:29:17.285 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] VM Paused (Lifecycle Event)
2016-12-30 10:29:17.355 27161 DEBUG nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Synchronizing instance power state after lifecycle event "Paused"; current vm_state: active, current task_state: image_snapshot, current DB power_state: 1, VM power_state: 3 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 10:29:17.427 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] During sync_power_state the instance has a pending task (image_snapshot). Skip.

这里可以看到冷快照  cold snapshot
2016-12-30 10:29:23.177 27161 INFO nova.virt.libvirt.driver [req-b14539de-0aee-4a0a-b9cb-12e6124af779 None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Beginning cold snapshot process
2016-12-30 10:29:23.181 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483064963.18, 3e11d9ce-7e64-4029-a5cd-84afd39a5cda => Stopped> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 10:29:23.182 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] VM Stopped (Lifecycle Event)
2016-12-30 10:29:23.254 27161 DEBUG nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Synchronizing instance power state after lifecycle event "Stopped"; current vm_state: active, current task_state: image_snapshot, current DB power_state: 1, VM power_state: 4 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108


qemu-img convert -f qcow2 -O qcow2 /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk /var/lib/nova/instances/snapshots/tmpK9nRD1/0e84ad3856c544b58346efc2bbec5b56

2016-12-30 10:29:23.280 27161 DEBUG nova.openstack.common.processutils [req-b14539de-0aee-4a0a-b9cb-12e6124af779 None] Running cmd (subprocess): qemu-img convert -f qcow2 -O qcow2 /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk /var/lib/nova/instances/snapshots/tmpK9nRD1/0e84ad3856c544b58346efc2bbec5b56 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:23.338 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] During sync_power_state the instance has a pending task (image_pending_upload). Skip.
2016-12-30 10:29:38.036 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager.update_available_resource run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:29:38.036 27161 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2016-12-30 10:29:38.037 27161 DEBUG nova.virt.libvirt.driver [-] Updating host stats update_status /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:6399
2016-12-30 10:29:50.560 27161 DEBUG nova.openstack.common.processutils [req-b14539de-0aee-4a0a-b9cb-12e6124af779 None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:54.191 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483064994.19, 3e11d9ce-7e64-4029-a5cd-84afd39a5cda => Started> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 10:29:54.191 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] VM Started (Lifecycle Event)
2016-12-30 10:29:54.245 27161 DEBUG nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Synchronizing instance power state after lifecycle event "Started"; current vm_state: active, current task_state: image_pending_upload, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 10:29:54.308 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] During sync_power_state the instance has a pending task (image_pending_upload). Skip.

Snapshot extracted, beginning image upload  提取快照并上传至glance
可以看到compute03的eth0 网卡向 control01的eth0网卡发送快照  
[root@dzc-o-control01v images]# ll -lrth
total 88G
-rw-r----- 1 glance glance  13M Dec 18  2015 db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5
-rw-r----- 1 glance glance  16G Dec 29  2015 651a3d42-d699-45b8-92d3-2ca51a9d15df
-rw-r----- 1 glance glance  56G Jun  1  2016 a8da627c-dd8d-4541-9785-793222017537
-rw-r----- 1 glance glance 8.5G Dec 15 15:10 e274bc23-475e-4102-ac66-9012731690ec
-rw-r----- 1 glance glance 2.0G Dec 16 12:20 1ca0181d-24f4-4c13-938e-e05ade3d1ada
-rw-r----- 1 glance glance 1.3G Dec 16 15:36 17ed0c0a-55a6-4044-8ecf-8af8844b4199
-rw-r----- 1 glance glance 3.4G Dec 16 16:02 02d2fae8-6991-434e-8ee5-f3ef276fccd0
-rw-r----- 1 glance glance 2.5G Dec 30 10:30 8231f02f-2ab0-4762-8b80-437ddb72b7cd



rebuild的日志 spawn的日志可以看到计算机点还是通过网络去下载镜像
2016-12-30 10:29:54.330 27161 INFO nova.virt.libvirt.driver [req-b14539de-0aee-4a0a-b9cb-12e6124af779 None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Snapshot extracted, beginning image upload
2016-12-30 10:29:54.419 27161 DEBUG glanceclient.common.http [req-b14539de-0aee-4a0a-b9cb-12e6124af779 ] curl -i -X PUT -H 'X-Auth-Token: {SHA1}ef83b2f864d1bd45a47f8cd0e19ce35908c4bbc3' -H 'x-image-meta-property-image_state: available' -H 'x-image-meta-name: ceph-01' -H 'x-image-meta-container_format: bare' -H 'User-Agent: python-glanceclient' -H 'X-User-Id: 2a1a6dee31a0461c95a5ba8e7e1f30e1' -H 'x-image-meta-property-image_location: snapshot' -H 'Accept-Encoding: gzip, deflate' -H 'x-glance-registry-purge-props: false' -H 'x-image-meta-property-ramdisk_id: ' -H 'Connection: keep-alive' -H 'x-image-meta-property-kernel_id: ' -H 'X-Service-Catalog: [{"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "id": "37eb9ae73b72467b8695247873083335", "internalURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02"}], "type": "volumev2", "name": "cinderv2"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "id": "30d052b7f62344dabab8c29928d29f99", "internalURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02"}], "type": "volume", "name": "cinder"}]' -H 'x-image-meta-property-owner_id: 516953ba1481400fb69642e9ffe14b02' -H 'Accept: */*' -H 'X-Roles: _member_,admin' -H 'x-image-meta-size: 2582052864' -H 'X-Tenant-Id: 516953ba1481400fb69642e9ffe14b02' -H 'x-image-meta-is_public: False' -H 'X-Identity-Status: Confirmed' -H 'Content-Type: application/octet-stream' -H 'x-image-meta-disk_format: qcow2' http://controller.light.fang.com:9292/v1/images/8231f02f-2ab0-4762-8b80-437ddb72b7cd log_curl_request /usr/lib/python2.7/site-packages/glanceclient/common/http.py:122
2016-12-30 10:29:58.088 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/456c88aa-407b-4ea4-81f8-41d0f72a5395/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.157 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.157 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/456c88aa-407b-4ea4-81f8-41d0f72a5395/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.220 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.222 27161 DEBUG nova.virt.libvirt.driver [-] skipping disk for instance-00000054 as it does not have a path _get_instance_disk_info /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:5689
2016-12-30 10:29:58.224 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.284 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.285 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.346 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.349 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.408 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.409 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.470 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.473 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/247de80d-0a2c-4639-89b6-597cf243053a/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.535 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.535 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/247de80d-0a2c-4639-89b6-597cf243053a/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.597 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.601 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/5d2bbe60-9400-459e-982d-ef6fb8971748/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.676 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.677 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/5d2bbe60-9400-459e-982d-ef6fb8971748/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.737 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.738 27161 DEBUG nova.virt.libvirt.driver [-] skipping disk for instance-0000009b as it does not have a path _get_instance_disk_info /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:5689
2016-12-30 10:29:58.740 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d2ed6f28-8e5a-428b-bea6-d9ec89e5da1c/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.820 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.820 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d2ed6f28-8e5a-428b-bea6-d9ec89e5da1c/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.880 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.883 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:58.945 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:58.946 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:29:59.007 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:29:59.008 27161 DEBUG nova.virt.libvirt.driver [-] skipping disk for instance-000000c6 as it does not have a path _get_instance_disk_info /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:5689
2016-12-30 10:29:59.137 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: free ram (MB): 139878 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:454
2016-12-30 10:29:59.137 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: free disk (GB): 522 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:455
2016-12-30 10:29:59.137 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: free VCPUs: -23 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:460
2016-12-30 10:29:59.137 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: assignable PCI devices: [] _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:467
2016-12-30 10:29:59.138 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:29:59.138 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:29:59.138 27161 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 10:30:01.206 27161 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 116352
2016-12-30 10:30:01.206 27161 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 304
2016-12-30 10:30:01.207 27161 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 47
2016-12-30 10:30:01.207 27161 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2016-12-30 10:30:01.411 27161 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute03', 'sjhl-o-compute03')
2016-12-30 10:30:01.412 27161 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute03:sjhl-o-compute03
2016-12-30 10:30:01.412 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:30:01.412 27161 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 10:30:01.736 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_volume_usage run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:30:01.736 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._reclaim_queued_deletes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:30:01.737 27161 DEBUG nova.compute.manager [-] CONF.reclaim_instance_interval <= 0, skipping... _reclaim_queued_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:5932
2016-12-30 10:30:01.737 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rescued_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:30:01.737 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._check_instance_build_time run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:30:01.738 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 1.08 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 10:30:02.816 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:30:02.817 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_unconfirmed_resizes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:30:02.817 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 11.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132




2016-12-30 10:53:59.997 27161 DEBUG nova.virt.libvirt.vif [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-26T07:41:49Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='trestle',display_name='ceph-01',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute03',hostname='trestle',id=198,image_ref='8231f02f-2ab0-4762-8b80-437ddb72b7cd',info_cache=InstanceInfoCache,instance_type_id=22,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2016-12-30T02:10:17Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=16000,metadata={},node='sjhl-o-compute03',numa_topology=<?>,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-h8jb081h',root_device_name='/dev/vda',root_gb=100,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='17ed0c0a-55a6-4044-8ecf-8af8844b4199',image_container_format='bare',image_disk_format='qcow2',image_image_location='snapshot',image_image_state='available',image_image_type='snapshot',image_instance_type_ephemeral_gb='0',image_instance_type_flavorid='621313b5-1e0b-4819-a33b-68990cbb93ca',image_instance_type_id='22',image_instance_type_memory_mb='16000',image_instance_type_name='fang-kafka',image_instance_type_root_gb='100',image_instance_type_rxtx_factor='1.0',image_instance_type_swap='0',image_instance_type_vcpus='16',image_instance_uuid='3e11d9ce-7e64-4029-a5cd-84afd39a5cda',image_min_disk='100',image_min_ram='0',image_owner_id='516953ba1481400fb69642e9ffe14b02',image_user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',instance_type_ephemeral_gb='0',instance_type_flavorid='621313b5-1e0b-4819-a33b-68990cbb93ca',instance_type_id='22',instance_type_memory_mb='16000',instance_type_name='fang-kafka',instance_type_root_gb='100',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='16'},task_state='rebuilding',terminated_at=None,updated_at=2016-12-30T02:53:58Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=3e11d9ce-7e64-4029-a5cd-84afd39a5cda,vcpus=16,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'network': Network({'bridge': u'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': u'fixed', 'floating_ips': [], 'address': u'192.168.34.90'})], 'version': 4, 'meta': {u'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': u'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': u'gateway', 'address': None})})], 'meta': {u'injected': False, u'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tapee77baef-a6', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:37:81:0f', 'active': False, 'type': u'ovs', 'id': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'qbg_params': None}) unplug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:678

brctl delif qbree77baef-a6 qvbee77baef-a6
ip link set qbree77baef-a6 down
brctl delbr qbree77baef-a6
ovs-vsctl --timeout=120 -- --if-exists del-port br-int qvoee77baef-a6
ip link delete qvoee77baef-a6
mv /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda_del
Deleting instance files /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda_del
Deletion of /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda_del complete
Booting with volume 2007fa22-4282-421b-bc23-439ea07cc86d at /dev/vdb




cat /etc/iscsi/initiatorname.iscsi
systool -c fc_host -v
Creating image
qemu-img convert -O raw /var/lib/nova/instances/_base/2cf2d05104349469a636cc36228e0092b559eadc.part /var/lib/nova/instances/_base/2cf2d05104349469a636cc36228e0092b559eadc.converted
qemu-img create -f qcow2 -o backing_file=/var/lib/nova/instances/_base/2cf2d05104349469a636cc36228e0092b559eadc /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk
Checking if we can resize image /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk. size=107374182400 can_resize_image
Cannot resize image /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk to a smaller size.
brctl addbr qbree77baef-a6 
brctl setfd qbree77baef-a6
brctl stp qbree77baef-a6
tee /sys/class/net/qbree77baef-a6/bridge/multicast_snooping
ip link add qvbee77baef-a6 type veth peer name qvoee77baef-a6
ip link set qvbee77baef-a6 up
ip link set qvbee77baef-a6 promisc on
ip link set qvoee77baef-a6 up
ip link set qvoee77baef-a6 promisc on
ip link set qbree77baef-a6 up
brctl addif qbree77baef-a6 qvbee77baef-a6
ovs-vsctl --timeout=120 -- --if-exists del-port qvoee77baef-a6 -- add-port br-int qvoee77baef-a6 -- set Interface qvoee77baef-a6
VM Resumed 
VM Started

2016-12-30 10:53:59.998 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delif qbree77baef-a6 qvbee77baef-a6 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:00.058 27161 DEBUG nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Synchronizing instance power state after lifecycle event "Stopped"; current vm_state: active, current task_state: rebuilding, current DB power_state: 1, VM power_state: 4 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 10:54:00.069 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:00.070 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qbree77baef-a6 down execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:00.119 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] During sync_power_state the instance has a pending task (rebuilding). Skip.
2016-12-30 10:54:00.142 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:00.143 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delbr qbree77baef-a6 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:00.238 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:00.239 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl --timeout=120 -- --if-exists del-port br-int qvoee77baef-a6 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:00.321 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:00.322 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link delete qvoee77baef-a6 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:00.417 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:00.418 27161 DEBUG nova.network.linux_net [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Net device removed: 'qvoee77baef-a6' delete_net_dev /usr/lib/python2.7/site-packages/nova/network/linux_net.py:1380
2016-12-30 10:54:00.474 27161 DEBUG nova.objects.instance [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Lazy-loading `system_metadata' on Instance uuid 3e11d9ce-7e64-4029-a5cd-84afd39a5cda obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 10:54:00.535 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): mv /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda_del execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:00.543 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:00.544 27161 INFO nova.virt.libvirt.driver [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Deleting instance files /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda_del
2016-12-30 10:54:00.559 27161 INFO nova.virt.libvirt.driver [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Deletion of /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda_del complete
2016-12-30 10:54:00.734 27161 AUDIT nova.virt.block_device [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Booting with volume 2007fa22-4282-421b-bc23-439ea07cc86d at /dev/vdb
2016-12-30 10:54:00.735 27161 DEBUG nova.volume.cinder [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Cinderclient connection created using URL: http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02 get_cinder_client_version /usr/lib/python2.7/site-packages/nova/volume/cinder.py:255
2016-12-30 10:54:00.837 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager.update_available_resource run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:54:00.838 27161 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2016-12-30 10:54:00.838 27161 DEBUG nova.virt.libvirt.driver [-] Updating host stats update_status /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:6399
2016-12-30 10:54:00.980 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf cat /etc/iscsi/initiatorname.iscsi execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:01.059 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:01.059 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf systool -c fc_host -v execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:01.140 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 1 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:01.141 27161 DEBUG nova.virt.libvirt.driver [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Could not determine fibre channel world wide node names get_volume_connector /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:1280
2016-12-30 10:54:01.141 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf systool -c fc_host -v execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:01.209 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 1 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:01.210 27161 DEBUG nova.virt.libvirt.driver [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Could not determine fibre channel world wide port names get_volume_connector /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:1287
2016-12-30 10:54:01.210 27161 DEBUG nova.volume.cinder [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Cinderclient connection created using URL: http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02 get_cinder_client_version /usr/lib/python2.7/site-packages/nova/volume/cinder.py:255
2016-12-30 10:54:01.959 27161 DEBUG nova.volume.cinder [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Cinderclient connection created using URL: http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02 get_cinder_client_version /usr/lib/python2.7/site-packages/nova/volume/cinder.py:255
2016-12-30 10:54:02.611 27161 DEBUG nova.block_device [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] block_device_list [u'vdb'] volume_in_mapping /usr/lib/python2.7/site-packages/nova/block_device.py:555
2016-12-30 10:54:02.612 27161 INFO nova.virt.libvirt.driver [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Creating image
2016-12-30 10:54:02.613 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Created new semaphore "/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk.info" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:02.613 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Acquired semaphore "/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:02.613 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Got semaphore / lock "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 10:54:02.614 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Releasing semaphore "/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:02.614 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Semaphore / lock released "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 10:54:02.614 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Created new semaphore "2cf2d05104349469a636cc36228e0092b559eadc" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:02.614 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Acquired semaphore "2cf2d05104349469a636cc36228e0092b559eadc" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:02.615 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Attempting to grab external lock "2cf2d05104349469a636cc36228e0092b559eadc" external_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:178
2016-12-30 10:54:02.615 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Got file lock "/var/lib/nova/instances/locks/nova-2cf2d05104349469a636cc36228e0092b559eadc" acquire /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:93
2016-12-30 10:54:02.615 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Got semaphore / lock "fetch_func_sync" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 10:54:02.617 27161 DEBUG glanceclient.common.http [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] curl -i -X GET -H 'X-Service-Catalog: [{"endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "id": "37eb9ae73b72467b8695247873083335", "internalURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "publicURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02"}], "endpoints_links": [], "type": "volumev2", "name": "cinderv2"}, {"endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "internalURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "serviceName": "cinder", "id": "30d052b7f62344dabab8c29928d29f99", "publicURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02"}], "endpoints_links": [], "type": "volume", "name": "cinder"}]' -H 'X-Auth-Token: {SHA1}e26c34e839c269cedbfe72d614b1e6ecea0fde18' -H 'Accept-Encoding: gzip, deflate' -H 'Connection: keep-alive' -H 'Accept: */*' -H 'X-Roles: _member_,admin' -H 'User-Agent: python-glanceclient' -H 'X-Tenant-Id: 516953ba1481400fb69642e9ffe14b02' -H 'X-User-Id: 2a1a6dee31a0461c95a5ba8e7e1f30e1' -H 'X-Identity-Status: Confirmed' -H 'Content-Type: application/octet-stream' http://controller.light.fang.com:9292/v1/images/8231f02f-2ab0-4762-8b80-437ddb72b7cd log_curl_request /usr/lib/python2.7/site-packages/glanceclient/common/http.py:122
2016-12-30 10:54:02.980 27161 DEBUG glanceclient.common.http [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] 
HTTP/1.1 200 OK
content-length: 2582052864
x-image-meta-property-user_id: 2a1a6dee31a0461c95a5ba8e7e1f30e1
x-image-meta-property-instance_type_memory_mb: 16000
x-image-meta-status: active
x-image-meta-property-image_state: available
x-image-meta-owner: 516953ba1481400fb69642e9ffe14b02
x-image-meta-name: ceph-01
x-image-meta-property-instance_type_ephemeral_gb: 0
x-image-meta-container_format: bare
connection: keep-alive
x-image-meta-property-image_type: snapshot
x-image-meta-created_at: 2016-12-30T02:29:16
x-image-meta-property-instance_type_root_gb: 100
etag: a09d5f7e89bc8b6301085d04a5e7b926
x-image-meta-property-image_location: snapshot
x-image-meta-property-instance_type_vcpus: 16
x-image-meta-updated_at: 2016-12-30T02:30:18
x-image-meta-property-instance_type_rxtx_factor: 1.0
x-image-meta-id: 8231f02f-2ab0-4762-8b80-437ddb72b7cd
x-image-meta-property-instance_type_id: 22
x-image-meta-property-instance_uuid: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda
x-image-meta-min_ram: 0
x-image-meta-property-instance_type_name: fang-kafka
date: Fri, 30 Dec 2016 02:54:02 GMT
x-image-meta-property-instance_type_flavorid: 621313b5-1e0b-4819-a33b-68990cbb93ca
x-openstack-request-id: req-9eddb6c6-1233-4a32-8c89-17a12d1e063e
x-image-meta-deleted: False
x-image-meta-checksum: a09d5f7e89bc8b6301085d04a5e7b926
x-image-meta-property-owner_id: 516953ba1481400fb69642e9ffe14b02
x-image-meta-property-instance_type_swap: 0
x-image-meta-protected: False
x-image-meta-min_disk: 100
x-image-meta-size: 2582052864
x-image-meta-property-base_image_ref: 17ed0c0a-55a6-4044-8ecf-8af8844b4199
x-image-meta-is_public: False
content-type: application/octet-stream
x-image-meta-disk_format: qcow2
 log_http_response /usr/lib/python2.7/site-packages/glanceclient/common/http.py:135
2016-12-30 10:54:20.891 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/456c88aa-407b-4ea4-81f8-41d0f72a5395/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.101 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.102 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/456c88aa-407b-4ea4-81f8-41d0f72a5395/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.164 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.165 27161 DEBUG nova.virt.libvirt.driver [-] skipping disk for instance-00000054 as it does not have a path _get_instance_disk_info /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:5689
2016-12-30 10:54:21.168 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.233 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.234 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.296 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.299 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.365 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.366 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/0df36b9f-1f4d-4f9f-9418-b38535afdda5/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.428 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.431 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/247de80d-0a2c-4639-89b6-597cf243053a/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.494 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.495 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/247de80d-0a2c-4639-89b6-597cf243053a/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.559 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.562 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/5d2bbe60-9400-459e-982d-ef6fb8971748/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.785 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.786 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/5d2bbe60-9400-459e-982d-ef6fb8971748/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:21.849 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:21.850 27161 DEBUG nova.virt.libvirt.driver [-] skipping disk for instance-0000009b as it does not have a path _get_instance_disk_info /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:5689
2016-12-30 10:54:21.852 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d2ed6f28-8e5a-428b-bea6-d9ec89e5da1c/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:22.109 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:22.110 27161 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d2ed6f28-8e5a-428b-bea6-d9ec89e5da1c/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:22.171 27161 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:22.396 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: free ram (MB): 139799 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:454
2016-12-30 10:54:22.396 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: free disk (GB): 526 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:455
2016-12-30 10:54:22.396 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: free VCPUs: -23 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:460
2016-12-30 10:54:22.396 27161 DEBUG nova.compute.resource_tracker [-] Hypervisor: assignable PCI devices: [] _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:467
2016-12-30 10:54:22.397 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:22.397 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:22.397 27161 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 10:54:22.613 27161 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 116352
2016-12-30 10:54:22.614 27161 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 304
2016-12-30 10:54:22.614 27161 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 47
2016-12-30 10:54:22.614 27161 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2016-12-30 10:54:22.641 27161 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute03', 'sjhl-o-compute03')
2016-12-30 10:54:22.642 27161 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute03:sjhl-o-compute03
2016-12-30 10:54:22.642 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:22.642 27161 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 10:54:22.729 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_volume_usage run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:54:22.729 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._reclaim_queued_deletes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:54:22.730 27161 DEBUG nova.compute.manager [-] CONF.reclaim_instance_interval <= 0, skipping... _reclaim_queued_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:5932
2016-12-30 10:54:22.730 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rescued_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:54:22.730 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._check_instance_build_time run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:54:22.731 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 5.08 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 10:54:27.813 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:54:27.814 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_unconfirmed_resizes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:54:27.815 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 10.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 10:54:29.936 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/_base/2cf2d05104349469a636cc36228e0092b559eadc.part execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:30.000 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:30.001 27161 DEBUG nova.virt.images [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] 8231f02f-2ab0-4762-8b80-437ddb72b7cd was qcow2, converting to raw fetch_to_raw /usr/lib/python2.7/site-packages/nova/virt/images.py:115
2016-12-30 10:54:30.001 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): qemu-img convert -O raw /var/lib/nova/instances/_base/2cf2d05104349469a636cc36228e0092b559eadc.part /var/lib/nova/instances/_base/2cf2d05104349469a636cc36228e0092b559eadc.converted execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:32.693 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:33.405 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/_base/2cf2d05104349469a636cc36228e0092b559eadc.converted execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:33.469 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:33.470 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Released file lock "/var/lib/nova/instances/locks/nova-2cf2d05104349469a636cc36228e0092b559eadc" release /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:115
2016-12-30 10:54:33.470 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Releasing semaphore "2cf2d05104349469a636cc36228e0092b559eadc" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:33.470 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Semaphore / lock released "fetch_func_sync" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 10:54:33.470 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Created new semaphore "2cf2d05104349469a636cc36228e0092b559eadc" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:33.471 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Acquired semaphore "2cf2d05104349469a636cc36228e0092b559eadc" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:33.471 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Attempting to grab external lock "2cf2d05104349469a636cc36228e0092b559eadc" external_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:178
2016-12-30 10:54:33.471 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Got file lock "/var/lib/nova/instances/locks/nova-2cf2d05104349469a636cc36228e0092b559eadc" acquire /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:93
2016-12-30 10:54:33.471 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Got semaphore / lock "copy_qcow2_image" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 10:54:33.472 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/_base/2cf2d05104349469a636cc36228e0092b559eadc execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:33.531 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:33.532 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): qemu-img create -f qcow2 -o backing_file=/var/lib/nova/instances/_base/2cf2d05104349469a636cc36228e0092b559eadc /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:33.623 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:33.623 27161 DEBUG nova.virt.disk.api [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Checking if we can resize image /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk. size=107374182400 can_resize_image /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:192
2016-12-30 10:54:33.624 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:33.687 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:33.687 27161 DEBUG nova.virt.disk.api [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Cannot resize image /var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk to a smaller size. can_resize_image /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:198
2016-12-30 10:54:33.688 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Released file lock "/var/lib/nova/instances/locks/nova-2cf2d05104349469a636cc36228e0092b559eadc" release /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:115
2016-12-30 10:54:33.688 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Releasing semaphore "2cf2d05104349469a636cc36228e0092b559eadc" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:33.688 27161 DEBUG nova.openstack.common.lockutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a ] Semaphore / lock released "copy_qcow2_image" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 10:54:33.689 27161 DEBUG nova.virt.libvirt.driver [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Start _get_guest_xml network_info=[VIF({'profile': {}, 'ovs_interfaceid': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'network': Network({'bridge': u'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': u'fixed', 'floating_ips': [], 'address': u'192.168.34.90'})], 'version': 4, 'meta': {u'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': u'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': u'gateway', 'address': None})})], 'meta': {u'injected': False, u'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tapee77baef-a6', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:37:81:0f', 'active': False, 'type': u'ovs', 'id': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'qbg_params': None})] disk_info={'disk_bus': 'virtio', 'cdrom_bus': 'ide', 'mapping': {u'/dev/vdb': {'bus': 'virtio', 'type': 'disk', 'dev': u'vdb'}, 'disk': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': u'vda'}, 'root': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': u'vda'}}} image_meta={'status': u'active', 'name': u'ceph-01', 'deleted': False, 'container_format': u'bare', 'created_at': datetime.datetime(2016, 12, 30, 2, 29, 16, tzinfo=<iso8601.iso8601.Utc object at 0x3a79fd0>), 'disk_format': u'qcow2', 'updated_at': datetime.datetime(2016, 12, 30, 2, 30, 18, tzinfo=<iso8601.iso8601.Utc object at 0x3a79fd0>), 'id': u'8231f02f-2ab0-4762-8b80-437ddb72b7cd', 'owner': u'516953ba1481400fb69642e9ffe14b02', 'min_ram': 0, 'checksum': u'a09d5f7e89bc8b6301085d04a5e7b926', 'min_disk': 100, 'is_public': False, 'deleted_at': None, 'properties': {u'instance_uuid': u'3e11d9ce-7e64-4029-a5cd-84afd39a5cda', u'image_location': u'snapshot', u'image_state': u'available', u'instance_type_memory_mb': u'16000', u'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', u'image_type': u'snapshot', u'instance_type_id': u'22', u'instance_type_name': u'fang-kafka', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'instance_type_root_gb': u'100', u'instance_type_flavorid': u'621313b5-1e0b-4819-a33b-68990cbb93ca', u'instance_type_vcpus': u'16', u'base_image_ref': u'17ed0c0a-55a6-4044-8ecf-8af8844b4199', u'instance_type_swap': u'0', u'owner_id': u'516953ba1481400fb69642e9ffe14b02'}, 'size': 2582052864} rescue=None block_device_info={'block_device_mapping': [{'guest_format': None, 'boot_index': None, 'mount_device': u'/dev/vdb', 'connection_info': {u'driver_volume_type': u'rbd', 'serial': u'2007fa22-4282-421b-bc23-439ea07cc86d', u'data': {u'secret_type': u'ceph', u'name': u'volumes/volume-2007fa22-4282-421b-bc23-439ea07cc86d', u'secret_uuid': u'3c97b1d3-1153-4454-8ffb-0fbf9ff01311', u'qos_specs': None, u'hosts': [u'10.20.8.31', u'10.20.8.32', u'10.20.8.33'], u'auth_enabled': True, u'access_mode': u'rw', u'auth_username': u'cinder', u'ports': [u'6789', u'6789', u'6789']}}, 'disk_bus': 'virtio', 'device_type': 'disk', 'delete_on_termination': False}], 'root_device_name': u'/dev/vda', 'ephemerals': [], 'swap': None} _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4156
2016-12-30 10:54:33.724 27161 DEBUG nova.objects.instance [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Lazy-loading `numa_topology' on Instance uuid 3e11d9ce-7e64-4029-a5cd-84afd39a5cda obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 10:54:33.736 27161 DEBUG nova.virt.libvirt.driver [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] CPU mode 'host-model' model '' was chosen _get_guest_cpu_model_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:3362
2016-12-30 10:54:33.736 27161 DEBUG nova.virt.hardware [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Getting desirable topologies for flavor Flavor(created_at=2016-08-31T10:42:03Z,deleted=False,deleted_at=None,disabled=False,ephemeral_gb=0,extra_specs={},flavorid='621313b5-1e0b-4819-a33b-68990cbb93ca',id=22,is_public=False,memory_mb=16000,name='fang-kafka',projects=<?>,root_gb=100,rxtx_factor=1.0,swap=0,updated_at=None,vcpu_weight=0,vcpus=16) and image_meta {'status': u'active', 'name': u'ceph-01', 'deleted': False, 'container_format': u'bare', 'created_at': datetime.datetime(2016, 12, 30, 2, 29, 16, tzinfo=<iso8601.iso8601.Utc object at 0x3a79fd0>), 'disk_format': u'qcow2', 'updated_at': datetime.datetime(2016, 12, 30, 2, 30, 18, tzinfo=<iso8601.iso8601.Utc object at 0x3a79fd0>), 'id': u'8231f02f-2ab0-4762-8b80-437ddb72b7cd', 'owner': u'516953ba1481400fb69642e9ffe14b02', 'min_ram': 0, 'checksum': u'a09d5f7e89bc8b6301085d04a5e7b926', 'min_disk': 100, 'is_public': False, 'deleted_at': None, 'properties': {u'instance_uuid': u'3e11d9ce-7e64-4029-a5cd-84afd39a5cda', u'image_location': u'snapshot', u'image_state': u'available', u'instance_type_memory_mb': u'16000', u'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', u'image_type': u'snapshot', u'instance_type_id': u'22', u'instance_type_name': u'fang-kafka', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'instance_type_root_gb': u'100', u'instance_type_flavorid': u'621313b5-1e0b-4819-a33b-68990cbb93ca', u'instance_type_vcpus': u'16', u'base_image_ref': u'17ed0c0a-55a6-4044-8ecf-8af8844b4199', u'instance_type_swap': u'0', u'owner_id': u'516953ba1481400fb69642e9ffe14b02'}, 'size': 2582052864} get_desirable_configs /usr/lib/python2.7/site-packages/nova/virt/hardware.py:502
2016-12-30 10:54:33.737 27161 DEBUG nova.virt.hardware [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Flavor limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:296
2016-12-30 10:54:33.737 27161 DEBUG nova.virt.hardware [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Image limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:309
2016-12-30 10:54:33.737 27161 DEBUG nova.virt.hardware [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Flavor pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:332
2016-12-30 10:54:33.738 27161 DEBUG nova.virt.hardware [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Image pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:354
2016-12-30 10:54:33.738 27161 DEBUG nova.virt.hardware [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Chosen -1:-1:-1 limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:383
2016-12-30 10:54:33.738 27161 DEBUG nova.virt.hardware [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Build topologies for 16 vcpu(s) 16:16:16 get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:419
2016-12-30 10:54:33.739 27161 DEBUG nova.virt.hardware [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Got 15 possible topologies get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:442
2016-12-30 10:54:33.767 27161 DEBUG nova.virt.libvirt.vif [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-26T07:41:49Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='trestle',display_name='ceph-01',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute03',hostname='trestle',id=198,image_ref='8231f02f-2ab0-4762-8b80-437ddb72b7cd',info_cache=InstanceInfoCache,instance_type_id=22,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2016-12-30T02:10:17Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=16000,metadata={},node='sjhl-o-compute03',numa_topology=None,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-h8jb081h',root_device_name='/dev/vda',root_gb=100,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={clean_attempts='1',image_base_image_ref='17ed0c0a-55a6-4044-8ecf-8af8844b4199',image_container_format='bare',image_disk_format='qcow2',image_image_location='snapshot',image_image_state='available',image_image_type='snapshot',image_instance_type_ephemeral_gb='0',image_instance_type_flavorid='621313b5-1e0b-4819-a33b-68990cbb93ca',image_instance_type_id='22',image_instance_type_memory_mb='16000',image_instance_type_name='fang-kafka',image_instance_type_root_gb='100',image_instance_type_rxtx_factor='1.0',image_instance_type_swap='0',image_instance_type_vcpus='16',image_instance_uuid='3e11d9ce-7e64-4029-a5cd-84afd39a5cda',image_min_disk='100',image_min_ram='0',image_owner_id='516953ba1481400fb69642e9ffe14b02',image_user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',instance_type_ephemeral_gb='0',instance_type_flavorid='621313b5-1e0b-4819-a33b-68990cbb93ca',instance_type_id='22',instance_type_memory_mb='16000',instance_type_name='fang-kafka',instance_type_root_gb='100',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='16'},task_state='rebuild_spawning',terminated_at=None,updated_at=2016-12-30T02:54:02Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=3e11d9ce-7e64-4029-a5cd-84afd39a5cda,vcpus=16,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'network': Network({'bridge': u'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': u'fixed', 'floating_ips': [], 'address': u'192.168.34.90'})], 'version': 4, 'meta': {u'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': u'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': u'gateway', 'address': None})})], 'meta': {u'injected': False, u'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tapee77baef-a6', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:37:81:0f', 'active': False, 'type': u'ovs', 'id': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'qbg_params': None}) virt_typekvm get_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:342
2016-12-30 10:54:33.769 27161 DEBUG nova.objects.instance [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Lazy-loading `pci_devices' on Instance uuid 3e11d9ce-7e64-4029-a5cd-84afd39a5cda obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 10:54:33.843 27161 DEBUG nova.virt.libvirt.config [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Generated XML ('<domain type="kvm">\n  <uuid>3e11d9ce-7e64-4029-a5cd-84afd39a5cda</uuid>\n  <name>instance-000000c6</name>\n  <memory>16384000</memory>\n  <vcpu>16</vcpu>\n  <metadata>\n    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">\n      <nova:package version="2014.2.2-1.el7"/>\n      <nova:name>ceph-01</nova:name>\n      <nova:creationTime>2016-12-30 02:54:33</nova:creationTime>\n      <nova:flavor name="fang-kafka">\n        <nova:memory>16000</nova:memory>\n        <nova:disk>100</nova:disk>\n        <nova:swap>0</nova:swap>\n        <nova:ephemeral>0</nova:ephemeral>\n        <nova:vcpus>16</nova:vcpus>\n      </nova:flavor>\n      <nova:owner>\n        <nova:user uuid="2a1a6dee31a0461c95a5ba8e7e1f30e1">fang</nova:user>\n        <nova:project uuid="516953ba1481400fb69642e9ffe14b02">fang</nova:project>\n      </nova:owner>\n      <nova:root type="image" uuid="8231f02f-2ab0-4762-8b80-437ddb72b7cd"/>\n    </nova:instance>\n  </metadata>\n  <sysinfo type="smbios">\n    <system>\n      <entry name="manufacturer">Fedora Project</entry>\n      <entry name="product">OpenStack Nova</entry>\n      <entry name="version">2014.2.2-1.el7</entry>\n      <entry name="serial">a6dae68f-0d87-4f34-9359-73beb2000e47</entry>\n      <entry name="uuid">3e11d9ce-7e64-4029-a5cd-84afd39a5cda</entry>\n    </system>\n  </sysinfo>\n  <os>\n    <type>hvm</type>\n    <boot dev="hd"/>\n    <smbios mode="sysinfo"/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n  </features>\n  <clock offset="utc">\n    <timer name="pit" tickpolicy="delay"/>\n    <timer name="rtc" tickpolicy="catchup"/>\n    <timer name="hpet" present="no"/>\n  </clock>\n  <cpu mode="host-model" match="exact">\n    <topology sockets="16" cores="1" threads="1"/>\n  </cpu>\n  <devices>\n    <disk type="file" device="disk">\n      <driver name="qemu" type="qcow2" cache="none"/>\n      <source file="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk"/>\n      <target bus="virtio" dev="vda"/>\n    </disk>\n    <disk type="network" device="disk">\n      <driver name="qemu" type="raw" cache="none"/>\n      <source protocol="rbd" name="volumes/volume-2007fa22-4282-421b-bc23-439ea07cc86d">\n        <host name="10.20.8.31" port="6789"/>\n        <host name="10.20.8.32" port="6789"/>\n        <host name="10.20.8.33" port="6789"/>\n      </source>\n      <auth username="cinder">\n        <secret type="ceph" uuid="3c97b1d3-1153-4454-8ffb-0fbf9ff01311"/>\n      </auth>\n      <target bus="virtio" dev="vdb"/>\n      <serial>2007fa22-4282-421b-bc23-439ea07cc86d</serial>\n    </disk>\n    <interface type="bridge">\n      <mac address="fa:16:3e:37:81:0f"/>\n      <model type="virtio"/>\n      <source bridge="qbree77baef-a6"/>\n      <target dev="tapee77baef-a6"/>\n    </interface>\n    <serial type="file">\n      <source path="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/console.log"/>\n    </serial>\n    <serial type="pty"/>\n    <input type="tablet" bus="usb"/>\n    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>\n    <video>\n      <model type="cirrus"/>\n    </video>\n    <memballoon model="virtio">\n      <stats period="10"/>\n    </memballoon>\n  </devices>\n</domain>\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82
2016-12-30 10:54:33.844 27161 DEBUG nova.virt.libvirt.driver [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] End _get_guest_xml xml=<domain type="kvm">
  <uuid>3e11d9ce-7e64-4029-a5cd-84afd39a5cda</uuid>
  <name>instance-000000c6</name>
  <memory>16384000</memory>
  <vcpu>16</vcpu>
  <metadata>
    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">
      <nova:package version="2014.2.2-1.el7"/>
      <nova:name>ceph-01</nova:name>
      <nova:creationTime>2016-12-30 02:54:33</nova:creationTime>
      <nova:flavor name="fang-kafka">
        <nova:memory>16000</nova:memory>
        <nova:disk>100</nova:disk>
        <nova:swap>0</nova:swap>
        <nova:ephemeral>0</nova:ephemeral>
        <nova:vcpus>16</nova:vcpus>
      </nova:flavor>
      <nova:owner>
        <nova:user uuid="2a1a6dee31a0461c95a5ba8e7e1f30e1">fang</nova:user>
        <nova:project uuid="516953ba1481400fb69642e9ffe14b02">fang</nova:project>
      </nova:owner>
      <nova:root type="image" uuid="8231f02f-2ab0-4762-8b80-437ddb72b7cd"/>
    </nova:instance>
  </metadata>
  <sysinfo type="smbios">
    <system>
      <entry name="manufacturer">Fedora Project</entry>
      <entry name="product">OpenStack Nova</entry>
      <entry name="version">2014.2.2-1.el7</entry>
      <entry name="serial">a6dae68f-0d87-4f34-9359-73beb2000e47</entry>
      <entry name="uuid">3e11d9ce-7e64-4029-a5cd-84afd39a5cda</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev="hd"/>
    <smbios mode="sysinfo"/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset="utc">
    <timer name="pit" tickpolicy="delay"/>
    <timer name="rtc" tickpolicy="catchup"/>
    <timer name="hpet" present="no"/>
  </clock>
  <cpu mode="host-model" match="exact">
    <topology sockets="16" cores="1" threads="1"/>
  </cpu>
  <devices>
    <disk type="file" device="disk">
      <driver name="qemu" type="qcow2" cache="none"/>
      <source file="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/disk"/>
      <target bus="virtio" dev="vda"/>
    </disk>
    <disk type="network" device="disk">
      <driver name="qemu" type="raw" cache="none"/>
      <source protocol="rbd" name="volumes/volume-2007fa22-4282-421b-bc23-439ea07cc86d">
        <host name="10.20.8.31" port="6789"/>
        <host name="10.20.8.32" port="6789"/>
        <host name="10.20.8.33" port="6789"/>
      </source>
      <auth username="cinder">
        <secret type="ceph" uuid="3c97b1d3-1153-4454-8ffb-0fbf9ff01311"/>
      </auth>
      <target bus="virtio" dev="vdb"/>
      <serial>2007fa22-4282-421b-bc23-439ea07cc86d</serial>
    </disk>
    <interface type="bridge">
      <mac address="fa:16:3e:37:81:0f"/>
      <model type="virtio"/>
      <source bridge="qbree77baef-a6"/>
      <target dev="tapee77baef-a6"/>
    </interface>
    <serial type="file">
      <source path="/var/lib/nova/instances/3e11d9ce-7e64-4029-a5cd-84afd39a5cda/console.log"/>
    </serial>
    <serial type="pty"/>
    <input type="tablet" bus="usb"/>
    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>
    <video>
      <model type="cirrus"/>
    </video>
    <memballoon model="virtio">
      <stats period="10"/>
    </memballoon>
  </devices>
</domain>
 _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4168
2016-12-30 10:54:33.866 27161 DEBUG nova.virt.libvirt.vif [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-26T07:41:49Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='trestle',display_name='ceph-01',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute03',hostname='trestle',id=198,image_ref='8231f02f-2ab0-4762-8b80-437ddb72b7cd',info_cache=InstanceInfoCache,instance_type_id=22,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2016-12-30T02:10:17Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=16000,metadata={},node='sjhl-o-compute03',numa_topology=None,os_type=None,pci_devices=PciDeviceList,power_state=1,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-h8jb081h',root_device_name='/dev/vda',root_gb=100,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={clean_attempts='1',image_base_image_ref='17ed0c0a-55a6-4044-8ecf-8af8844b4199',image_container_format='bare',image_disk_format='qcow2',image_image_location='snapshot',image_image_state='available',image_image_type='snapshot',image_instance_type_ephemeral_gb='0',image_instance_type_flavorid='621313b5-1e0b-4819-a33b-68990cbb93ca',image_instance_type_id='22',image_instance_type_memory_mb='16000',image_instance_type_name='fang-kafka',image_instance_type_root_gb='100',image_instance_type_rxtx_factor='1.0',image_instance_type_swap='0',image_instance_type_vcpus='16',image_instance_uuid='3e11d9ce-7e64-4029-a5cd-84afd39a5cda',image_min_disk='100',image_min_ram='0',image_owner_id='516953ba1481400fb69642e9ffe14b02',image_user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',instance_type_ephemeral_gb='0',instance_type_flavorid='621313b5-1e0b-4819-a33b-68990cbb93ca',instance_type_id='22',instance_type_memory_mb='16000',instance_type_name='fang-kafka',instance_type_root_gb='100',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='16'},task_state='rebuild_spawning',terminated_at=None,updated_at=2016-12-30T02:54:02Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=3e11d9ce-7e64-4029-a5cd-84afd39a5cda,vcpus=16,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'network': Network({'bridge': u'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': u'fixed', 'floating_ips': [], 'address': u'192.168.34.90'})], 'version': 4, 'meta': {u'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': u'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': u'gateway', 'address': None})})], 'meta': {u'injected': False, u'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tapee77baef-a6', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:37:81:0f', 'active': False, 'type': u'ovs', 'id': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'qbg_params': None}) plug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:531
2016-12-30 10:54:33.867 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl addbr qbree77baef-a6 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:33.950 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:33.951 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl setfd qbree77baef-a6 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.024 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.025 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl stp qbree77baef-a6 off execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.120 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.121 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf tee /sys/class/net/qbree77baef-a6/bridge/multicast_snooping execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.210 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.210 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link add qvbee77baef-a6 type veth peer name qvoee77baef-a6 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.287 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.287 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qvbee77baef-a6 up execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.356 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.357 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qvbee77baef-a6 promisc on execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.426 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.428 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qvoee77baef-a6 up execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.495 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.495 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qvoee77baef-a6 promisc on execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.562 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.563 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link set qbree77baef-a6 up execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.633 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.634 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl addif qbree77baef-a6 qvbee77baef-a6 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.726 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:34.727 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl --timeout=120 -- --if-exists del-port qvoee77baef-a6 -- add-port br-int qvoee77baef-a6 -- set Interface qvoee77baef-a6 external-ids:iface-id=ee77baef-a63f-490e-9c61-a9217b2ecce7 external-ids:iface-status=active external-ids:attached-mac=fa:16:3e:37:81:0f external-ids:vm-uuid=3e11d9ce-7e64-4029-a5cd-84afd39a5cda execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 10:54:34.807 27161 DEBUG nova.openstack.common.processutils [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 10:54:37.816 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rebooting_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:54:37.817 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 6.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 10:54:43.814 27161 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._heal_instance_info_cache run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 10:54:43.814 27161 DEBUG nova.compute.manager [-] Starting heal instance info cache _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5289
2016-12-30 10:54:43.907 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "refresh_cache-3e11d9ce-7e64-4029-a5cd-84afd39a5cda" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:43.907 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "refresh_cache-3e11d9ce-7e64-4029-a5cd-84afd39a5cda" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:43.907 27161 DEBUG nova.network.neutronv2.api [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
2016-12-30 10:54:43.908 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:43.908 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:43.908 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:43.909 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:43.909 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:43.909 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:43.909 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:43.909 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:43.910 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:43.910 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:43.910 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:43.910 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:43.911 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=3e11d9ce-7e64-4029-a5cd-84afd39a5cda -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:54:43.937 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:54:43 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-7c4d0317-3a1a-408d-b0d9-b87645f4ffc9'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.90"}], "id": "ee77baef-a63f-490e-9c61-a9217b2ecce7", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "3e11d9ce-7e64-4029-a5cd-84afd39a5cda", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:37:81:0f"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:54:43.947 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/networks.json?id=d647355e-b019-4de0-999e-00107531edc0 -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:54:43.967 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:54:43 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '376', 'x-openstack-request-id': 'req-1933eaaa-6fe9-4820-8d7e-8dcf9b89981b'} {"networks": [{"status": "ACTIVE", "subnets": ["b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"], "name": "DMZ_NET", "provider:physical_network": "physnet1", "admin_state_up": true, "tenant_id": "e599088c985f42e7948b12f601705cd3", "provider:network_type": "vlan", "router:external": false, "shared": true, "id": "d647355e-b019-4de0-999e-00107531edc0", "provider:segmentation_id": 502}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
REQ: curl -i http://controller.light.fang.com:9696/v2.0/floatingips.json?fixed_ip_address=192.168.34.90&port_id=ee77baef-a63f-490e-9c61-a9217b2ecce7 -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:54:43.985 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:54:43 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '19', 'x-openstack-request-id': 'req-69d4d4d9-c04d-4348-a3bd-b14f5762759d'} {"floatingips": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
REQ: curl -i http://controller.light.fang.com:9696/v2.0/subnets.json?id=b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:54:44.010 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:54:44 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '519', 'x-openstack-request-id': 'req-d3986d18-b7e2-4dd7-8c78-34a1e9b73be3'} {"subnets": [{"name": "DMZ_SUBNET", "enable_dhcp": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "dns_nameservers": ["218.30.110.17", "8.8.8.8"], "gateway_ip": null, "ipv6_ra_mode": null, "allocation_pools": [{"start": "192.168.34.3", "end": "192.168.34.250"}], "host_routes": [{"nexthop": "192.168.34.254", "destination": "0.0.0.0/0"}], "ip_version": 4, "ipv6_address_mode": null, "cidr": "192.168.34.0/24", "id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:54:44.017 27161 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?network_id=d647355e-b019-4de0-999e-00107531edc0&device_owner=network%3Adhcp -X GET -H "X-Auth-Token: 17f5075a081c43f9a853d130748ed096" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 10:54:44.042 27161 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 02:54:44 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '746', 'x-openstack-request-id': 'req-ce6a8601-e977-495f-acdb-afaaa4bbdc4a'} {"ports": [{"status": "ACTIVE", "binding:host_id": "dzc-o-neutron01v", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "network:dhcp", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.3"}], "id": "db21d3d0-9960-43c7-a811-53c63050a6ce", "security_groups": [], "device_id": "dhcp95fc7688-e5d9-5615-b6e9-fc077300258a-d647355e-b019-4de0-999e-00107531edc0", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:96:f0:bf"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 10:54:44.043 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:44.043 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:44.043 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:44.043 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:44.043 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:44.044 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:44.044 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:44.044 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:44.044 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:44.044 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:44.044 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:44.045 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:44.045 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:44.045 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:44.045 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:44.045 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:44.045 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:44.045 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:44.046 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:44.046 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:44.046 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:44.046 27161 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 10:54:44.046 27161 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 10:54:44.046 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:44.047 27161 DEBUG nova.network.base_api [-] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.90'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tapee77baef-a6', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:37:81:0f', 'active': False, 'type': u'ovs', 'id': u'ee77baef-a63f-490e-9c61-a9217b2ecce7', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2016-12-30 10:54:44.067 27161 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "refresh_cache-3e11d9ce-7e64-4029-a5cd-84afd39a5cda" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 10:54:44.067 27161 DEBUG nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Updated the network info_cache for instance _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5350
2016-12-30 10:54:44.067 27161 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2616ed0>> sleeping for 18.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 10:54:55.462 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483066495.46, 3e11d9ce-7e64-4029-a5cd-84afd39a5cda => Resumed> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 10:54:55.462 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] VM Resumed (Lifecycle Event)
2016-12-30 10:54:55.464 27161 DEBUG nova.virt.libvirt.driver [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Instance is running spawn /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:2623
2016-12-30 10:54:55.466 27161 INFO nova.virt.libvirt.driver [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Instance spawned successfully.
2016-12-30 10:54:55.467 27161 DEBUG nova.compute.manager [req-799f02e9-8254-4602-8c29-6a11b4f3979a None] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2016-12-30 10:54:55.517 27161 DEBUG nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] Synchronizing instance power state after lifecycle event "Resumed"; current vm_state: active, current task_state: rebuild_spawning, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 10:54:55.587 27161 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483066495.46, 3e11d9ce-7e64-4029-a5cd-84afd39a5cda => Started> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2016-12-30 10:54:55.588 27161 INFO nova.compute.manager [-] [instance: 3e11d9ce-7e64-4029-a5cd-84afd39a5cda] VM Started (Lifecycle Event)



Migrate 操作的作用是将 instance 从当前的计算节点迁移到其他节点上。
Migrate 不要求源和目标节点必须共享存储，当然共享存储也是可以的。 Migrate 前必须满足一个条件：计算节点间需要配置 nova 用户无密码访问。


scheduler日志
2016-12-30 11:31:48.683 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._run_periodic_tasks run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:31:48.684 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 26.16 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:32:14.852 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._expire_reservations run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:32:14.863 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 33.82 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:32:48.683 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._run_periodic_tasks run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:32:48.684 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 28.16 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:33:16.727 29308 WARNING nova.scheduler.host_manager [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Host has more disk space than database expected (389gb > 304gb)
2016-12-30 11:33:16.727 29308 WARNING nova.scheduler.host_manager [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Host has more disk space than database expected (362gb > 297gb)
2016-12-30 11:33:16.728 29308 WARNING nova.scheduler.host_manager [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Host has more disk space than database expected (284gb > 207gb)
2016-12-30 11:33:16.728 29308 WARNING nova.scheduler.host_manager [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Host has more disk space than database expected (514gb > 337gb)
2016-12-30 11:33:16.729 29308 WARNING nova.scheduler.host_manager [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Host has more disk space than database expected (374gb > 297gb)
2016-12-30 11:33:16.730 29308 AUDIT nova.scheduler.host_manager [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Host filter ignoring hosts: sjhl-o-compute03
2016-12-30 11:33:16.730 29308 DEBUG nova.filters [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Starting with 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:70
2016-12-30 11:33:16.731 29308 DEBUG nova.filters [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Filter RetryFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 11:33:16.789 29308 DEBUG nova.filters [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Filter AvailabilityZoneFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 11:33:16.790 29308 DEBUG nova.filters [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Filter RamFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 11:33:16.790 29308 DEBUG nova.servicegroup.drivers.db [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Seems service is down. Last heartbeat was 2016-12-25 00:28:00. Elapsed time is 443116.790704 is_up /usr/lib/python2.7/site-packages/nova/servicegroup/drivers/db.py:75
2016-12-30 11:33:16.791 29308 WARNING nova.scheduler.filters.compute_filter [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] (dzc-o-compute02, dzc-o-compute02) ram:31626 disk:278528 io_ops:0 instances:0 has not been heard from in a while
2016-12-30 11:33:16.791 29308 DEBUG nova.filters [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Filter ComputeFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 11:33:16.791 29308 DEBUG nova.filters [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Filter ComputeCapabilitiesFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 11:33:16.792 29308 DEBUG nova.filters [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Filter ImagePropertiesFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 11:33:16.793 29308 DEBUG nova.filters [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Filter ServerGroupAntiAffinityFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 11:33:16.795 29308 DEBUG nova.filters [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Filter ServerGroupAffinityFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 11:33:16.795 29308 DEBUG nova.scheduler.filter_scheduler [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Filtered [(sjhl-o-compute07, sjhl-o-compute07) ram:61328 disk:394240 io_ops:0 instances:3, (sjhl-o-compute08, sjhl-o-compute08) ram:152336 disk:345088 io_ops:0 instances:2, (sjhl-o-compute09, sjhl-o-compute09) ram:70800 disk:205824 io_ops:0 instances:5, (sjhl-o-compute04, sjhl-o-compute04) ram:70032 disk:304128 io_ops:0 instances:4, (sjhl-o-compute05, sjhl-o-compute05) ram:71184 disk:135168 io_ops:0 instances:5, (sjhl-o-compute10, sjhl-o-compute10) ram:70032 disk:304128 io_ops:0 instances:4, (dzc-o-compute01, dzc-o-compute01) ram:31626 disk:229376 io_ops:0 instances:0, (sjhl-o-compute06, sjhl-o-compute06) ram:70416 disk:211968 io_ops:0 instances:4] _schedule /usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:281
2016-12-30 11:33:16.796 29308 DEBUG nova.scheduler.filter_scheduler [req-730e9057-55be-47ac-bbfd-15dc364598d3 None] Weighed [WeighedHost [host: (sjhl-o-compute08, sjhl-o-compute08) ram:152336 disk:345088 io_ops:0 instances:2, weight: 1.0], WeighedHost [host: (sjhl-o-compute05, sjhl-o-compute05) ram:71184 disk:135168 io_ops:0 instances:5, weight: 0.46728284844], WeighedHost [host: (sjhl-o-compute09, sjhl-o-compute09) ram:70800 disk:205824 io_ops:0 instances:5, weight: 0.464762104821], WeighedHost [host: (sjhl-o-compute06, sjhl-o-compute06) ram:70416 disk:211968 io_ops:0 instances:4, weight: 0.462241361202], WeighedHost [host: (sjhl-o-compute04, sjhl-o-compute04) ram:70032 disk:304128 io_ops:0 instances:4, weight: 0.459720617582], WeighedHost [host: (sjhl-o-compute10, sjhl-o-compute10) ram:70032 disk:304128 io_ops:0 instances:4, weight: 0.459720617582], WeighedHost [host: (sjhl-o-compute07, sjhl-o-compute07) ram:61328 disk:394240 io_ops:0 instances:3, weight: 0.40258376221], WeighedHost [host: (dzc-o-compute01, dzc-o-compute01) ram:31626 disk:229376 io_ops:0 instances:0, weight: 0.207606869026]] _schedule /usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:286
2016-12-30 11:33:16.845 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._expire_reservations run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:33:16.852 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 32.83 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:33:49.684 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._run_periodic_tasks run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:33:49.684 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 29.16 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:34:18.845 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._expire_reservations run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:34:18.855 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 32.83 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:34:51.686 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._run_periodic_tasks run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:34:51.687 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 27.16 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:35:18.845 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._expire_reservations run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:35:18.855 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 33.83 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:35:52.687 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._run_periodic_tasks run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:35:52.688 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 28.16 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:36:20.848 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._expire_reservations run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:36:20.857 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 32.83 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:36:53.688 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._run_periodic_tasks run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:36:53.689 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 29.16 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132





源计算节点日志
Running cmd (subprocess): ssh 10.20.8.43 touch /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/8e413196101d47178bac61bf0f16c7c0.tmp

Running cmd (subprocess): ssh 10.20.8.43 mkdir -p /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b 
Shutting down instance from state 1 _clean_shutdown /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:2432
[instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] VM Stopped (Lifecycle Event)
instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Synchronizing instance power state after lifecycle event "Stopped"; current vm_state: active, current task_state: resize_migrating, current DB power_state: 1, VM power_state: 4 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2016-12-30 11:58:39.932 27161 INFO nova.compute.manager [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] During sync_power_state the instance has a pending task (resize_migrating). Skip.
2016-12-30 11:58:40.387 27161 INFO nova.virt.libvirt.driver [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Instance shutdown successfully after 3 seconds.
2016-12-30 11:58:40.390 27161 INFO nova.virt.libvirt.driver [-] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Instance destroyed successfully.

mv /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b_resize
qemu-img convert -f qcow2 -O qcow2 /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b_resize/disk /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b_resize/disk_rbase
rsync --sparse --compress --dry-run /var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b_resize/disk_rbase 10.20.8.43:/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk 
########################################################################################################################  
sjhl-o-compute08错误日志
2016-12-30 11:57:55.751 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 44.45 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:58:40.197 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:40.198 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rescued_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:40.198 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 1.71 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:58:41.907 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._reclaim_queued_deletes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:41.907 18293 DEBUG nova.compute.manager [-] CONF.reclaim_instance_interval <= 0, skipping... _reclaim_queued_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:5932
2016-12-30 11:58:41.907 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_unconfirmed_resizes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:41.908 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 0.99 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:58:42.903 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._check_instance_build_time run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:42.904 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 2.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:58:44.908 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._run_pending_deletes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:44.908 18293 DEBUG nova.compute.manager [-] Cleaning up deleted instances _run_pending_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:6223
2016-12-30 11:58:44.943 18293 DEBUG nova.compute.manager [-] There are 0 instances to clean _run_pending_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:6232
2016-12-30 11:58:44.944 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 1.96 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:58:46.908 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager.update_available_resource run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:46.908 18293 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2016-12-30 11:58:46.909 18293 DEBUG nova.virt.libvirt.driver [-] Updating host stats update_status /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:6399
2016-12-30 11:58:56.959 18293 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/59a37da5-1c21-4d0d-9629-2de59c188869/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 11:58:57.023 18293 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 11:58:57.024 18293 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/59a37da5-1c21-4d0d-9629-2de59c188869/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 11:58:57.081 18293 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 11:58:57.182 18293 DEBUG nova.compute.resource_tracker [-] Hypervisor: free ram (MB): 179642 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:454
2016-12-30 11:58:57.183 18293 DEBUG nova.compute.resource_tracker [-] Hypervisor: free disk (GB): 539 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:455
2016-12-30 11:58:57.183 18293 DEBUG nova.compute.resource_tracker [-] Hypervisor: free VCPUs: 8 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:460
2016-12-30 11:58:57.183 18293 DEBUG nova.compute.resource_tracker [-] Hypervisor: assignable PCI devices: [] _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:467
2016-12-30 11:58:57.183 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.183 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.184 18293 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 11:58:57.319 18293 AUDIT nova.compute.resource_tracker [-] Updating from migration 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b
2016-12-30 11:58:57.336 18293 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 25600
2016-12-30 11:58:57.336 18293 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 436
2016-12-30 11:58:57.336 18293 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 16
2016-12-30 11:58:57.336 18293 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2016-12-30 11:58:57.336 18293 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute08:sjhl-o-compute08
2016-12-30 11:58:57.337 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.337 18293 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 11:58:57.415 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rebooting_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:57.415 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_volume_usage run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:57.416 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._heal_instance_info_cache run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:58:57.416 18293 DEBUG nova.compute.manager [-] Starting heal instance info cache _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5289
2016-12-30 11:58:57.416 18293 DEBUG nova.compute.manager [-] Rebuilding the list of instances to heal _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5293
2016-12-30 11:58:57.461 18293 DEBUG nova.objects.instance [-] Lazy-loading `system_metadata' on Instance uuid 25fd252f-65de-45d5-8ae1-371acabfb730 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 11:58:57.545 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "refresh_cache-25fd252f-65de-45d5-8ae1-371acabfb730" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.545 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "refresh_cache-25fd252f-65de-45d5-8ae1-371acabfb730" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.545 18293 DEBUG nova.network.neutronv2.api [-] [instance: 25fd252f-65de-45d5-8ae1-371acabfb730] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
2016-12-30 11:58:57.546 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.546 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.546 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.547 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.547 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.547 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.548 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.548 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.548 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.549 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.549 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.549 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.549 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=25fd252f-65de-45d5-8ae1-371acabfb730 -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:58:57.578 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:58:57 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '743', 'x-openstack-request-id': 'req-6f0586bc-dab7-412c-b07a-928a79fb86ab'} {"ports": [{"status": "ACTIVE", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "e347242a-4fdb-40d8-901d-77987644ecca", "ip_address": "192.168.33.19"}], "id": "7636b137-627d-4656-9564-b3fcd7f0edcf", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "25fd252f-65de-45d5-8ae1-371acabfb730", "name": "", "admin_state_up": true, "network_id": "ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:03:dc:68"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:58:57.579 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.579 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.579 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.579 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.579 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.580 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.580 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.580 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.580 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.581 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.581 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.581 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.581 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.581 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.582 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.582 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.582 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.582 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.582 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.583 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.583 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.583 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.583 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.583 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.584 18293 DEBUG nova.objects.instance [-] Lazy-loading `info_cache' on Instance uuid 25fd252f-65de-45d5-8ae1-371acabfb730 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 11:58:57.645 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.645 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.645 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.646 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.646 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.646 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.646 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.647 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.647 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.647 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.647 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.647 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.648 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/networks.json?id=ba31704e-c1b6-4a43-8eb6-6162d25e7b29 -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:58:57.674 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:58:57 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '379', 'x-openstack-request-id': 'req-6a0e79d1-37c6-4c5f-bcab-a18a141258b6'} {"networks": [{"status": "ACTIVE", "subnets": ["e347242a-4fdb-40d8-901d-77987644ecca"], "name": "INSIDE_NET", "provider:physical_network": "physnet1", "admin_state_up": true, "tenant_id": "e599088c985f42e7948b12f601705cd3", "provider:network_type": "vlan", "router:external": false, "shared": true, "id": "ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "provider:segmentation_id": 501}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:58:57.674 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.674 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.674 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.675 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.675 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.675 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.675 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.675 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.676 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.676 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.676 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.676 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.676 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.677 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.677 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.677 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.677 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.677 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.677 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.678 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.678 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.678 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.678 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.678 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.679 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.679 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.679 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.679 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.679 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.679 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.680 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.680 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.680 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.680 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/floatingips.json?fixed_ip_address=192.168.33.19&port_id=7636b137-627d-4656-9564-b3fcd7f0edcf -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:58:57.689 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:58:57 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '19', 'x-openstack-request-id': 'req-70a456e1-5527-48bf-bcfd-dc4a5ce277fb'} {"floatingips": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:58:57.690 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.690 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.690 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.690 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.690 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.690 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.691 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.691 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.691 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.691 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.692 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.692 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.692 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.692 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.692 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.692 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.693 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.693 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.693 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.693 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.693 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.693 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.694 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.694 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.694 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.694 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.694 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.695 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.695 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.695 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.695 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.695 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.696 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.696 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.696 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.696 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.696 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/subnets.json?id=e347242a-4fdb-40d8-901d-77987644ecca -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:58:57.715 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:58:57 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '522', 'x-openstack-request-id': 'req-30f49de8-d6c5-4a5c-9e3c-c9f2c9ea2253'} {"subnets": [{"name": "INSIDE_SUBNET", "enable_dhcp": true, "network_id": "ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "tenant_id": "e599088c985f42e7948b12f601705cd3", "dns_nameservers": ["218.30.110.17", "8.8.8.8"], "gateway_ip": null, "ipv6_ra_mode": null, "allocation_pools": [{"start": "192.168.33.3", "end": "192.168.33.250"}], "host_routes": [{"nexthop": "192.168.33.254", "destination": "0.0.0.0/0"}], "ip_version": 4, "ipv6_address_mode": null, "cidr": "192.168.33.0/24", "id": "e347242a-4fdb-40d8-901d-77987644ecca"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:58:57.715 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.715 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.715 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.716 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.716 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.716 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.716 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.716 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.717 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.717 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.717 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.717 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.717 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.717 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.718 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.718 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.718 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.718 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.718 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.719 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.719 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.719 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.719 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.719 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.719 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.720 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.720 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.720 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.720 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.720 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.721 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.721 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.721 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.721 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.721 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.722 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.722 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?network_id=ba31704e-c1b6-4a43-8eb6-6162d25e7b29&device_owner=network%3Adhcp -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:58:57.745 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:58:57 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '746', 'x-openstack-request-id': 'req-0e852f76-25ec-4bc9-9abc-e611ab689e1a'} {"ports": [{"status": "ACTIVE", "binding:host_id": "dzc-o-neutron01v", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "network:dhcp", "binding:profile": {}, "fixed_ips": [{"subnet_id": "e347242a-4fdb-40d8-901d-77987644ecca", "ip_address": "192.168.33.4"}], "id": "905ede5c-4d14-4653-a78a-d0c11a3c0e1b", "security_groups": [], "device_id": "dhcp95fc7688-e5d9-5615-b6e9-fc077300258a-ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "name": "", "admin_state_up": true, "network_id": "ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:1d:e6:ac"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:58:57.746 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.746 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.746 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.746 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.746 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.747 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.747 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.747 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.747 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.747 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.748 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.748 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.748 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.748 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.748 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.749 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.749 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.749 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.749 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.749 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.749 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.750 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:58:57.750 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:58:57.750 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.751 18293 DEBUG nova.network.base_api [-] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'7636b137-627d-4656-9564-b3fcd7f0edcf', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.33.19'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.33.4'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.33.254'})})], 'cidr': u'192.168.33.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'ba31704e-c1b6-4a43-8eb6-6162d25e7b29', 'label': u'INSIDE_NET'}), 'devname': u'tap7636b137-62', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:03:dc:68', 'active': True, 'type': u'ovs', 'id': u'7636b137-627d-4656-9564-b3fcd7f0edcf', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2016-12-30 11:58:57.765 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "refresh_cache-25fd252f-65de-45d5-8ae1-371acabfb730" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:58:57.765 18293 DEBUG nova.compute.manager [-] [instance: 25fd252f-65de-45d5-8ae1-371acabfb730] Updated the network info_cache for instance _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5350
2016-12-30 11:58:57.766 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 44.49 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:59:42.260 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:59:42.261 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._reclaim_queued_deletes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:59:42.261 18293 DEBUG nova.compute.manager [-] CONF.reclaim_instance_interval <= 0, skipping... _reclaim_queued_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:5932
2016-12-30 11:59:42.261 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rescued_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:59:42.262 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 0.64 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:59:42.907 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_unconfirmed_resizes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:59:42.907 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 0.99 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:59:43.903 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._check_instance_build_time run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:59:43.904 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 3.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:59:46.906 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_volume_usage run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:59:46.907 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._heal_instance_info_cache run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:59:46.907 18293 DEBUG nova.compute.manager [-] Starting heal instance info cache _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5289
2016-12-30 11:59:46.987 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "refresh_cache-59a37da5-1c21-4d0d-9629-2de59c188869" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:46.987 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "refresh_cache-59a37da5-1c21-4d0d-9629-2de59c188869" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:46.987 18293 DEBUG nova.network.neutronv2.api [-] [instance: 59a37da5-1c21-4d0d-9629-2de59c188869] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
2016-12-30 11:59:46.987 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:46.987 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:46.988 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:46.988 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:46.988 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:46.989 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:46.989 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:46.989 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:46.989 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:46.990 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:46.990 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:46.990 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:46.990 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=59a37da5-1c21-4d0d-9629-2de59c188869 -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:47.026 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:47 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-a3ed960f-b1f6-4672-8dbe-213e109f25a5'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute03", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.11"}], "id": "51472ee3-bcd7-435b-a278-ccd452ed63a6", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "59a37da5-1c21-4d0d-9629-2de59c188869", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:a9:15:53"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:47.026 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.026 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.027 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.027 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.027 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.027 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.027 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.028 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.028 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.028 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.028 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.028 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.029 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.029 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.029 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.029 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.029 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.030 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.030 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.030 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.030 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.030 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.031 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.031 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.031 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.031 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.031 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.032 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.032 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.032 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.032 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.032 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.033 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.033 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.033 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.033 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.033 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/networks.json?id=d647355e-b019-4de0-999e-00107531edc0 -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:47.060 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:47 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '376', 'x-openstack-request-id': 'req-ce326fe5-841a-44ea-b70f-3678468ef803'} {"networks": [{"status": "ACTIVE", "subnets": ["b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"], "name": "DMZ_NET", "provider:physical_network": "physnet1", "admin_state_up": true, "tenant_id": "e599088c985f42e7948b12f601705cd3", "provider:network_type": "vlan", "router:external": false, "shared": true, "id": "d647355e-b019-4de0-999e-00107531edc0", "provider:segmentation_id": 502}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:47.061 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.061 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.061 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.061 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.061 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.061 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.062 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.062 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.062 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.062 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.062 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.063 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.063 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.063 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.063 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.063 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.063 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.064 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.064 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.064 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.064 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.064 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.064 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.065 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.065 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.065 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.065 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.066 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.066 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.066 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.066 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.066 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.066 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.067 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/floatingips.json?fixed_ip_address=192.168.34.11&port_id=51472ee3-bcd7-435b-a278-ccd452ed63a6 -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:47.077 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:47 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '19', 'x-openstack-request-id': 'req-b664612f-bd10-45b1-bc22-e43e7d14ae3a'} {"floatingips": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:47.077 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.077 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.077 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.078 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.078 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.078 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.078 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.078 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.078 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.079 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.079 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.079 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.079 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.079 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.080 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.080 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.080 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.080 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.080 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.080 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.081 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.081 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.081 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.081 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.081 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.081 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.082 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.082 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.082 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.082 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.082 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.083 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.083 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.083 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.083 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.083 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.083 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/subnets.json?id=b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:47.105 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:47 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '519', 'x-openstack-request-id': 'req-aacdf137-54be-4345-a50b-4987e4d59db5'} {"subnets": [{"name": "DMZ_SUBNET", "enable_dhcp": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "dns_nameservers": ["218.30.110.17", "8.8.8.8"], "gateway_ip": null, "ipv6_ra_mode": null, "allocation_pools": [{"start": "192.168.34.3", "end": "192.168.34.250"}], "host_routes": [{"nexthop": "192.168.34.254", "destination": "0.0.0.0/0"}], "ip_version": 4, "ipv6_address_mode": null, "cidr": "192.168.34.0/24", "id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:47.105 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.105 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.106 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.106 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.106 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.106 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.106 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.107 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.107 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.107 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.107 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.107 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.107 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.108 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.108 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.108 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.108 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.108 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.108 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.109 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.109 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.109 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.109 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.109 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.109 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.110 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.110 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.110 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.110 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.110 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.111 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.111 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.111 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.111 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.111 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.112 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.112 18293 DEBUG neutronclient.client [-] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?network_id=d647355e-b019-4de0-999e-00107531edc0&device_owner=network%3Adhcp -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:47.145 18293 DEBUG neutronclient.client [-] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:47 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '746', 'x-openstack-request-id': 'req-5e9c9a9d-317d-48cd-903a-e20c6bb10792'} {"ports": [{"status": "ACTIVE", "binding:host_id": "dzc-o-neutron01v", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "network:dhcp", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.3"}], "id": "db21d3d0-9960-43c7-a811-53c63050a6ce", "security_groups": [], "device_id": "dhcp95fc7688-e5d9-5615-b6e9-fc077300258a-d647355e-b019-4de0-999e-00107531edc0", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:96:f0:bf"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:47.145 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.145 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.146 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.146 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.146 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.146 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.146 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.147 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.147 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.147 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.147 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.147 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.148 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.148 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.148 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.148 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.148 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.148 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.149 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.149 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.149 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.149 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:47.149 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:47.149 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.150 18293 DEBUG nova.network.base_api [-] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'51472ee3-bcd7-435b-a278-ccd452ed63a6', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.11'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tap51472ee3-bc', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:a9:15:53', 'active': False, 'type': u'ovs', 'id': u'51472ee3-bcd7-435b-a278-ccd452ed63a6', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2016-12-30 11:59:47.170 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "refresh_cache-59a37da5-1c21-4d0d-9629-2de59c188869" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:47.170 18293 DEBUG nova.compute.manager [-] [instance: 59a37da5-1c21-4d0d-9629-2de59c188869] Updated the network info_cache for instance _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5350
2016-12-30 11:59:47.170 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 2.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 11:59:49.171 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager.update_available_resource run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 11:59:49.171 18293 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2016-12-30 11:59:49.171 18293 DEBUG nova.virt.libvirt.driver [-] Updating host stats update_status /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:6399
2016-12-30 11:59:52.562 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/extensions.json -X GET -H "X-Auth-Token: 5f95b041a0b047d2a32da0651fc59eb4" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:52.572 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:52 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '4111', 'x-openstack-request-id': 'req-6700cf7b-80c0-4742-9789-33a28ba688f9'} {"extensions": [{"updated": "2012-10-05T10:00:00-00:00", "name": "security-group", "links": [], "namespace": "http://docs.openstack.org/ext/securitygroups/api/v2.0", "alias": "security-group", "description": "The security groups extension."}, {"updated": "2013-02-07T10:00:00-00:00", "name": "L3 Agent Scheduler", "links": [], "namespace": "http://docs.openstack.org/ext/l3_agent_scheduler/api/v1.0", "alias": "l3_agent_scheduler", "description": "Schedule routers among l3 agents"}, {"updated": "2013-03-28T10:00:00-00:00", "name": "Neutron L3 Configurable external gateway mode", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/ext-gw-mode/api/v1.0", "alias": "ext-gw-mode", "description": "Extension of the router abstraction for specifying whether SNAT should occur on the external gateway"}, {"updated": "2014-02-03T10:00:00-00:00", "name": "Port Binding", "links": [], "namespace": "http://docs.openstack.org/ext/binding/api/v1.0", "alias": "binding", "description": "Expose port bindings of a virtual port to external application"}, {"updated": "2012-09-07T10:00:00-00:00", "name": "Provider Network", "links": [], "namespace": "http://docs.openstack.org/ext/provider/api/v1.0", "alias": "provider", "description": "Expose mapping of virtual networks to physical networks"}, {"updated": "2013-02-03T10:00:00-00:00", "name": "agent", "links": [], "namespace": "http://docs.openstack.org/ext/agent/api/v2.0", "alias": "agent", "description": "The agent management extension."}, {"updated": "2012-07-29T10:00:00-00:00", "name": "Quota management support", "links": [], "namespace": "http://docs.openstack.org/network/ext/quotas-sets/api/v2.0", "alias": "quotas", "description": "Expose functions for quotas management per tenant"}, {"updated": "2013-02-07T10:00:00-00:00", "name": "DHCP Agent Scheduler", "links": [], "namespace": "http://docs.openstack.org/ext/dhcp_agent_scheduler/api/v1.0", "alias": "dhcp_agent_scheduler", "description": "Schedule networks among dhcp agents"}, {"updated": "2014-04-26T00:00:00-00:00", "name": "HA Router extension", "links": [], "namespace": "", "alias": "l3-ha", "description": "Add HA capability to routers."}, {"updated": "2013-06-27T10:00:00-00:00", "name": "Multi Provider Network", "links": [], "namespace": "http://docs.openstack.org/ext/multi-provider/api/v1.0", "alias": "multi-provider", "description": "Expose mapping of virtual networks to multiple physical networks"}, {"updated": "2013-01-14T10:00:00-00:00", "name": "Neutron external network", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/external_net/api/v1.0", "alias": "external-net", "description": "Adds external network attribute to network resource."}, {"updated": "2012-07-20T10:00:00-00:00", "name": "Neutron L3 Router", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/router/api/v1.0", "alias": "router", "description": "Router abstraction for basic L3 forwarding between L2 Neutron networks and access to external networks via a NAT gateway."}, {"updated": "2013-07-23T10:00:00-00:00", "name": "Allowed Address Pairs", "links": [], "namespace": "http://docs.openstack.org/ext/allowedaddresspairs/api/v2.0", "alias": "allowed-address-pairs", "description": "Provides allowed address pairs"}, {"updated": "2013-02-01T10:00:00-00:00", "name": "Neutron Extra Route", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/extraroutes/api/v1.0", "alias": "extraroute", "description": "Extra routes configuration for L3 router"}, {"updated": "2013-03-17T12:00:00-00:00", "name": "Neutron Extra DHCP opts", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/extra_dhcp_opt/api/v1.0", "alias": "extra_dhcp_opt", "description": "Extra options configuration for DHCP. For example PXE boot options to DHCP clients can be specified (e.g. tftp-server, server-ip-address, bootfile-name)"}, {"updated": "2014-06-1T10:00:00-00:00", "name": "Distributed Virtual Router", "links": [], "namespace": "http://docs.openstack.org/ext/dvr/api/v1.0", "alias": "dvr", "description": "Enables configuration of Distributed Virtual Routers."}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:52.572 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.572 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.573 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.573 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.573 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.573 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.574 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.574 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.574 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.574 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.575 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.575 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.575 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:52.609 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:52 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-3cdeee4a-0c22-4267-8583-a9b06885f9ba'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.97"}], "id": "e647790f-0ffe-44c6-b887-67635a8137a0", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:9c:bc:87"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:52.609 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.609 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.610 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.610 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.610 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.610 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.611 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.611 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.611 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.611 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.612 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.612 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.612 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.612 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.613 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.613 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.613 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.613 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.614 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.614 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.614 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.615 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.615 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.615 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.615 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.616 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.616 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.616 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.616 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.616 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.617 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.617 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.617 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.617 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.618 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.618 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.618 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.618 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.618 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.619 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports/e647790f-0ffe-44c6-b887-67635a8137a0.json -X PUT -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient" -d '{"port": {"binding:host_id": "sjhl-o-compute08"}}'
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:52.791 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:52 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '739', 'x-openstack-request-id': 'req-3b579ed0-b0a7-413f-9522-70c5ce878618'} {"port": {"status": "BUILD", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.97"}], "id": "e647790f-0ffe-44c6-b887-67635a8137a0", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:9c:bc:87"}}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:52.792 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.792 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.793 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.793 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.793 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.793 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.794 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.794 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.794 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.795 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.795 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.795 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.796 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.796 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.796 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.797 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.797 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.797 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.797 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.798 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.798 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.798 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "refresh_cache-6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.799 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "refresh_cache-6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.799 18293 DEBUG nova.network.neutronv2.api [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
2016-12-30 11:59:52.799 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.800 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.800 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.800 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.801 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.801 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.801 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.801 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.802 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.802 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.802 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.802 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.803 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=516953ba1481400fb69642e9ffe14b02&device_id=6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:52.834 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:52 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-80e7f85e-f57b-43ec-8a6f-6de3811a843c'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute08", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.97"}], "id": "e647790f-0ffe-44c6-b887-67635a8137a0", "security_groups": ["edfca931-a45a-4f22-90f1-1294e9af43df"], "device_id": "6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "516953ba1481400fb69642e9ffe14b02", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:9c:bc:87"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:52.834 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.834 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.834 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.835 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.835 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.835 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.836 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.836 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.836 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.836 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.837 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.837 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.837 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.837 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.838 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.838 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.838 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.838 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.839 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.839 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.839 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.839 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.840 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.840 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.840 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/networks.json?id=d647355e-b019-4de0-999e-00107531edc0 -X GET -H "X-Auth-Token: 5f95b041a0b047d2a32da0651fc59eb4" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:52.869 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:52 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '376', 'x-openstack-request-id': 'req-7f79d2dc-aa2d-45da-aea5-d2a701990270'} {"networks": [{"status": "ACTIVE", "subnets": ["b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"], "name": "DMZ_NET", "provider:physical_network": "physnet1", "admin_state_up": true, "tenant_id": "e599088c985f42e7948b12f601705cd3", "provider:network_type": "vlan", "router:external": false, "shared": true, "id": "d647355e-b019-4de0-999e-00107531edc0", "provider:segmentation_id": 502}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:52.870 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.870 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.871 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.871 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.871 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.871 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.872 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.872 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.872 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.873 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/floatingips.json?fixed_ip_address=192.168.34.97&port_id=e647790f-0ffe-44c6-b887-67635a8137a0 -X GET -H "X-Auth-Token: 9c9e8d0901944bc0ad795b8f04cf001b" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:52.883 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:52 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '19', 'x-openstack-request-id': 'req-1901756d-7a93-4a7b-98c8-5be2829b5503'} {"floatingips": []}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:52.884 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.884 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.884 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.884 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.884 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.885 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.885 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.885 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.885 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.886 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.886 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.886 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.886 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.886 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.887 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.887 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.887 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.887 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.888 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.888 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.888 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.888 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:52.888 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:52.889 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:52.889 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/subnets.json?id=b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 -X GET -H "X-Auth-Token: 5f95b041a0b047d2a32da0651fc59eb4" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:52.906 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:52 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '519', 'x-openstack-request-id': 'req-acf8ce96-6e6f-4f5d-9d37-058a7692f835'} {"subnets": [{"name": "DMZ_SUBNET", "enable_dhcp": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "dns_nameservers": ["218.30.110.17", "8.8.8.8"], "gateway_ip": null, "ipv6_ra_mode": null, "allocation_pools": [{"start": "192.168.34.3", "end": "192.168.34.250"}], "host_routes": [{"nexthop": "192.168.34.254", "destination": "0.0.0.0/0"}], "ip_version": 4, "ipv6_address_mode": null, "cidr": "192.168.34.0/24", "id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:52.906 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?network_id=d647355e-b019-4de0-999e-00107531edc0&device_owner=network%3Adhcp -X GET -H "X-Auth-Token: 5f95b041a0b047d2a32da0651fc59eb4" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2016-12-30 11:59:52.934 18293 DEBUG neutronclient.client [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] RESP:200 {'date': 'Fri, 30 Dec 2016 03:59:52 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '746', 'x-openstack-request-id': 'req-ff62f5d2-77e0-40b1-8bdb-67c08aa8b6da'} {"ports": [{"status": "ACTIVE", "binding:host_id": "dzc-o-neutron01v", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "network:dhcp", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.3"}], "id": "db21d3d0-9960-43c7-a811-53c63050a6ce", "security_groups": [], "device_id": "dhcp95fc7688-e5d9-5615-b6e9-fc077300258a-d647355e-b019-4de0-999e-00107531edc0", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:96:f0:bf"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2016-12-30 11:59:52.934 18293 DEBUG nova.network.base_api [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.34.97'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape647790f-0f', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:9c:bc:87', 'active': False, 'type': u'ovs', 'id': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2016-12-30 11:59:52.950 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "refresh_cache-6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238





2016-12-30 11:59:53.057 18293 DEBUG nova.virt.libvirt.driver [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Starting finish_migration finish_migration /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:6026
2016-12-30 11:59:53.058 18293 DEBUG nova.block_device [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] block_device_list [] volume_in_mapping /usr/lib/python2.7/site-packages/nova/block_device.py:555
2016-12-30 11:59:53.059 18293 INFO nova.virt.libvirt.driver [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Creating image
2016-12-30 11:59:53.060 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk.info" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 11:59:53.060 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 11:59:53.060 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Got semaphore / lock "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 11:59:53.061 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 11:59:53.061 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Semaphore / lock released "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 11:59:53.109 18293 DEBUG glanceclient.common.http [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] curl -i -X HEAD -H 'X-Service-Catalog: [{"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "internalURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "id": "37eb9ae73b72467b8695247873083335"}], "type": "volumev2", "name": "cinderv2"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "internalURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "id": "30d052b7f62344dabab8c29928d29f99"}], "type": "volume", "name": "cinder"}]' -H 'X-Auth-Token: {SHA1}473d036b1f2a1b929d33ccdcb0d59e6c2def50ab' -H 'Accept-Encoding: gzip, deflate' -H 'Connection: keep-alive' -H 'Accept: */*' -H 'X-Roles: admin' -H 'User-Agent: python-glanceclient' -H 'X-Tenant-Id: e599088c985f42e7948b12f601705cd3' -H 'X-User-Id: 7d5b5abc30ea463690567e5f8cc794f9' -H 'X-Identity-Status: Confirmed' -H 'Content-Type: application/octet-stream' http://controller.light.fang.com:9292/v1/images/db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5 log_curl_request /usr/lib/python2.7/site-packages/glanceclient/common/http.py:122
2016-12-30 11:59:53.307 18293 DEBUG glanceclient.common.http [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] 
HTTP/1.1 200 OK
content-length: 0
x-image-meta-id: db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5
date: Fri, 30 Dec 2016 03:59:53 GMT
x-image-meta-deleted: False
x-image-meta-container_format: bare
connection: keep-alive
x-image-meta-checksum: 133eae9fb1c98f45894a4e60d8736619
x-image-meta-protected: False
x-image-meta-min_disk: 0
x-image-meta-created_at: 2015-12-18T08:55:31
x-image-meta-size: 13200896
x-image-meta-status: active
etag: 133eae9fb1c98f45894a4e60d8736619
x-image-meta-is_public: True
x-image-meta-min_ram: 0
x-image-meta-owner: e599088c985f42e7948b12f601705cd3
x-image-meta-updated_at: 2015-12-18T08:55:33
content-type: text/html; charset=UTF-8
x-openstack-request-id: req-4964f63f-059c-41b8-9f9b-1787b0f64fa8
x-image-meta-disk_format: qcow2
x-image-meta-name: cirros-0.3.3-x86_64
 log_http_response /usr/lib/python2.7/site-packages/glanceclient/common/http.py:135
2016-12-30 11:59:53.309 18293 DEBUG nova.virt.libvirt.driver [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Start _get_guest_xml network_info=[VIF({'profile': {}, 'ovs_interfaceid': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:9c:bc:87', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.97', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape647790f-0f', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:9c:bc:87', 'active': False, 'type': u'ovs', 'id': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'qbg_params': None})] disk_info={'disk_bus': 'virtio', 'cdrom_bus': 'ide', 'mapping': {'disk': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': 'vda'}, 'root': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': 'vda'}}} image_meta={u'min_disk': 1, u'container_format': u'bare', u'min_ram': 0, u'disk_format': u'qcow2', 'properties': {u'old_vm_state': u'active', 'old_instance_type_flavorid': u'1', 'old_instance_type_vcpus': u'1', u'instance_type_name': u'm1.tiny', u'new_instance_type_ephemeral_gb': u'0', u'base_image_ref': u'db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5', u'new_instance_type_root_gb': u'1', u'instance_type_ephemeral_gb': u'0', u'new_instance_type_rxtx_factor': u'1.0', u'new_instance_type_swap': u'0', 'old_instance_type_swap': u'0', 'old_instance_type_root_gb': u'1', 'old_instance_type_ephemeral_gb': u'0', 'old_instance_type_rxtx_factor': u'1.0', u'instance_type_root_gb': u'1', u'new_instance_type_name': u'm1.tiny', 'old_instance_type_id': u'2', u'new_instance_type_flavorid': u'1', u'instance_type_rxtx_factor': u'1.0', u'new_instance_type_memory_mb': u'512', u'new_instance_type_id': u'2', u'instance_type_vcpus': u'1', u'instance_type_memory_mb': u'512', u'instance_type_swap': u'0', u'new_instance_type_vcpus': u'1', 'old_instance_type_memory_mb': u'512', u'instance_type_id': u'2', u'instance_type_flavorid': u'1', 'old_instance_type_name': u'm1.tiny'}} rescue=None block_device_info={'swap': None, 'ephemerals': [], 'block_device_mapping': []} _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4156
2016-12-30 11:59:53.340 18293 DEBUG nova.objects.instance [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Lazy-loading `numa_topology' on Instance uuid 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 11:59:53.357 18293 DEBUG nova.virt.libvirt.driver [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] CPU mode 'host-model' model '' was chosen _get_guest_cpu_model_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:3362
2016-12-30 11:59:53.357 18293 DEBUG nova.virt.hardware [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Getting desirable topologies for flavor Flavor(created_at=None,deleted=False,deleted_at=None,disabled=False,ephemeral_gb=0,extra_specs={},flavorid='1',id=2,is_public=True,memory_mb=512,name='m1.tiny',projects=<?>,root_gb=1,rxtx_factor=1.0,swap=0,updated_at=None,vcpu_weight=0,vcpus=1) and image_meta {u'min_disk': 1, u'container_format': u'bare', u'min_ram': 0, u'disk_format': u'qcow2', 'properties': {u'old_vm_state': u'active', 'old_instance_type_flavorid': u'1', 'old_instance_type_vcpus': u'1', u'instance_type_name': u'm1.tiny', u'new_instance_type_ephemeral_gb': u'0', u'base_image_ref': u'db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5', u'new_instance_type_root_gb': u'1', u'instance_type_ephemeral_gb': u'0', u'new_instance_type_rxtx_factor': u'1.0', u'new_instance_type_swap': u'0', 'old_instance_type_swap': u'0', 'old_instance_type_root_gb': u'1', 'old_instance_type_ephemeral_gb': u'0', 'old_instance_type_rxtx_factor': u'1.0', u'instance_type_root_gb': u'1', u'new_instance_type_name': u'm1.tiny', 'old_instance_type_id': u'2', u'new_instance_type_flavorid': u'1', u'instance_type_rxtx_factor': u'1.0', u'new_instance_type_memory_mb': u'512', u'new_instance_type_id': u'2', u'instance_type_vcpus': u'1', u'instance_type_memory_mb': u'512', u'instance_type_swap': u'0', u'new_instance_type_vcpus': u'1', 'old_instance_type_memory_mb': u'512', u'instance_type_id': u'2', u'instance_type_flavorid': u'1', 'old_instance_type_name': u'm1.tiny'}} get_desirable_configs /usr/lib/python2.7/site-packages/nova/virt/hardware.py:502
2016-12-30 11:59:53.357 18293 DEBUG nova.virt.hardware [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Flavor limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:296
2016-12-30 11:59:53.358 18293 DEBUG nova.virt.hardware [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Image limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:309
2016-12-30 11:59:53.358 18293 DEBUG nova.virt.hardware [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Flavor pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:332
2016-12-30 11:59:53.358 18293 DEBUG nova.virt.hardware [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Image pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:354
2016-12-30 11:59:53.358 18293 DEBUG nova.virt.hardware [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Chosen -1:-1:-1 limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:383
2016-12-30 11:59:53.359 18293 DEBUG nova.virt.hardware [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Build topologies for 1 vcpu(s) 1:1:1 get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:419
2016-12-30 11:59:53.359 18293 DEBUG nova.virt.hardware [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Got 1 possible topologies get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:442
2016-12-30 11:59:53.367 18293 DEBUG nova.virt.libvirt.driver [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Path '/var/lib/nova/instances' supports direct I/O _supports_direct_io /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:2784
2016-12-30 11:59:53.369 18293 DEBUG nova.virt.libvirt.vif [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-29T08:08:24Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='asdasd',display_name='asdasd',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute08',hostname='asdasd',id=205,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=2,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2016-12-29T08:08:59Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=512,metadata={},node='sjhl-o-compute08',numa_topology=None,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-iu52nhwd',root_device_name='/dev/vda',root_gb=1,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='1',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='1',instance_type_id='2',instance_type_memory_mb='512',instance_type_name='m1.tiny',instance_type_root_gb='1',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1',new_instance_type_ephemeral_gb='0',new_instance_type_flavorid='1',new_instance_type_id='2',new_instance_type_memory_mb='512',new_instance_type_name='m1.tiny',new_instance_type_root_gb='1',new_instance_type_rxtx_factor='1.0',new_instance_type_swap='0',new_instance_type_vcpu_weight=None,new_instance_type_vcpus='1',old_instance_type_ephemeral_gb='0',old_instance_type_flavorid='1',old_instance_type_id='2',old_instance_type_memory_mb='512',old_instance_type_name='m1.tiny',old_instance_type_root_gb='1',old_instance_type_rxtx_factor='1.0',old_instance_type_swap='0',old_instance_type_vcpu_weight=None,old_instance_type_vcpus='1',old_vm_state='active'},task_state='resize_finish',terminated_at=None,updated_at=2016-12-30T03:59:53Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b,vcpus=1,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:9c:bc:87', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.97', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape647790f-0f', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:9c:bc:87', 'active': False, 'type': u'ovs', 'id': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'qbg_params': None}) virt_typekvm get_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:342
2016-12-30 11:59:53.371 18293 DEBUG nova.objects.instance [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Lazy-loading `pci_devices' on Instance uuid 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 11:59:53.431 18293 DEBUG nova.virt.libvirt.config [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Generated XML ('<domain type="kvm">\n  <uuid>6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</uuid>\n  <name>instance-000000cd</name>\n  <memory>524288</memory>\n  <vcpu cpuset="6-11,18-23">1</vcpu>\n  <metadata>\n    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">\n      <nova:package version="2014.2.2-1.el7"/>\n      <nova:name>asdasd</nova:name>\n      <nova:creationTime>2016-12-30 03:59:53</nova:creationTime>\n      <nova:flavor name="m1.tiny">\n        <nova:memory>512</nova:memory>\n        <nova:disk>1</nova:disk>\n        <nova:swap>0</nova:swap>\n        <nova:ephemeral>0</nova:ephemeral>\n        <nova:vcpus>1</nova:vcpus>\n      </nova:flavor>\n      <nova:owner>\n        <nova:user uuid="7d5b5abc30ea463690567e5f8cc794f9">admin</nova:user>\n        <nova:project uuid="e599088c985f42e7948b12f601705cd3">admin</nova:project>\n      </nova:owner>\n      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>\n    </nova:instance>\n  </metadata>\n  <sysinfo type="smbios">\n    <system>\n      <entry name="manufacturer">Fedora Project</entry>\n      <entry name="product">OpenStack Nova</entry>\n      <entry name="version">2014.2.2-1.el7</entry>\n      <entry name="serial">9e3cdbcf-990a-4540-b9a9-7a000901d4a8</entry>\n      <entry name="uuid">6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</entry>\n    </system>\n  </sysinfo>\n  <os>\n    <type>hvm</type>\n    <boot dev="hd"/>\n    <smbios mode="sysinfo"/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n  </features>\n  <clock offset="utc">\n    <timer name="pit" tickpolicy="delay"/>\n    <timer name="rtc" tickpolicy="catchup"/>\n    <timer name="hpet" present="no"/>\n  </clock>\n  <cpu mode="host-model" match="exact">\n    <topology sockets="1" cores="1" threads="1"/>\n  </cpu>\n  <devices>\n    <disk type="file" device="disk">\n      <driver name="qemu" type="qcow2" cache="none"/>\n      <source file="/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk"/>\n      <target bus="virtio" dev="vda"/>\n    </disk>\n    <interface type="bridge">\n      <mac address="fa:16:3e:9c:bc:87"/>\n      <model type="virtio"/>\n      <source bridge="qbre647790f-0f"/>\n      <target dev="tape647790f-0f"/>\n    </interface>\n    <serial type="file">\n      <source path="/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/console.log"/>\n    </serial>\n    <serial type="pty"/>\n    <input type="tablet" bus="usb"/>\n    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>\n    <video>\n      <model type="cirrus"/>\n    </video>\n    <memballoon model="virtio">\n      <stats period="10"/>\n    </memballoon>\n  </devices>\n</domain>\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82
2016-12-30 11:59:53.432 18293 DEBUG nova.virt.libvirt.driver [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] End _get_guest_xml xml=<domain type="kvm">
  <uuid>6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</uuid>
  <name>instance-000000cd</name>
  <memory>524288</memory>
  <vcpu cpuset="6-11,18-23">1</vcpu>
  <metadata>
    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">
      <nova:package version="2014.2.2-1.el7"/>
      <nova:name>asdasd</nova:name>
      <nova:creationTime>2016-12-30 03:59:53</nova:creationTime>
      <nova:flavor name="m1.tiny">
        <nova:memory>512</nova:memory>
        <nova:disk>1</nova:disk>
        <nova:swap>0</nova:swap>
        <nova:ephemeral>0</nova:ephemeral>
        <nova:vcpus>1</nova:vcpus>
      </nova:flavor>
      <nova:owner>
        <nova:user uuid="7d5b5abc30ea463690567e5f8cc794f9">admin</nova:user>
        <nova:project uuid="e599088c985f42e7948b12f601705cd3">admin</nova:project>
      </nova:owner>
      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>
    </nova:instance>
  </metadata>
  <sysinfo type="smbios">
    <system>
      <entry name="manufacturer">Fedora Project</entry>
      <entry name="product">OpenStack Nova</entry>
      <entry name="version">2014.2.2-1.el7</entry>
      <entry name="serial">9e3cdbcf-990a-4540-b9a9-7a000901d4a8</entry>
      <entry name="uuid">6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev="hd"/>
    <smbios mode="sysinfo"/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset="utc">
    <timer name="pit" tickpolicy="delay"/>
    <timer name="rtc" tickpolicy="catchup"/>
    <timer name="hpet" present="no"/>
  </clock>
  <cpu mode="host-model" match="exact">
    <topology sockets="1" cores="1" threads="1"/>
  </cpu>
  <devices>
    <disk type="file" device="disk">
      <driver name="qemu" type="qcow2" cache="none"/>
      <source file="/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk"/>
      <target bus="virtio" dev="vda"/>
    </disk>
    <interface type="bridge">
      <mac address="fa:16:3e:9c:bc:87"/>
      <model type="virtio"/>
      <source bridge="qbre647790f-0f"/>
      <target dev="tape647790f-0f"/>
    </interface>
    <serial type="file">
      <source path="/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/console.log"/>
    </serial>
    <serial type="pty"/>
    <input type="tablet" bus="usb"/>
    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>
    <video>
      <model type="cirrus"/>
    </video>
    <memballoon model="virtio">
      <stats period="10"/>
    </memballoon>
  </devices>
</domain>
 _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4168
2016-12-30 11:59:53.433 18293 DEBUG nova.virt.libvirt.vif [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-29T08:08:24Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='asdasd',display_name='asdasd',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute08',hostname='asdasd',id=205,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=2,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2016-12-29T08:08:59Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=512,metadata={},node='sjhl-o-compute08',numa_topology=None,os_type=None,pci_devices=PciDeviceList,power_state=1,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-iu52nhwd',root_device_name='/dev/vda',root_gb=1,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='1',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='1',instance_type_id='2',instance_type_memory_mb='512',instance_type_name='m1.tiny',instance_type_root_gb='1',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1',new_instance_type_ephemeral_gb='0',new_instance_type_flavorid='1',new_instance_type_id='2',new_instance_type_memory_mb='512',new_instance_type_name='m1.tiny',new_instance_type_root_gb='1',new_instance_type_rxtx_factor='1.0',new_instance_type_swap='0',new_instance_type_vcpu_weight=None,new_instance_type_vcpus='1',old_instance_type_ephemeral_gb='0',old_instance_type_flavorid='1',old_instance_type_id='2',old_instance_type_memory_mb='512',old_instance_type_name='m1.tiny',old_instance_type_root_gb='1',old_instance_type_rxtx_factor='1.0',old_instance_type_swap='0',old_instance_type_vcpu_weight=None,old_instance_type_vcpus='1',old_vm_state='active'},task_state='resize_finish',terminated_at=None,updated_at=2016-12-30T03:59:53Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b,vcpus=1,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:9c:bc:87', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.97', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tape647790f-0f', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:9c:bc:87', 'active': False, 'type': u'ovs', 'id': u'e647790f-0ffe-44c6-b887-67635a8137a0', 'qbg_params': None}) plug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:531
2016-12-30 12:00:18.756 18293 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/59a37da5-1c21-4d0d-9629-2de59c188869/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 12:00:18.769 18293 ERROR nova.virt.libvirt.driver [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Error launching a defined domain with XML: <domain type='kvm'>
  <name>instance-000000cd</name>
  <uuid>6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</uuid>
  <metadata>
    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">
      <nova:package version="2014.2.2-1.el7"/>
      <nova:name>asdasd</nova:name>
      <nova:creationTime>2016-12-30 03:59:53</nova:creationTime>
      <nova:flavor name="m1.tiny">
        <nova:memory>512</nova:memory>
        <nova:disk>1</nova:disk>
        <nova:swap>0</nova:swap>
        <nova:ephemeral>0</nova:ephemeral>
        <nova:vcpus>1</nova:vcpus>
      </nova:flavor>
      <nova:owner>
        <nova:user uuid="7d5b5abc30ea463690567e5f8cc794f9">admin</nova:user>
        <nova:project uuid="e599088c985f42e7948b12f601705cd3">admin</nova:project>
      </nova:owner>
      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>
    </nova:instance>
  </metadata>
  <memory unit='KiB'>524288</memory>
  <currentMemory unit='KiB'>524288</currentMemory>
  <vcpu placement='static' cpuset='6-11,18-23'>1</vcpu>
  <sysinfo type='smbios'>
    <system>
      <entry name='manufacturer'>Fedora Project</entry>
      <entry name='product'>OpenStack Nova</entry>
      <entry name='version'>2014.2.2-1.el7</entry>
      <entry name='serial'>9e3cdbcf-990a-4540-b9a9-7a000901d4a8</entry>
      <entry name='uuid'>6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b</entry>
    </system>
  </sysinfo>
  <os>
    <type arch='x86_64' machine='pc-i440fx-rhel7.0.0'>hvm</type>
    <boot dev='hd'/>
    <smbios mode='sysinfo'/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <cpu mode='host-model'>
    <model fallback='allow'/>
    <topology sockets='1' cores='1' threads='1'/>
  </cpu>
  <clock offset='utc'>
    <timer name='pit' tickpolicy='delay'/>
    <timer name='rtc' tickpolicy='catchup'/>
    <timer name='hpet' present='no'/>
  </clock>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/disk'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </disk>
    <controller type='usb' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>
    </controller>
    <controller type='pci' index='0' model='pci-root'/>
    <interface type='bridge'>
      <mac address='fa:16:3e:9c:bc:87'/>
      <source bridge='qbre647790f-0f'/>
      <target dev='tape647790f-0f'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='file'>
      <source path='/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/console.log'/>
      <target port='0'/>
    </serial>
    <serial type='pty'>
      <target port='1'/>
    </serial>
    <console type='file'>
      <source path='/var/lib/nova/instances/6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b/console.log'/>
      <target type='serial' port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0' keymap='en-us'>
      <listen type='address' address='0.0.0.0'/>
    </graphics>
    <video>
      <model type='cirrus' vram='16384' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <stats period='10'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
</domain>

2016-12-30 12:00:18.769 18293 ERROR nova.compute.manager [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Setting instance vm_state to ERROR
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] Traceback (most recent call last):
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3904, in finish_resize
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     disk_info, image)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3872, in _finish_resize
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     old_instance_type, sys_meta)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     six.reraise(self.type_, self.value, self.tb)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3867, in _finish_resize
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     block_device_info, power_on)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 6051, in finish_migration
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     vifs_already_plugged=True)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4425, in _create_domain_and_network
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     power_on=power_on)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4349, in _create_domain
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     LOG.error(err)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     six.reraise(self.type_, self.value, self.tb)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4339, in _create_domain
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     domain.createWithFlags(launch_flags)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 183, in doit
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     result = proxy_call(self._autowrap, f, *args, **kwargs)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 141, in proxy_call
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     rv = execute(f, *args, **kwargs)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 122, in execute
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     six.reraise(c, e, tb)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 80, in tworker
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     rv = meth(*args, **kwargs)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]   File "/usr/lib64/python2.7/site-packages/libvirt.py", line 1059, in createWithFlags
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b]     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] libvirtError: Activation of org.freedesktop.machine1 timed out
2016-12-30 12:00:18.769 18293 TRACE nova.compute.manager [instance: 6ff39e2e-7b63-4b7c-8c9d-4ecc2b16f27b] 
2016-12-30 12:00:18.834 18293 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 12:00:18.835 18293 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/59a37da5-1c21-4d0d-9629-2de59c188869/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 12:00:18.899 18293 DEBUG nova.openstack.common.processutils [-] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 12:00:18.907 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 12:00:18.907 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 12:00:18.907 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Got semaphore / lock "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 12:00:18.908 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 12:00:18.908 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Semaphore / lock released "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 12:00:19.038 18293 DEBUG nova.compute.resource_tracker [-] Hypervisor: free ram (MB): 179639 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:454
2016-12-30 12:00:19.038 18293 DEBUG nova.compute.resource_tracker [-] Hypervisor: free disk (GB): 539 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:455
2016-12-30 12:00:19.038 18293 DEBUG nova.compute.resource_tracker [-] Hypervisor: free VCPUs: 8 _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:460
2016-12-30 12:00:19.039 18293 DEBUG nova.compute.resource_tracker [-] Hypervisor: assignable PCI devices: [] _report_hypervisor_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:467
2016-12-30 12:00:19.039 18293 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 12:00:19.039 18293 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 12:00:19.039 18293 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 12:00:19.070 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Using existing semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:202
2016-12-30 12:00:19.149 18293 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 25600
2016-12-30 12:00:19.149 18293 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 436
2016-12-30 12:00:19.149 18293 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 16
2016-12-30 12:00:19.150 18293 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2016-12-30 12:00:19.175 18293 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute08', 'sjhl-o-compute08')
2016-12-30 12:00:19.175 18293 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute08:sjhl-o-compute08
2016-12-30 12:00:19.176 18293 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 12:00:19.176 18293 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released "_update_available_resource" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 12:00:19.177 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 12:00:19.177 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Got semaphore / lock "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 12:00:19.213 18293 INFO nova.scheduler.client.report [req-02210811-876c-4f72-b2fe-ada6c6afdb2d None] Compute_service record updated for ('sjhl-o-compute08', 'sjhl-o-compute08')
2016-12-30 12:00:19.213 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 12:00:19.213 18293 DEBUG nova.openstack.common.lockutils [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Semaphore / lock released "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 12:00:19.216 18293 ERROR oslo.messaging.rpc.dispatcher [req-02210811-876c-4f72-b2fe-ada6c6afdb2d ] Exception during message handling: Activation of org.freedesktop.machine1 timed out
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py", line 134, in _dispatch_and_reply
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py", line 177, in _dispatch
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py", line 123, in _do_dispatch
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/exception.py", line 88, in wrapped
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     payload)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/exception.py", line 71, in wrapped
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 298, in decorated_function
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     pass
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 284, in decorated_function
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 348, in decorated_function
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 272, in decorated_function
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     migration.instance_uuid, exc_info=True)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 259, in decorated_function
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 326, in decorated_function
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 314, in decorated_function
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3916, in finish_resize
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     self._set_instance_error_state(context, instance)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3904, in finish_resize
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     disk_info, image)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3872, in _finish_resize
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     old_instance_type, sys_meta)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3867, in _finish_resize
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     block_device_info, power_on)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 6051, in finish_migration
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher     vifs_already_plugged=True)
2016-12-30 12:00:19.216 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4425, in _create_domain_and_network






由于sjhl-o-compute08之前创建失败了，所以retry_filter 去掉了
2016-12-30 15:47:00.056 29308 WARNING nova.scheduler.host_manager [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host has more disk space than database expected (362gb > 297gb)
2016-12-30 15:47:00.057 29308 WARNING nova.scheduler.host_manager [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host has more disk space than database expected (284gb > 207gb)
2016-12-30 15:47:00.057 29308 WARNING nova.scheduler.host_manager [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host has more disk space than database expected (512gb > 431gb)
2016-12-30 15:47:00.058 29308 WARNING nova.scheduler.host_manager [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host has more disk space than database expected (374gb > 297gb)
2016-12-30 15:47:00.059 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Starting with 10 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:70
2016-12-30 15:47:00.059 29308 DEBUG nova.scheduler.filters.retry_filter [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host [u'sjhl-o-compute08', u'sjhl-o-compute08'] fails.  Previously tried hosts: [[u'sjhl-o-compute08', u'sjhl-o-compute08']] host_passes /usr/lib/python2.7/site-packages/nova/scheduler/filters/retry_filter.py:42
2016-12-30 15:47:00.060 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter RetryFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:00.113 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter AvailabilityZoneFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:00.114 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter RamFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:00.114 29308 DEBUG nova.servicegroup.drivers.db [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Seems service is down. Last heartbeat was 2016-12-25 00:28:00. Elapsed time is 458340.114643 is_up /usr/lib/python2.7/site-packages/nova/servicegroup/drivers/db.py:75
2016-12-30 15:47:00.115 29308 WARNING nova.scheduler.filters.compute_filter [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] (dzc-o-compute02, dzc-o-compute02) ram:31626 disk:278528 io_ops:0 instances:0 has not been heard from in a while
2016-12-30 15:47:00.115 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ComputeFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:00.115 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ComputeCapabilitiesFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:00.116 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ImagePropertiesFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:00.117 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ServerGroupAntiAffinityFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:00.118 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ServerGroupAffinityFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:00.119 29308 DEBUG nova.scheduler.filter_scheduler [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filtered [(sjhl-o-compute07, sjhl-o-compute07) ram:61328 disk:394240 io_ops:0 instances:3, (sjhl-o-compute05, sjhl-o-compute05) ram:71184 disk:135168 io_ops:0 instances:5, (dzc-o-compute01, dzc-o-compute01) ram:31626 disk:229376 io_ops:0 instances:0, (sjhl-o-compute03, sjhl-o-compute03) ram:94608 disk:401408 io_ops:0 instances:3, (sjhl-o-compute09, sjhl-o-compute09) ram:70800 disk:205824 io_ops:0 instances:5, (sjhl-o-compute10, sjhl-o-compute10) ram:70032 disk:304128 io_ops:0 instances:4, (sjhl-o-compute04, sjhl-o-compute04) ram:70032 disk:304128 io_ops:0 instances:4, (sjhl-o-compute06, sjhl-o-compute06) ram:70416 disk:211968 io_ops:0 instances:4] _schedule /usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:281
2016-12-30 15:47:00.119 29308 DEBUG nova.scheduler.filter_scheduler [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Weighed [WeighedHost [host: (sjhl-o-compute03, sjhl-o-compute03) ram:94608 disk:401408 io_ops:0 instances:3, weight: 1.0], WeighedHost [host: (sjhl-o-compute05, sjhl-o-compute05) ram:71184 disk:135168 io_ops:0 instances:5, weight: 0.752409944191], WeighedHost [host: (sjhl-o-compute09, sjhl-o-compute09) ram:70800 disk:205824 io_ops:0 instances:5, weight: 0.748351090817], WeighedHost [host: (sjhl-o-compute06, sjhl-o-compute06) ram:70416 disk:211968 io_ops:0 instances:4, weight: 0.744292237443], WeighedHost [host: (sjhl-o-compute10, sjhl-o-compute10) ram:70032 disk:304128 io_ops:0 instances:4, weight: 0.740233384069], WeighedHost [host: (sjhl-o-compute04, sjhl-o-compute04) ram:70032 disk:304128 io_ops:0 instances:4, weight: 0.740233384069], WeighedHost [host: (sjhl-o-compute07, sjhl-o-compute07) ram:61328 disk:394240 io_ops:0 instances:3, weight: 0.648232707593], WeighedHost [host: (dzc-o-compute01, dzc-o-compute01) ram:31626 disk:229376 io_ops:0 instances:0, weight: 0.334284627093]] _schedule /usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:286
2016-12-30 15:47:06.689 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._run_periodic_tasks run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 15:47:06.690 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 21.15 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 15:47:25.081 29308 WARNING nova.scheduler.host_manager [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host has more disk space than database expected (362gb > 297gb)
2016-12-30 15:47:25.082 29308 WARNING nova.scheduler.host_manager [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host has more disk space than database expected (284gb > 207gb)
2016-12-30 15:47:25.082 29308 WARNING nova.scheduler.host_manager [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host has more disk space than database expected (512gb > 433gb)
2016-12-30 15:47:25.083 29308 WARNING nova.scheduler.host_manager [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host has more disk space than database expected (374gb > 297gb)
2016-12-30 15:47:25.085 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Starting with 10 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:70
2016-12-30 15:47:25.085 29308 DEBUG nova.scheduler.filters.retry_filter [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Host [u'sjhl-o-compute08', u'sjhl-o-compute08'] fails.  Previously tried hosts: [[u'sjhl-o-compute08', u'sjhl-o-compute08']] host_passes /usr/lib/python2.7/site-packages/nova/scheduler/filters/retry_filter.py:42
2016-12-30 15:47:25.086 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter RetryFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:25.150 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter AvailabilityZoneFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:25.150 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter RamFilter returned 9 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:25.151 29308 DEBUG nova.servicegroup.drivers.db [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Seems service is down. Last heartbeat was 2016-12-25 00:28:00. Elapsed time is 458365.151338 is_up /usr/lib/python2.7/site-packages/nova/servicegroup/drivers/db.py:75
2016-12-30 15:47:25.151 29308 WARNING nova.scheduler.filters.compute_filter [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] (dzc-o-compute02, dzc-o-compute02) ram:31626 disk:278528 io_ops:0 instances:0 has not been heard from in a while
2016-12-30 15:47:25.152 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ComputeFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:25.152 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ComputeCapabilitiesFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:25.153 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ImagePropertiesFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:25.156 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ServerGroupAntiAffinityFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:25.158 29308 DEBUG nova.filters [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filter ServerGroupAffinityFilter returned 8 host(s) get_filtered_objects /usr/lib/python2.7/site-packages/nova/filters.py:88
2016-12-30 15:47:25.159 29308 DEBUG nova.scheduler.filter_scheduler [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Filtered [(sjhl-o-compute07, sjhl-o-compute07) ram:61328 disk:394240 io_ops:0 instances:3, (sjhl-o-compute05, sjhl-o-compute05) ram:71184 disk:135168 io_ops:0 instances:5, (dzc-o-compute01, dzc-o-compute01) ram:31626 disk:229376 io_ops:0 instances:0, (sjhl-o-compute03, sjhl-o-compute03) ram:94096 disk:400384 io_ops:1 instances:4, (sjhl-o-compute09, sjhl-o-compute09) ram:70800 disk:205824 io_ops:0 instances:5, (sjhl-o-compute10, sjhl-o-compute10) ram:70032 disk:304128 io_ops:0 instances:4, (sjhl-o-compute04, sjhl-o-compute04) ram:70032 disk:304128 io_ops:0 instances:4, (sjhl-o-compute06, sjhl-o-compute06) ram:70416 disk:211968 io_ops:0 instances:4] _schedule /usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:281
2016-12-30 15:47:25.160 29308 DEBUG nova.scheduler.filter_scheduler [req-bf55c4de-20ae-47e9-9891-fc3bed6c9321 None] Weighed [WeighedHost [host: (sjhl-o-compute03, sjhl-o-compute03) ram:94096 disk:400384 io_ops:1 instances:4, weight: 1.0], WeighedHost [host: (sjhl-o-compute05, sjhl-o-compute05) ram:71184 disk:135168 io_ops:0 instances:5, weight: 0.756503995919], WeighedHost [host: (sjhl-o-compute09, sjhl-o-compute09) ram:70800 disk:205824 io_ops:0 instances:5, weight: 0.752423057303], WeighedHost [host: (sjhl-o-compute06, sjhl-o-compute06) ram:70416 disk:211968 io_ops:0 instances:4, weight: 0.748342118687], WeighedHost [host: (sjhl-o-compute10, sjhl-o-compute10) ram:70032 disk:304128 io_ops:0 instances:4, weight: 0.744261180071], WeighedHost [host: (sjhl-o-compute04, sjhl-o-compute04) ram:70032 disk:304128 io_ops:0 instances:4, weight: 0.744261180071], WeighedHost [host: (sjhl-o-compute07, sjhl-o-compute07) ram:61328 disk:394240 io_ops:0 instances:3, weight: 0.651759904778], WeighedHost [host: (dzc-o-compute01, dzc-o-compute01) ram:31626 disk:229376 io_ops:0 instances:0, weight: 0.336103553817]] _schedule /usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:286
2016-12-30 15:47:27.848 29308 DEBUG nova.openstack.common.periodic_task [-] Running periodic task SchedulerManager._expire_reservations run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 15:47:27.856 29308 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3f80950>> sleeping for 40.83 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132


########################################################################################################################
2016-12-30 16:03:20.039 18293 DEBUG nova.virt.disk.api [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Unable to mount image /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk with error Error mounting /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk with libguestfs (mount_options: /dev/sda on / (options: ''): mount: /dev/sda is write-protected, mounting read-only
mount: unknown filesystem type '(null)'). Cannot resize. is_image_partitionless /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:218
2016-12-30 16:03:20.040 18293 DEBUG nova.virt.disk.api [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Checking if we can resize image /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk. size=21474836480 can_resize_image /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:192
2016-12-30 16:03:20.040 18293 DEBUG nova.openstack.common.processutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 16:03:20.105 18293 DEBUG nova.openstack.common.processutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 16:03:20.106 18293 DEBUG nova.openstack.common.processutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Running cmd (subprocess): qemu-img resize /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk 21474836480 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2016-12-30 16:03:20.263 18293 DEBUG nova.openstack.common.processutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2016-12-30 16:03:20.264 18293 DEBUG nova.virt.disk.api [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Checking if we can resize filesystem inside /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk. CoW=True is_image_partitionless /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:206
2016-12-30 16:03:20.264 18293 DEBUG nova.virt.disk.vfs.api [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Instance for image imgfile=/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk imgfmt=qcow2 partition=None instance_for_image /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/api.py:45
2016-12-30 16:03:20.264 18293 DEBUG nova.virt.disk.vfs.api [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Using primary VFSGuestFS instance_for_image /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/api.py:49
2016-12-30 16:03:20.264 18293 DEBUG nova.virt.disk.vfs.guestfs [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Setting up appliance for /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk qcow2 setup /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:137
2016-12-30 16:03:22.650 18293 DEBUG nova.virt.disk.vfs.guestfs [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Mount guest OS image /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk partition None setup_os_static /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:83
2016-12-30 16:03:22.717 18293 DEBUG nova.virt.disk.vfs.guestfs [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Tearing down appliance teardown /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:185
2016-12-30 16:03:22.720 18293 WARNING nova.virt.disk.vfs.guestfs [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Failed to close augeas aug_close: do_aug_close: you must call 'aug-init' first to initialize Augeas
2016-12-30 16:03:22.750 18293 DEBUG nova.virt.disk.api [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Unable to mount image /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk with error Error mounting /var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk with libguestfs (mount_options: /dev/sda on / (options: ''): mount: /dev/sda is write-protected, mounting read-only
mount: unknown filesystem type '(null)'). Cannot resize. is_image_partitionless /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:218
2016-12-30 16:03:22.751 18293 DEBUG nova.block_device [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] block_device_list [] volume_in_mapping /usr/lib/python2.7/site-packages/nova/block_device.py:555
2016-12-30 16:03:22.752 18293 INFO nova.virt.libvirt.driver [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9] Creating image
2016-12-30 16:03:22.752 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Created new semaphore "/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk.info" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 16:03:22.753 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Acquired semaphore "/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 16:03:22.753 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Got semaphore / lock "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 16:03:22.753 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Releasing semaphore "/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 16:03:22.754 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Semaphore / lock released "write_to_disk_info_file" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 16:03:22.755 18293 DEBUG glanceclient.common.http [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] curl -i -X HEAD -H 'X-Service-Catalog: [{"endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "id": "37eb9ae73b72467b8695247873083335", "internalURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02", "publicURL": "http://controller.light.fang.com:8776/v2/516953ba1481400fb69642e9ffe14b02"}], "endpoints_links": [], "type": "volumev2", "name": "cinderv2"}, {"endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "region": "regionOne", "id": "30d052b7f62344dabab8c29928d29f99", "internalURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02", "publicURL": "http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02"}], "endpoints_links": [], "type": "volume", "name": "cinder"}]' -H 'X-Auth-Token: {SHA1}31511781e2f0fbb9aba7bbb5fe78128d10202a32' -H 'Accept-Encoding: gzip, deflate' -H 'Connection: keep-alive' -H 'Accept: */*' -H 'X-Roles: _member_,admin' -H 'User-Agent: python-glanceclient' -H 'X-Tenant-Id: 516953ba1481400fb69642e9ffe14b02' -H 'X-User-Id: 2a1a6dee31a0461c95a5ba8e7e1f30e1' -H 'X-Identity-Status: Confirmed' -H 'Content-Type: application/octet-stream' http://controller.light.fang.com:9292/v1/images/db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5 log_curl_request /usr/lib/python2.7/site-packages/glanceclient/common/http.py:122
2016-12-30 16:03:23.024 18293 DEBUG glanceclient.common.http [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] 
HTTP/1.1 200 OK
content-length: 0
x-image-meta-id: db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5
date: Fri, 30 Dec 2016 08:03:23 GMT
x-image-meta-deleted: False
x-image-meta-container_format: bare
connection: keep-alive
x-image-meta-checksum: 133eae9fb1c98f45894a4e60d8736619
x-image-meta-protected: False
x-image-meta-min_disk: 0
x-image-meta-created_at: 2015-12-18T08:55:31
x-image-meta-size: 13200896
x-image-meta-status: active
etag: 133eae9fb1c98f45894a4e60d8736619
x-image-meta-is_public: True
x-image-meta-min_ram: 0
x-image-meta-owner: e599088c985f42e7948b12f601705cd3
x-image-meta-updated_at: 2015-12-18T08:55:33
content-type: text/html; charset=UTF-8
x-openstack-request-id: req-650c158f-235b-439c-abbd-9883ba21aefa
x-image-meta-disk_format: qcow2
x-image-meta-name: cirros-0.3.3-x86_64
 log_http_response /usr/lib/python2.7/site-packages/glanceclient/common/http.py:135
2016-12-30 16:03:23.027 18293 DEBUG nova.virt.libvirt.driver [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9] Start _get_guest_xml network_info=[VIF({'profile': {}, 'ovs_interfaceid': u'69c4daab-2a1d-443a-9c0a-13f8c8f5fbdf', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:a1:d7:6b', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.110', 'type': 'fixed'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tap69c4daab-2a', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:a1:d7:6b', 'active': False, 'type': u'ovs', 'id': u'69c4daab-2a1d-443a-9c0a-13f8c8f5fbdf', 'qbg_params': None})] disk_info={'disk_bus': 'virtio', 'cdrom_bus': 'ide', 'mapping': {'disk': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': 'vda'}, 'root': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': 'vda'}}} image_meta={u'min_disk': 20, u'container_format': u'bare', u'min_ram': 0, u'disk_format': u'qcow2', 'properties': {u'old_vm_state': u'active', 'old_instance_type_flavorid': u'1', 'old_instance_type_vcpus': u'1', u'instance_type_name': u'm1.small', u'new_instance_type_ephemeral_gb': u'0', u'base_image_ref': u'db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5', u'new_instance_type_root_gb': u'20', u'instance_type_ephemeral_gb': u'0', u'new_instance_type_rxtx_factor': u'1.0', u'new_instance_type_swap': u'0', 'old_instance_type_swap': u'0', 'old_instance_type_root_gb': u'1', 'old_instance_type_ephemeral_gb': u'0', 'old_instance_type_rxtx_factor': u'1.0', u'instance_type_root_gb': u'20', u'new_instance_type_name': u'm1.small', 'old_instance_type_id': u'2', u'new_instance_type_flavorid': u'2', u'instance_type_rxtx_factor': u'1.0', u'new_instance_type_memory_mb': u'2048', u'new_instance_type_id': u'5', u'instance_type_vcpus': u'1', u'instance_type_memory_mb': u'2048', u'instance_type_swap': u'0', u'new_instance_type_vcpus': u'1', 'old_instance_type_memory_mb': u'512', u'instance_type_id': u'5', u'instance_type_flavorid': u'2', 'old_instance_type_name': u'm1.tiny'}} rescue=None block_device_info={'swap': None, 'ephemerals': [], 'block_device_mapping': []} _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4156
2016-12-30 16:03:23.054 18293 DEBUG nova.objects.instance [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Lazy-loading `numa_topology' on Instance uuid d89a5983-7eaa-423e-9ca8-2214c6a540c9 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 16:03:23.069 18293 DEBUG nova.virt.libvirt.driver [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] CPU mode 'host-model' model '' was chosen _get_guest_cpu_model_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:3362
2016-12-30 16:03:23.070 18293 DEBUG nova.virt.hardware [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Getting desirable topologies for flavor Flavor(created_at=None,deleted=False,deleted_at=None,disabled=False,ephemeral_gb=0,extra_specs={},flavorid='2',id=5,is_public=True,memory_mb=2048,name='m1.small',projects=<?>,root_gb=20,rxtx_factor=1.0,swap=0,updated_at=None,vcpu_weight=0,vcpus=1) and image_meta {u'min_disk': 20, u'container_format': u'bare', u'min_ram': 0, u'disk_format': u'qcow2', 'properties': {u'old_vm_state': u'active', 'old_instance_type_flavorid': u'1', 'old_instance_type_vcpus': u'1', u'instance_type_name': u'm1.small', u'new_instance_type_ephemeral_gb': u'0', u'base_image_ref': u'db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5', u'new_instance_type_root_gb': u'20', u'instance_type_ephemeral_gb': u'0', u'new_instance_type_rxtx_factor': u'1.0', u'new_instance_type_swap': u'0', 'old_instance_type_swap': u'0', 'old_instance_type_root_gb': u'1', 'old_instance_type_ephemeral_gb': u'0', 'old_instance_type_rxtx_factor': u'1.0', u'instance_type_root_gb': u'20', u'new_instance_type_name': u'm1.small', 'old_instance_type_id': u'2', u'new_instance_type_flavorid': u'2', u'instance_type_rxtx_factor': u'1.0', u'new_instance_type_memory_mb': u'2048', u'new_instance_type_id': u'5', u'instance_type_vcpus': u'1', u'instance_type_memory_mb': u'2048', u'instance_type_swap': u'0', u'new_instance_type_vcpus': u'1', 'old_instance_type_memory_mb': u'512', u'instance_type_id': u'5', u'instance_type_flavorid': u'2', 'old_instance_type_name': u'm1.tiny'}} get_desirable_configs /usr/lib/python2.7/site-packages/nova/virt/hardware.py:502
2016-12-30 16:03:23.071 18293 DEBUG nova.virt.hardware [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Flavor limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:296
2016-12-30 16:03:23.071 18293 DEBUG nova.virt.hardware [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Image limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:309
2016-12-30 16:03:23.071 18293 DEBUG nova.virt.hardware [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Flavor pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:332
2016-12-30 16:03:23.072 18293 DEBUG nova.virt.hardware [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Image pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:354
2016-12-30 16:03:23.072 18293 DEBUG nova.virt.hardware [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Chosen -1:-1:-1 limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:383
2016-12-30 16:03:23.073 18293 DEBUG nova.virt.hardware [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Build topologies for 1 vcpu(s) 1:1:1 get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:419
2016-12-30 16:03:23.073 18293 DEBUG nova.virt.hardware [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Got 1 possible topologies get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:442
2016-12-30 16:03:23.076 18293 DEBUG nova.virt.libvirt.vif [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-30T07:46:28Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='ki',display_name='ki-d89a5983-7eaa-423e-9ca8-2214c6a540c9',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute08',hostname='ki-d89a5983-7eaa-423e-9ca8-2214c6a540c9',id=218,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=5,kernel_id='',key_data=None,key_name=None,launch_index=1,launched_at=2016-12-30T07:47:30Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=2048,metadata={},node='sjhl-o-compute08',numa_topology=None,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-vs0y0svw',root_device_name='/dev/vda',root_gb=20,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='20',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='2',instance_type_id='5',instance_type_memory_mb='2048',instance_type_name='m1.small',instance_type_root_gb='20',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1',new_instance_type_ephemeral_gb='0',new_instance_type_flavorid='2',new_instance_type_id='5',new_instance_type_memory_mb='2048',new_instance_type_name='m1.small',new_instance_type_root_gb='20',new_instance_type_rxtx_factor='1.0',new_instance_type_swap='0',new_instance_type_vcpu_weight=None,new_instance_type_vcpus='1',old_instance_type_ephemeral_gb='0',old_instance_type_flavorid='1',old_instance_type_id='2',old_instance_type_memory_mb='512',old_instance_type_name='m1.tiny',old_instance_type_root_gb='1',old_instance_type_rxtx_factor='1.0',old_instance_type_swap='0',old_instance_type_vcpu_weight=None,old_instance_type_vcpus='1',old_vm_state='active'},task_state='resize_finish',terminated_at=None,updated_at=2016-12-30T08:03:17Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=d89a5983-7eaa-423e-9ca8-2214c6a540c9,vcpus=1,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'69c4daab-2a1d-443a-9c0a-13f8c8f5fbdf', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:a1:d7:6b', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.110', 'type': 'fixed'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tap69c4daab-2a', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:a1:d7:6b', 'active': False, 'type': u'ovs', 'id': u'69c4daab-2a1d-443a-9c0a-13f8c8f5fbdf', 'qbg_params': None}) virt_typekvm get_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:342
2016-12-30 16:03:23.079 18293 DEBUG nova.objects.instance [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Lazy-loading `pci_devices' on Instance uuid d89a5983-7eaa-423e-9ca8-2214c6a540c9 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2016-12-30 16:03:23.138 18293 DEBUG nova.virt.libvirt.config [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Generated XML ('<domain type="kvm">\n  <uuid>d89a5983-7eaa-423e-9ca8-2214c6a540c9</uuid>\n  <name>instance-000000da</name>\n  <memory>2097152</memory>\n  <vcpu cpuset="6-11,18-23">1</vcpu>\n  <metadata>\n    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">\n      <nova:package version="2014.2.2-1.el7"/>\n      <nova:name>ki-d89a5983-7eaa-423e-9ca8-2214c6a540c9</nova:name>\n      <nova:creationTime>2016-12-30 08:03:23</nova:creationTime>\n      <nova:flavor name="m1.small">\n        <nova:memory>2048</nova:memory>\n        <nova:disk>20</nova:disk>\n        <nova:swap>0</nova:swap>\n        <nova:ephemeral>0</nova:ephemeral>\n        <nova:vcpus>1</nova:vcpus>\n      </nova:flavor>\n      <nova:owner>\n        <nova:user uuid="2a1a6dee31a0461c95a5ba8e7e1f30e1">fang</nova:user>\n        <nova:project uuid="516953ba1481400fb69642e9ffe14b02">fang</nova:project>\n      </nova:owner>\n      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>\n    </nova:instance>\n  </metadata>\n  <sysinfo type="smbios">\n    <system>\n      <entry name="manufacturer">Fedora Project</entry>\n      <entry name="product">OpenStack Nova</entry>\n      <entry name="version">2014.2.2-1.el7</entry>\n      <entry name="serial">9e3cdbcf-990a-4540-b9a9-7a000901d4a8</entry>\n      <entry name="uuid">d89a5983-7eaa-423e-9ca8-2214c6a540c9</entry>\n    </system>\n  </sysinfo>\n  <os>\n    <type>hvm</type>\n    <boot dev="hd"/>\n    <smbios mode="sysinfo"/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n  </features>\n  <clock offset="utc">\n    <timer name="pit" tickpolicy="delay"/>\n    <timer name="rtc" tickpolicy="catchup"/>\n    <timer name="hpet" present="no"/>\n  </clock>\n  <cpu mode="host-model" match="exact">\n    <topology sockets="1" cores="1" threads="1"/>\n  </cpu>\n  <devices>\n    <disk type="file" device="disk">\n      <driver name="qemu" type="qcow2" cache="none"/>\n      <source file="/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk"/>\n      <target bus="virtio" dev="vda"/>\n    </disk>\n    <interface type="bridge">\n      <mac address="fa:16:3e:a1:d7:6b"/>\n      <model type="virtio"/>\n      <source bridge="qbr69c4daab-2a"/>\n      <target dev="tap69c4daab-2a"/>\n    </interface>\n    <serial type="file">\n      <source path="/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/console.log"/>\n    </serial>\n    <serial type="pty"/>\n    <input type="tablet" bus="usb"/>\n    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>\n    <video>\n      <model type="cirrus"/>\n    </video>\n    <memballoon model="virtio">\n      <stats period="10"/>\n    </memballoon>\n  </devices>\n</domain>\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82
2016-12-30 16:03:23.138 18293 DEBUG nova.virt.libvirt.driver [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9] End _get_guest_xml xml=<domain type="kvm">
  <uuid>d89a5983-7eaa-423e-9ca8-2214c6a540c9</uuid>
  <name>instance-000000da</name>
  <memory>2097152</memory>
  <vcpu cpuset="6-11,18-23">1</vcpu>
  <metadata>
    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">
      <nova:package version="2014.2.2-1.el7"/>
      <nova:name>ki-d89a5983-7eaa-423e-9ca8-2214c6a540c9</nova:name>
      <nova:creationTime>2016-12-30 08:03:23</nova:creationTime>
      <nova:flavor name="m1.small">
        <nova:memory>2048</nova:memory>
        <nova:disk>20</nova:disk>
        <nova:swap>0</nova:swap>
        <nova:ephemeral>0</nova:ephemeral>
        <nova:vcpus>1</nova:vcpus>
      </nova:flavor>
      <nova:owner>
        <nova:user uuid="2a1a6dee31a0461c95a5ba8e7e1f30e1">fang</nova:user>
        <nova:project uuid="516953ba1481400fb69642e9ffe14b02">fang</nova:project>
      </nova:owner>
      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>
    </nova:instance>
  </metadata>
  <sysinfo type="smbios">
    <system>
      <entry name="manufacturer">Fedora Project</entry>
      <entry name="product">OpenStack Nova</entry>
      <entry name="version">2014.2.2-1.el7</entry>
      <entry name="serial">9e3cdbcf-990a-4540-b9a9-7a000901d4a8</entry>
      <entry name="uuid">d89a5983-7eaa-423e-9ca8-2214c6a540c9</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev="hd"/>
    <smbios mode="sysinfo"/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset="utc">
    <timer name="pit" tickpolicy="delay"/>
    <timer name="rtc" tickpolicy="catchup"/>
    <timer name="hpet" present="no"/>
  </clock>
  <cpu mode="host-model" match="exact">
    <topology sockets="1" cores="1" threads="1"/>
  </cpu>
  <devices>
    <disk type="file" device="disk">
      <driver name="qemu" type="qcow2" cache="none"/>
      <source file="/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk"/>
      <target bus="virtio" dev="vda"/>
    </disk>
    <interface type="bridge">
      <mac address="fa:16:3e:a1:d7:6b"/>
      <model type="virtio"/>
      <source bridge="qbr69c4daab-2a"/>
      <target dev="tap69c4daab-2a"/>
    </interface>
    <serial type="file">
      <source path="/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/console.log"/>
    </serial>
    <serial type="pty"/>
    <input type="tablet" bus="usb"/>
    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>
    <video>
      <model type="cirrus"/>
    </video>
    <memballoon model="virtio">
      <stats period="10"/>
    </memballoon>
  </devices>
</domain>
 _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4168
2016-12-30 16:03:23.139 18293 DEBUG nova.virt.libvirt.vif [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=True,config_drive='',created_at=2016-12-30T07:46:28Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='ki',display_name='ki-d89a5983-7eaa-423e-9ca8-2214c6a540c9',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute08',hostname='ki-d89a5983-7eaa-423e-9ca8-2214c6a540c9',id=218,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=5,kernel_id='',key_data=None,key_name=None,launch_index=1,launched_at=2016-12-30T07:47:30Z,launched_on='sjhl-o-compute03',locked=False,locked_by=None,memory_mb=2048,metadata={},node='sjhl-o-compute08',numa_topology=None,os_type=None,pci_devices=PciDeviceList,power_state=1,progress=0,project_id='516953ba1481400fb69642e9ffe14b02',ramdisk_id='',reservation_id='r-vs0y0svw',root_device_name='/dev/vda',root_gb=20,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='20',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='2',instance_type_id='5',instance_type_memory_mb='2048',instance_type_name='m1.small',instance_type_root_gb='20',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1',new_instance_type_ephemeral_gb='0',new_instance_type_flavorid='2',new_instance_type_id='5',new_instance_type_memory_mb='2048',new_instance_type_name='m1.small',new_instance_type_root_gb='20',new_instance_type_rxtx_factor='1.0',new_instance_type_swap='0',new_instance_type_vcpu_weight=None,new_instance_type_vcpus='1',old_instance_type_ephemeral_gb='0',old_instance_type_flavorid='1',old_instance_type_id='2',old_instance_type_memory_mb='512',old_instance_type_name='m1.tiny',old_instance_type_root_gb='1',old_instance_type_rxtx_factor='1.0',old_instance_type_swap='0',old_instance_type_vcpu_weight=None,old_instance_type_vcpus='1',old_vm_state='active'},task_state='resize_finish',terminated_at=None,updated_at=2016-12-30T08:03:17Z,user_data=None,user_id='2a1a6dee31a0461c95a5ba8e7e1f30e1',uuid=d89a5983-7eaa-423e-9ca8-2214c6a540c9,vcpus=1,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'69c4daab-2a1d-443a-9c0a-13f8c8f5fbdf', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:a1:d7:6b', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.110', 'type': 'fixed'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tap69c4daab-2a', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:a1:d7:6b', 'active': False, 'type': u'ovs', 'id': u'69c4daab-2a1d-443a-9c0a-13f8c8f5fbdf', 'qbg_params': None}) plug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:531
2016-12-30 16:03:31.113 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 16:03:31.114 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._check_instance_build_time run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 16:03:31.114 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 10.79 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 16:03:41.907 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_volume_usage run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 16:03:41.907 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 3.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 16:03:44.908 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rescued_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 16:03:44.909 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 1.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 16:03:45.906 18293 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_unconfirmed_resizes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2016-12-30 16:03:45.906 18293 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2b08ed0>> sleeping for 6.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2016-12-30 16:03:48.447 18293 ERROR nova.virt.libvirt.driver [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] Error launching a defined domain with XML: <domain type='kvm'>
  <name>instance-000000da</name>
  <uuid>d89a5983-7eaa-423e-9ca8-2214c6a540c9</uuid>
  <metadata>
    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">
      <nova:package version="2014.2.2-1.el7"/>
      <nova:name>ki-d89a5983-7eaa-423e-9ca8-2214c6a540c9</nova:name>
      <nova:creationTime>2016-12-30 08:03:23</nova:creationTime>
      <nova:flavor name="m1.small">
        <nova:memory>2048</nova:memory>
        <nova:disk>20</nova:disk>
        <nova:swap>0</nova:swap>
        <nova:ephemeral>0</nova:ephemeral>
        <nova:vcpus>1</nova:vcpus>
      </nova:flavor>
      <nova:owner>
        <nova:user uuid="2a1a6dee31a0461c95a5ba8e7e1f30e1">fang</nova:user>
        <nova:project uuid="516953ba1481400fb69642e9ffe14b02">fang</nova:project>
      </nova:owner>
      <nova:root type="image" uuid="db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5"/>
    </nova:instance>
  </metadata>
  <memory unit='KiB'>2097152</memory>
  <currentMemory unit='KiB'>2097152</currentMemory>
  <vcpu placement='static' cpuset='6-11,18-23'>1</vcpu>
  <sysinfo type='smbios'>
    <system>
      <entry name='manufacturer'>Fedora Project</entry>
      <entry name='product'>OpenStack Nova</entry>
      <entry name='version'>2014.2.2-1.el7</entry>
      <entry name='serial'>9e3cdbcf-990a-4540-b9a9-7a000901d4a8</entry>
      <entry name='uuid'>d89a5983-7eaa-423e-9ca8-2214c6a540c9</entry>
    </system>
  </sysinfo>
  <os>
    <type arch='x86_64' machine='pc-i440fx-rhel7.0.0'>hvm</type>
    <boot dev='hd'/>
    <smbios mode='sysinfo'/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <cpu mode='host-model'>
    <model fallback='allow'/>
    <topology sockets='1' cores='1' threads='1'/>
  </cpu>
  <clock offset='utc'>
    <timer name='pit' tickpolicy='delay'/>
    <timer name='rtc' tickpolicy='catchup'/>
    <timer name='hpet' present='no'/>
  </clock>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/disk'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </disk>
    <controller type='usb' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>
    </controller>
    <controller type='pci' index='0' model='pci-root'/>
    <interface type='bridge'>
      <mac address='fa:16:3e:a1:d7:6b'/>
      <source bridge='qbr69c4daab-2a'/>
      <target dev='tap69c4daab-2a'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='file'>
      <source path='/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/console.log'/>
      <target port='0'/>
    </serial>
    <serial type='pty'>
      <target port='1'/>
    </serial>
    <console type='file'>
      <source path='/var/lib/nova/instances/d89a5983-7eaa-423e-9ca8-2214c6a540c9/console.log'/>
      <target type='serial' port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0' keymap='en-us'>
      <listen type='address' address='0.0.0.0'/>
    </graphics>
    <video>
      <model type='cirrus' vram='16384' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <stats period='10'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
</domain>

2016-12-30 16:03:48.578 18293 ERROR nova.compute.manager [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae None] [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9] Setting instance vm_state to ERROR
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9] Traceback (most recent call last):
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3904, in finish_resize
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     disk_info, image)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3872, in _finish_resize
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     old_instance_type, sys_meta)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     six.reraise(self.type_, self.value, self.tb)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3867, in _finish_resize
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     block_device_info, power_on)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 6051, in finish_migration
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     vifs_already_plugged=True)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4425, in _create_domain_and_network
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     power_on=power_on)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4349, in _create_domain
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     LOG.error(err)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     six.reraise(self.type_, self.value, self.tb)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4339, in _create_domain
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     domain.createWithFlags(launch_flags)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 183, in doit
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     result = proxy_call(self._autowrap, f, *args, **kwargs)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 141, in proxy_call
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     rv = execute(f, *args, **kwargs)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 122, in execute
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     six.reraise(c, e, tb)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 80, in tworker
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     rv = meth(*args, **kwargs)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]   File "/usr/lib64/python2.7/site-packages/libvirt.py", line 1059, in createWithFlags
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9]     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9] libvirtError: Activation of org.freedesktop.machine1 timed out
2016-12-30 16:03:48.578 18293 TRACE nova.compute.manager [instance: d89a5983-7eaa-423e-9ca8-2214c6a540c9] 
2016-12-30 16:03:48.688 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 16:03:48.689 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 16:03:48.689 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Got semaphore / lock "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 16:03:48.689 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 16:03:48.689 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Semaphore / lock released "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 16:03:48.847 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2016-12-30 16:03:48.848 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2016-12-30 16:03:48.848 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Got semaphore / lock "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2016-12-30 16:03:48.848 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Releasing semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2016-12-30 16:03:48.848 18293 DEBUG nova.openstack.common.lockutils [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Semaphore / lock released "update_usage" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2016-12-30 16:03:48.850 18293 ERROR oslo.messaging.rpc.dispatcher [req-9d73cebd-0765-4053-a281-7bb0d0aee8ae ] Exception during message handling: Activation of org.freedesktop.machine1 timed out
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py", line 134, in _dispatch_and_reply
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py", line 177, in _dispatch
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py", line 123, in _do_dispatch
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/exception.py", line 88, in wrapped
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     payload)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/exception.py", line 71, in wrapped
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 298, in decorated_function
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     pass
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 284, in decorated_function
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 348, in decorated_function
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 272, in decorated_function
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     migration.instance_uuid, exc_info=True)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 259, in decorated_function
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 326, in decorated_function
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 314, in decorated_function
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3916, in finish_resize
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     self._set_instance_error_state(context, instance)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3904, in finish_resize
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     disk_info, image)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3872, in _finish_resize
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     old_instance_type, sys_meta)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3867, in _finish_resize
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     block_device_info, power_on)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 6051, in finish_migration
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     vifs_already_plugged=True)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4425, in _create_domain_and_network
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     power_on=power_on)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4349, in _create_domain
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     LOG.error(err)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4339, in _create_domain
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     domain.createWithFlags(launch_flags)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 183, in doit
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     result = proxy_call(self._autowrap, f, *args, **kwargs)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 141, in proxy_call
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     rv = execute(f, *args, **kwargs)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 122, in execute
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     six.reraise(c, e, tb)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 80, in tworker
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     rv = meth(*args, **kwargs)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib64/python2.7/site-packages/libvirt.py", line 1059, in createWithFlags
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher libvirtError: Activation of org.freedesktop.machine1 timed out
2016-12-30 16:03:48.850 18293 TRACE oslo.messaging.rpc.dispatcher /lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206

########################################################################################################################
热迁移配置  
error: unable to connect to server at 'host:16509': Connection refused
sed -i 's/#listen_tls = 0/listen_tls = 0/g' /etc/libvirt/libvirtd.conf
sed -i 's/#listen_tcp = 1/listen_tcp = 1/g' /etc/libvirt/libvirtd.conf
sed -i 's/#auth_tcp = "sasl"/auth_tcp = "none"/g' /etc/libvirt/libvirtd.conf
sed -i 's/#LIBVIRTD_ARGS="--listen"/LIBVIRTD_ARGS="--listen"/g' /etc/sysconfig/libvirtd

service libvirtd restart
Failed to connect to remote libvirt URI qemu+tcp://sjhl-o-compute07/system: unable to connect to server at 'sjhl-o-compute07:16509': Connection refuse

virsh -c qemu+tcp://sjhl-o-compute07/system
2016-12-30 17:02:14.430 28032 ERROR nova.virt.libvirt.driver [-] [instance: d104c0a5-6c4b-4060-948a-c2e6ac2af623] Live Migration failure: internal error: unable to execute QEMU command 'migrate': this feature or command is not currently supported
########################################################################################################################
如上图所示，我们把对 Instance 的管理按运维工作的场景分为两类：常规操作和故障处理。

常规操作

常规操作中，Launch、Start、Reboot、Shut Off 和 Terminate 都很好理解。 下面几个操作重点回顾一下：

Resize
通过应用不同的 flavor 调整分配给 instance 的资源。

Lock/Unlock
可以防止对 instance 的误操作。

Pause/Suspend/Resume
暂停当前 instance，并在以后恢复。 Pause 和 Suspend 的区别在于 Pause 将 instance 的运行状态保存在计算节点的内存中，而 Suspend 保存在磁盘上。 Pause 的优点是 Resume 的速度比 Suspend 快；缺点是如果计算节点重启，内存数据丢失，就无法 Resume 了，而 Suspend 则没有这个问题。

Snapshot
备份 instance 到 Glance。产生的 image 可用于故障恢复，或者以此为模板部署新的 instance。

故障处理

故障处理有两种场景：计划内和计划外。

计划内是指提前安排时间窗口做的维护工作，比如服务器定期的微码升级，添加更换硬件等。 计划外是指发生了没有预料到的突发故障，比如强行关机造成 OS 系统文件损坏，服务器掉电，硬件故障等。

计划内故障处理

对于计划内的故障处理，可以在维护窗口中将 instance 迁移到其他计算节点。 涉及如下操作：

Migrate
将 instance 迁移到其他计算节点。 迁移之前，instance 会被 Shut Off，支持共享存储和非共享存储。

Live Migrate
与 Migrate 不同，Live Migrate 能不停机在线地迁移 instance，保证了业务的连续性。也支持共享存储和非共享存储（Block Migration）

Shelve/Unshelve Shelve 将 instance 保存到 Glance 上，之后可通过 Unshelve 重新部署。 Shelve 操作成功后，instance 会从原来的计算节点上删除。 Unshelve 会重新选择节点部署，可能不是原节点。

计划外故障处理

计划外的故障按照影响的范围又分为两类：Instance 故障和计算节点故障

Instance 故障

Instance 故障只限于某一个 instance 的操作系统层面，系统无法正常启动。 可以使用如下操作修复 instance：

Rescue/Unrescue
用指定的启动盘启动，进入 Rescue 模式，修复受损的系统盘。成功修复后，通过 Unrescue 正常启动 instance。

Rebuild
如果 Rescue 无法修复，则只能通过 Rebuild 从已有的备份恢复。Instance 的备份是通过 snapshot 创建的，所以需要有备份策略定期备份。

计算节点故障

Instance 故障的影响范围局限在特定的 instance，计算节点本身是正常工作的。如果计算节点发生故障，OpenStack 则无法与节点的 nova-compute 通信，其上运行的所有 instance 都会受到影响。这个时候，只能通过 Evacuate 操作在其他正常节点上重建 Instance。

Evacuate
利用共享存储上 Instance 的镜像文件在其他计算节点上重建 Instance。 所以提前规划共享存储是关键。

########################################################################################################################  
openstack cinder架构
Cinder 为提供 Instance 提供块存储（虚拟磁盘）
block  storage service 
获得文件系统存储空间的方式一般有两种：
1.通过某种协议（SAS,SCSI,SAN,iSCSI 等）挂接裸硬盘，然后分区、格式化、创建文件系统；或者直接使用裸硬盘存储数据（数据库）
2.通过 NFS、CIFS 等 协议，mount 远程的文件系统
第一种方式叫做块存储，每个裸盘通常称为volume卷
第二种叫做文件存储，
从虚拟机角度看，挂载的每个卷都是一块硬盘
OpenStack 提供 Block Storage Service 的是 Cinder，其具体功能是：

1.提供 REST API 使用户能够查询和管理 volume、volume snapshot 以及 volume type
    cinder-api 调用cinder-volume执行操作
2.提供 scheduler 调度 volume 创建请求，合理优化存储资源的分配
3.通过 driver 架构支持多种 back-end（后端）存储方式，包括 LVM，NFS，Ceph 和其他诸如 EMC、IBM 等商业存储产品和方案
     cinder-volume，管理volume的服务，与volume provider协调工作，管理volume的生命life-syscle运行 cinder-volume 服务的节点被称作为存储节点。
	 cinder-scheduler ：scheduler 通过调度算法选择最合适的存储节点创建 volume。
	 volume provider：数据的存储设备，为 volume 提供物理存储空间。 cinder-volume 支持多种 volume provider，每种 volume provider 通过自己的 driver 与cinder-volume 协调工作。

1. 客户（可以是 OpenStack 最终用户，也可以是其他程序）向 API（cinder-api）发送请求：“帮我创建一个 volume”
2. API 对请求做一些必要处理后，向 Messaging（RabbitMQ）发送了一条消息：“让 Scheduler 创建一个 volume”
3. Scheduler（cinder-scheduler）从 Messaging 获取到 API 发给它的消息，然后执行调度算法，从若干计存储点中选出节点 A
4. Scheduler 向 Messaging 发送了一条消息：“让存储节点 A 创建这个 volume”
5. 存储节点 A 的 Volume（cinder-volume）从 Messaging 中获取到 Scheduler 发给它的消息，然后通过 driver 在 volume provider 上创建 volume。


/etc/cinder/cinder.conf
#For LVM   lvm的driver
volume_driver=cinder.volume.drivers.lvm.LVMISCSIDriver
# FOR GLUSTERFS  glusterfs的driver
volume_driver=cinder.volume.drivers.glusterfs.GlusterfsDriver
# FOR CEPH ceph的后端driver
volume_driver=cinder.volume.drivers.rbd.RBDDriver


[root@dzc-o-control01v ~]# keystone endpoint-get --service volume
+------------------+---------------------------------------------------------------------------+
|     Property     |                                   Value                                   |
+------------------+---------------------------------------------------------------------------+
| volume.publicURL | http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3 |
+------------------+---------------------------------------------------------------------------+


scheduler_driver=cinder.scheduler.filter_scheduler.FilterScheduler
与 Nova 一样，Cinder 也允许使用第三方 scheduler，配置 scheduler_driver 即可。

scheduler 调度过程如下：
通过过滤器（filter）选择满足条件的存储节点（运行 cinder-volume）
通过权重计算（weighting）选择最优（权重值最大）的存储节点。
可见，cinder-scheduler 的运行机制与 nova-scheduler 完全一样。

scheduler_default_filters=AvailabilityZoneFilter,CapacityFilter,CapabilitiesFilter





/var/log/cinder/api.log
2017-01-03 10:51:15.116 31409 DEBUG cinder.api.v1.volumes [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Create volume request body: {u'volume': {u'status': u'creating', u'user_id': None, u'imageRef': None, u'availability_zone': None, 'scheduler_hints': {}, u'attach_status': u'detached', u'display_description': u'', u'metadata': {}, u'source_volid': None, u'snapshot_id': None, u'display_name': u'test01', u'project_id': None, u'volume_type': u'', u'size': 10}} create /usr/lib/python2.7/site-packages/cinder/api/v1/volumes.py:319
2017-01-03 10:51:15.117 31409 INFO cinder.api.v1.volumes [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Create volume of 10 GB
2017-01-03 10:51:15.125 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Availability zone cache updated, next update will occur around 2017-01-03 03:51:15.125414 list_availability_zones /usr/lib/python2.7/site-packages/cinder/volume/api.py:148
2017-01-03 10:51:15.187 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Translated 'taskflow.patterns.linear_flow.Flow: volume_create_api; 5' into a graph: _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:209
2017-01-03 10:51:15.188 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Name: volume_create_api _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.188 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Type: DiGraph _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.189 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Frozen: False _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.189 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Nodes: 5 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.189 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    - cinder.volume.flows.api.create_volume.ExtractVolumeRequestTask;volume:create==1.0 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.190 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    - cinder.volume.flows.api.create_volume.EntryCreateTask;volume:create==1.0 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.190 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    - cinder.volume.flows.api.create_volume.QuotaReserveTask;volume:create==1.0 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.190 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    - cinder.volume.flows.api.create_volume.VolumeCastTask;volume:create==1.0 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.191 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    - cinder.volume.flows.api.create_volume.QuotaCommitTask;volume:create==1.0 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.191 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Edges: 4 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.192 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    cinder.volume.flows.api.create_volume.EntryCreateTask;volume:create==1.0 -> cinder.volume.flows.api.create_volume.QuotaCommitTask;volume:create==1.0 ({'invariant': True}) _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.192 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    cinder.volume.flows.api.create_volume.ExtractVolumeRequestTask;volume:create==1.0 -> cinder.volume.flows.api.create_volume.QuotaReserveTask;volume:create==1.0 ({'invariant': True}) _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.192 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    cinder.volume.flows.api.create_volume.QuotaCommitTask;volume:create==1.0 -> cinder.volume.flows.api.create_volume.VolumeCastTask;volume:create==1.0 ({'invariant': True}) _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.193 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    cinder.volume.flows.api.create_volume.QuotaReserveTask;volume:create==1.0 -> cinder.volume.flows.api.create_volume.EntryCreateTask;volume:create==1.0 ({'invariant': True}) _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.193 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Density: 0.200 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.193 31409 DEBUG taskflow.engines.action_engine.compiler [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Cycles: 0 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:51:15.196 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Flow 'volume_create_api' (c8de963a-fbc4-4d4d-9b14-07e4981fadbb) transitioned into state 'RUNNING' from state 'PENDING' _flow_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:83
2017-01-03 10:51:15.197 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'RESUMING' in response to event 'start' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.198 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'RESUMING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.198 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.199 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.ExtractVolumeRequestTask;volume:create' (a85c0464-d58b-4bb9-badc-f99b439087c5) transitioned into state 'RUNNING' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-03 10:51:15.200 31409 DEBUG cinder.volume.flows.api.create_volume [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Validating volume 10 using validate_int _extract_size /usr/lib/python2.7/site-packages/cinder/volume/flows/api/create_volume.py:218
2017-01-03 10:51:15.200 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.201 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.201 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.202 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.202 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.ExtractVolumeRequestTask;volume:create' (a85c0464-d58b-4bb9-badc-f99b439087c5) transitioned into state 'SUCCESS' with result '{'volume_type_id': None, 'availability_zone': 'nova', 'source_replicaid': None, 'volume_type': {}, 'qos_specs': None, 'encryption_key_id': None, 'source_volid': None, 'snapshot_id': None, 'consistencygroup_id': None, 'size': 10}' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:113
2017-01-03 10:51:15.203 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.203 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.204 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.QuotaReserveTask;volume:create' (421d5f36-745c-4040-a0e2-6d7834f9f0bc) transitioned into state 'RUNNING' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-03 10:51:15.237 31409 DEBUG cinder.quota [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Created reservations ['881f6169-5423-4b71-ba1b-67253163fbac', '0a30b58b-2beb-4fe9-a99e-5fd247c60457'] reserve /usr/lib/python2.7/site-packages/cinder/quota.py:761
2017-01-03 10:51:15.238 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.238 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.238 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.239 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.240 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.QuotaReserveTask;volume:create' (421d5f36-745c-4040-a0e2-6d7834f9f0bc) transitioned into state 'SUCCESS' with result '{'reservations': ['881f6169-5423-4b71-ba1b-67253163fbac', '0a30b58b-2beb-4fe9-a99e-5fd247c60457']}' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:113
2017-01-03 10:51:15.240 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.241 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.241 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.EntryCreateTask;volume:create' (481befbb-14fa-4c0e-8daa-6a6a81f5f59b) transitioned into state 'RUNNING' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-03 10:51:15.287 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.287 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.287 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.288 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.290 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.EntryCreateTask;volume:create' (481befbb-14fa-4c0e-8daa-6a6a81f5f59b) transitioned into state 'SUCCESS' with result '{'volume': <cinder.db.sqlalchemy.models.Volume object at 0x4f990d0>, 'volume_properties': {'status': 'creating', 'volume_type_id': None, 'display_name': u'test01', 'volume_metadata': [], 'reservations': ['881f6169-5423-4b71-ba1b-67253163fbac', '0a30b58b-2beb-4fe9-a99e-5fd247c60457'], 'display_description': u'', 'availability_zone': 'nova', 'attach_status': 'detached', 'source_volid': None, 'metadata': {}, 'source_replicaid': None, 'encryption_key_id': None, 'consistencygroup_id': None, 'replication_status': 'disabled', 'snapshot_id': None, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'id': '46fe22d1-0352-4156-9cf6-08442ec93f64', 'size': 10}, 'volume_id': '46fe22d1-0352-4156-9cf6-08442ec93f64'}' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:113
2017-01-03 10:51:15.290 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.291 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.291 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.QuotaCommitTask;volume:create' (e00f0be5-8c83-4152-bf55-e1fb53275f52) transitioned into state 'RUNNING' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-03 10:51:15.305 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.306 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.306 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.307 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.308 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.QuotaCommitTask;volume:create' (e00f0be5-8c83-4152-bf55-e1fb53275f52) transitioned into state 'SUCCESS' with result '{'volume_properties': {'status': 'creating', 'volume_type_id': None, 'display_name': u'test01', 'volume_metadata': [], 'reservations': ['881f6169-5423-4b71-ba1b-67253163fbac', '0a30b58b-2beb-4fe9-a99e-5fd247c60457'], 'display_description': u'', 'availability_zone': 'nova', 'attach_status': 'detached', 'source_volid': None, 'metadata': {}, 'source_replicaid': None, 'encryption_key_id': None, 'consistencygroup_id': None, 'replication_status': 'disabled', 'snapshot_id': None, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'id': '46fe22d1-0352-4156-9cf6-08442ec93f64', 'size': 10}}' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:113
2017-01-03 10:51:15.309 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.309 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.310 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.VolumeCastTask;volume:create' (2ce16e35-0a78-42f7-833a-ab79237f60c9) transitioned into state 'RUNNING' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-03 10:51:15.313 31409 INFO oslo.messaging._drivers.impl_rabbit [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Connecting to AMQP server on controller.light.fang.com:5672
2017-01-03 10:51:15.336 31409 INFO oslo.messaging._drivers.impl_rabbit [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Connected to AMQP server on controller.light.fang.com:5672
2017-01-03 10:51:15.339 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.340 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.340 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.340 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.341 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.volume.flows.api.create_volume.VolumeCastTask;volume:create' (2ce16e35-0a78-42f7-833a-ab79237f60c9) transitioned into state 'SUCCESS' with result 'None' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:113
2017-01-03 10:51:15.342 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'ANALYZING' in response to event 'finished' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.342 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'GAME_OVER' in response to event 'finished' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.343 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'GAME_OVER' in response to event 'success' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:51:15.343 31409 DEBUG taskflow.engines.action_engine.runner [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'SUCCESS' in response to event 'success' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:51:15.344 31409 DEBUG cinder.volume.api [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Flow 'volume_create_api' (c8de963a-fbc4-4d4d-9b14-07e4981fadbb) transitioned into state 'SUCCESS' from state 'RUNNING' _flow_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:83
2017-01-03 10:51:15.345 31409 INFO cinder.api.v1.volumes [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': 'nova', 'terminated_at': None, 'reservations': ['881f6169-5423-4b71-ba1b-67253163fbac', '0a30b58b-2beb-4fe9-a99e-5fd247c60457'], 'updated_at': None, 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': 'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': '46fe22d1-0352-4156-9cf6-08442ec93f64', 'size': 10, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'source_replicaid': None, 'attached_host': None, 'display_description': u'', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': None, 'scheduled_at': None, 'status': 'creating', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': None, 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'test01', 'instance_uuid': None, 'bootable': False, 'created_at': datetime.datetime(2017, 1, 3, 2, 51, 15, 246384), 'attach_status': 'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {}}
2017-01-03 10:51:15.348 31409 INFO cinder.api.openstack.wsgi [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02/volumes returned with HTTP 200
2017-01-03 10:51:15.349 31409 INFO eventlet.wsgi.server [req-6e8178be-a43d-4f73-981f-96310dc471c3 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] 10.20.8.32 - - [03/Jan/2017 10:51:15] "POST /v1/516953ba1481400fb69642e9ffe14b02/volumes HTTP/1.1" 200 623 0.238995
2017-01-03 10:51:15.437 31400 INFO eventlet.wsgi.server [-] (31400) accepted ('10.20.8.32', 45932)
2017-01-03 10:51:15.443 31400 INFO cinder.api.openstack.wsgi [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] GET http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02/volumes/detail
2017-01-03 10:51:15.444 31400 DEBUG cinder.api.openstack.wsgi [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Empty body provided in request get_body /usr/lib/python2.7/site-packages/cinder/api/openstack/wsgi.py:789
2017-01-03 10:51:15.445 31400 DEBUG cinder.utils [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Removing options '' from query. remove_invalid_filter_options /usr/lib/python2.7/site-packages/cinder/utils.py:757
2017-01-03 10:51:15.446 31400 DEBUG cinder.volume.api [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Searching by: MultiDict([('no_migration_targets', True)]) get_all /usr/lib/python2.7/site-packages/cinder/volume/api.py:384
2017-01-03 10:51:15.494 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2017, 1, 3, 2, 51, 15), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'46fe22d1-0352-4156-9cf6-08442ec93f64', 'size': 10L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': [], 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': None, 'scheduled_at': datetime.datetime(2017, 1, 3, 2, 51, 15), 'status': u'creating', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'dzc-o-control01v#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'test01', 'instance_uuid': None, 'bootable': False, 'created_at': datetime.datetime(2017, 1, 3, 2, 51, 15), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None}
2017-01-03 10:51:15.498 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 26, 11, 18, 41), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'1e5fafbf-5178-4a6c-b03e-b7df85fe537a', 'ec2_id': None, 'mountpoint': u'/dev/vdc', 'deleted_at': None, 'id': u'173b9667-3d90-4ec4-bb3c-366260c572f1', 'size': 30L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-26T11:18:41.711447', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce61d0>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce6250>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 26, 11, 18, 24), 'scheduled_at': datetime.datetime(2016, 12, 26, 11, 18, 24), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'joun-03', 'instance_uuid': u'70e0cf4f-1171-4ee0-92cf-dcb580670e62', 'bootable': False, 'created_at': datetime.datetime(2016, 12, 26, 11, 18, 24), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.503 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 26, 11, 15, 42), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'1e5fafbf-5178-4a6c-b03e-b7df85fe537a', 'ec2_id': None, 'mountpoint': u'/dev/vdc', 'deleted_at': None, 'id': u'81942015-4347-4c19-bc5a-ab394fca82ae', 'size': 30L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-26T11:15:42.602346', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce6450>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce64d0>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 26, 11, 15, 32), 'scheduled_at': datetime.datetime(2016, 12, 26, 11, 15, 31), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'jou-02', 'instance_uuid': u'b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d', 'bootable': False, 'created_at': datetime.datetime(2016, 12, 26, 11, 15, 31), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.508 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 26, 11, 7, 25), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'1e5fafbf-5178-4a6c-b03e-b7df85fe537a', 'ec2_id': None, 'mountpoint': u'/dev/vdc', 'deleted_at': None, 'id': u'81612e07-2775-4804-ada7-ae39583b415f', 'size': 30L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-26T11:07:25.538761', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce66d0>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce6750>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 26, 11, 7, 13), 'scheduled_at': datetime.datetime(2016, 12, 26, 11, 7, 13), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'joun-01', 'instance_uuid': u'd104c0a5-6c4b-4060-948a-c2e6ac2af623', 'bootable': False, 'created_at': datetime.datetime(2016, 12, 26, 11, 7, 13), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.513 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 26, 10, 37, 20), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'e80422bc-2764-4f3e-aeaf-d64c0c323cfc', 'ec2_id': None, 'mountpoint': u'/dev/vdb', 'deleted_at': None, 'id': u'6f43b47a-24d7-4234-ba2a-0d90191ab7d1', 'size': 100L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-26T10:37:20.350633', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce6950>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce69d0>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 26, 10, 36, 29), 'scheduled_at': datetime.datetime(2016, 12, 26, 10, 36, 29), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'dzc-o-control01v#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'ceph-04', 'instance_uuid': u'b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d', 'bootable': False, 'created_at': datetime.datetime(2016, 12, 26, 10, 36, 28), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.518 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 26, 10, 37, 6), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'e80422bc-2764-4f3e-aeaf-d64c0c323cfc', 'ec2_id': None, 'mountpoint': u'/dev/vdb', 'deleted_at': None, 'id': u'57e52aca-c1db-4355-8387-27af2d3af84d', 'size': 100L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-26T10:37:06.751190', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce6bd0>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce6c50>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 26, 10, 36, 17), 'scheduled_at': datetime.datetime(2016, 12, 26, 10, 36, 16), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'dzc-o-control01v#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'ceph-03', 'instance_uuid': u'70e0cf4f-1171-4ee0-92cf-dcb580670e62', 'bootable': False, 'created_at': datetime.datetime(2016, 12, 26, 10, 36, 16), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.523 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 26, 10, 36, 8), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'e80422bc-2764-4f3e-aeaf-d64c0c323cfc', 'ec2_id': None, 'mountpoint': u'/dev/vdb', 'deleted_at': None, 'id': u'aadf34bb-9973-4229-a888-aca4ee77340e', 'size': 100L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-26T10:36:08.330674', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce6e50>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4ce6ed0>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 26, 10, 35, 39), 'scheduled_at': datetime.datetime(2016, 12, 26, 10, 35, 39), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'dzc-o-control01v#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'ceph-02', 'instance_uuid': u'd104c0a5-6c4b-4060-948a-c2e6ac2af623', 'bootable': False, 'created_at': datetime.datetime(2016, 12, 26, 10, 35, 38), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.528 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 30, 3, 55, 27), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'e80422bc-2764-4f3e-aeaf-d64c0c323cfc', 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'2007fa22-4282-421b-bc23-439ea07cc86d', 'size': 100L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'ceph-01', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4db7110>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 26, 7, 52, 53), 'scheduled_at': datetime.datetime(2016, 12, 26, 7, 52, 52), 'status': u'available', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'dzc-o-control01v#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'ceph-01', 'instance_uuid': None, 'bootable': False, 'created_at': datetime.datetime(2016, 12, 26, 7, 52, 52), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False'}}
2017-01-03 10:51:15.533 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 19, 5, 48, 1), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'e80422bc-2764-4f3e-aeaf-d64c0c323cfc', 'ec2_id': None, 'mountpoint': u'/dev/vdb', 'deleted_at': None, 'id': u'b3e766cc-871b-4583-823a-d632766795a7', 'size': 100L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-19T05:48:01.498796', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4db7310>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4db7390>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 19, 5, 47, 32), 'scheduled_at': datetime.datetime(2016, 12, 19, 5, 47, 32), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'dzc-o-control01v#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'storm-100G-02', 'instance_uuid': u'139d6041-f06f-4b5a-ad95-98847c0e5d20', 'bootable': False, 'created_at': datetime.datetime(2016, 12, 19, 5, 47, 32), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.538 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 19, 5, 47, 50), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'e80422bc-2764-4f3e-aeaf-d64c0c323cfc', 'ec2_id': None, 'mountpoint': u'/dev/vdb', 'deleted_at': None, 'id': u'1607ae31-5008-4e60-b2d0-ebc7200961a2', 'size': 100L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-19T05:47:49.853915', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4db7590>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4db7610>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 19, 5, 47, 15), 'scheduled_at': datetime.datetime(2016, 12, 19, 5, 47, 14), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'dzc-o-control01v#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'storm-100G-01', 'instance_uuid': u'348c1211-6767-4d65-96d5-13120a2ef774', 'bootable': False, 'created_at': datetime.datetime(2016, 12, 19, 5, 47, 14), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.543 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 19, 5, 46, 49), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'9f6c7a0a-097f-42a5-a862-02581f2687c3', 'size': 100L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': [], 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 19, 5, 46, 49), 'scheduled_at': datetime.datetime(2016, 12, 19, 5, 46, 49), 'status': u'available', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'dzc-o-control01v#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'storm-100G', 'instance_uuid': None, 'bootable': False, 'created_at': datetime.datetime(2016, 12, 19, 5, 46, 49), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None}
2017-01-03 10:51:15.547 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 19, 2, 57, 16), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': u'/dev/vdc', 'deleted_at': None, 'id': u'f0a5c4d6-2faf-4261-bc70-12e69412dfdc', 'size': 20L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-19T02:57:16.471419', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4db7990>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4db7a10>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 19, 2, 57), 'scheduled_at': datetime.datetime(2016, 12, 19, 2, 57), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'test1219', 'instance_uuid': u'af0b6d8b-a1e5-4bbc-b80d-f276e721bc0f', 'bootable': False, 'created_at': datetime.datetime(2016, 12, 19, 2, 57), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.552 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 16, 1, 49, 49), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'aac04eba-fa2d-456d-95ae-fe29f75dff94', 'size': 1L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4db7c10>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 16, 1, 48, 33), 'scheduled_at': datetime.datetime(2016, 12, 16, 1, 48, 29), 'status': u'available', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'', 'instance_uuid': None, 'bootable': True, 'created_at': datetime.datetime(2016, 12, 16, 1, 48, 29), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False'}}
2017-01-03 10:51:15.557 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 15, 14, 15, 35), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'da5b5e7b-61ca-4bc6-a630-4888f3e6ca9d', 'size': 1L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': [], 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': None, 'scheduled_at': datetime.datetime(2016, 12, 15, 14, 15, 35), 'status': u'creating', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'', 'instance_uuid': None, 'bootable': False, 'created_at': datetime.datetime(2016, 12, 15, 14, 15, 35), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None}
2017-01-03 10:51:15.561 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 15, 10, 43, 41), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'3c4e3926-080b-438f-9295-efa8892a4294', 'size': 1L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4db7f90>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 15, 10, 43, 31), 'scheduled_at': datetime.datetime(2016, 12, 15, 10, 43, 28), 'status': u'available', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'', 'instance_uuid': None, 'bootable': True, 'created_at': datetime.datetime(2016, 12, 15, 10, 43, 28), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False'}}
2017-01-03 10:51:15.566 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 15, 10, 38, 47), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'3b27925a-4b6b-4f45-91be-20d38ea6a2ce', 'size': 1L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbc1d0>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 15, 10, 38, 37), 'scheduled_at': datetime.datetime(2016, 12, 15, 10, 38, 34), 'status': u'available', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'', 'instance_uuid': None, 'bootable': True, 'created_at': datetime.datetime(2016, 12, 15, 10, 38, 34), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False'}}
2017-01-03 10:51:15.571 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 15, 10, 29, 34), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'd80033f4-a51f-4e9f-b3ee-47e5f4f16e41', 'size': 1L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbc3d0>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 15, 10, 29, 28), 'scheduled_at': datetime.datetime(2016, 12, 15, 10, 29, 24), 'status': u'available', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'', 'instance_uuid': None, 'bootable': True, 'created_at': datetime.datetime(2016, 12, 15, 10, 29, 24), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False'}}
2017-01-03 10:51:15.576 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 15, 10, 26, 43), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': None, 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'fcbdd832-9f24-4bb1-a87e-f915dc7d5f0b', 'size': 1L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbc5d0>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 12, 15, 10, 26, 37), 'scheduled_at': datetime.datetime(2016, 12, 15, 10, 26, 33), 'status': u'available', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'', 'instance_uuid': None, 'bootable': True, 'created_at': datetime.datetime(2016, 12, 15, 10, 26, 32), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False'}}
2017-01-03 10:51:15.583 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 11, 10, 3, 54, 45), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'e55376b7-a612-436e-87aa-7d21bc46fb5d', 'ec2_id': None, 'mountpoint': u'/dev/vdb', 'deleted_at': None, 'id': u'828f2ffc-574a-4ba1-aaae-b04784dfc84d', 'size': 100L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-11-10T03:54:45.332584', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbc7d0>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbc850>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 11, 10, 3, 54, 30), 'scheduled_at': datetime.datetime(2016, 11, 10, 3, 54, 30), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute09#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'docker-100', 'instance_uuid': u'af0b6d8b-a1e5-4bbc-b80d-f276e721bc0f', 'bootable': False, 'created_at': datetime.datetime(2016, 11, 10, 3, 54, 30), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.588 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 15, 7, 4, 36), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'7c253631-299d-4f42-a08e-161515db1ad8', 'ec2_id': None, 'mountpoint': None, 'deleted_at': None, 'id': u'b544fd26-ac27-4308-908d-2056e4ef6171', 'size': 50L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': None, 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbca50>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 9, 5, 6, 12), 'scheduled_at': datetime.datetime(2016, 9, 5, 6, 12), 'status': u'available', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'kafka-50G-tt', 'instance_uuid': None, 'bootable': False, 'created_at': datetime.datetime(2016, 9, 5, 6, 12), 'attach_status': u'detached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False'}}
2017-01-03 10:51:15.593 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 12, 16, 4, 45, 27), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'7c253631-299d-4f42-a08e-161515db1ad8', 'ec2_id': None, 'mountpoint': u'/dev/vdb', 'deleted_at': None, 'id': u'eec8673e-6624-48cb-8c3f-c18e2df0e5c3', 'size': 50L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-12-16T04:45:27.258362', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbcc50>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbccd0>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 9, 1, 7, 35, 54), 'scheduled_at': datetime.datetime(2016, 9, 1, 7, 35, 53), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'kafka-50G-03', 'instance_uuid': u'5d2bbe60-9400-459e-982d-ef6fb8971748', 'bootable': False, 'created_at': datetime.datetime(2016, 9, 1, 7, 35, 53), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}
2017-01-03 10:51:15.598 31400 INFO cinder.api.v1.volumes [req-8bda30a4-7443-433e-a078-b1a9bd551314 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] vol={'migration_status': None, 'availability_zone': u'nova', 'terminated_at': None, 'updated_at': datetime.datetime(2016, 9, 1, 7, 36, 8), 'provider_geometry': None, 'replication_extended_status': None, 'replication_status': u'disabled', 'snapshot_id': u'7c253631-299d-4f42-a08e-161515db1ad8', 'ec2_id': None, 'mountpoint': u'/dev/vdb', 'deleted_at': None, 'id': u'90be40be-e79e-401c-87d8-fc9387499e9d', 'size': 50L, 'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', 'attach_time': u'2016-09-01T07:36:08.081088', 'attached_host': None, 'display_description': u'', 'volume_admin_metadata': u'[<cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbced0>, <cinder.db.sqlalchemy.models.VolumeAdminMetadata object at 0x4dbcf50>]', 'project_id': u'516953ba1481400fb69642e9ffe14b02', 'launched_at': datetime.datetime(2016, 9, 1, 7, 35, 38), 'scheduled_at': datetime.datetime(2016, 9, 1, 7, 35, 38), 'status': u'in-use', 'volume_type_id': None, 'deleted': False, 'provider_location': None, 'host': u'sjhl-o-compute07#RBD', 'consistencygroup_id': None, 'source_volid': None, 'provider_auth': None, 'display_name': u'kafka-50G-02', 'instance_uuid': u'4808adf5-af83-4160-be3f-3479bbe6d664', 'bootable': False, 'created_at': datetime.datetime(2016, 9, 1, 7, 35, 38), 'attach_status': u'attached', 'volume_type': None, 'consistencygroup': None, 'volume_metadata': [], '_name_id': None, 'encryption_key_id': None, 'replication_driver_data': None, 'metadata': {u'readonly': u'False', u'attached_mode': u'rw'}}



/var/log/cinder/cinder-scheduler.log
2017-01-03 10:52:04.593 31434 DEBUG cinder.scheduler.host_manager [req-53e290d9-191d-496e-a9bd-61832b22a394 - - - - -] Received volume service update from sjhl-o-compute05: {u'volume_backend_name': u'RBD', u'free_capacity_gb': 5127, u'driver_version': u'1.1.0', u'total_capacity_gb': 7070, u'reserved_percentage': 0, u'vendor_name': u'Open Source', u'storage_protocol': u'ceph'} update_service_capabilities /usr/lib/python2.7/site-packages/cinder/scheduler/host_manager.py:434
2017-01-03 10:52:06.667 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Translated 'taskflow.patterns.linear_flow.Flow: volume_create_scheduler; 2' into a graph: _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:209
2017-01-03 10:52:06.668 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Name: volume_create_scheduler _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.668 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Type: DiGraph _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.669 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Frozen: False _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.669 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Nodes: 2 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.669 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    - cinder.scheduler.flows.create_volume.ScheduleCreateVolumeTask;volume:create==1.0 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.670 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    - cinder.scheduler.flows.create_volume.ExtractSchedulerSpecTask;volume:create==1.0 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.670 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Edges: 1 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.670 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]    cinder.scheduler.flows.create_volume.ExtractSchedulerSpecTask;volume:create==1.0 -> cinder.scheduler.flows.create_volume.ScheduleCreateVolumeTask;volume:create==1.0 ({'invariant': True}) _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.671 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Density: 0.500 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.671 31434 DEBUG taskflow.engines.action_engine.compiler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -]  Cycles: 0 _post_flatten /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/compiler.py:212
2017-01-03 10:52:06.673 31434 DEBUG cinder.scheduler.manager [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Flow 'volume_create_scheduler' (68feef60-3a04-4122-a321-768e1453ad32) transitioned into state 'RUNNING' from state 'PENDING' _flow_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:83
2017-01-03 10:52:06.674 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'RESUMING' in response to event 'start' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:52:06.675 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'RESUMING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:52:06.675 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:52:06.676 31434 DEBUG cinder.scheduler.manager [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.scheduler.flows.create_volume.ExtractSchedulerSpecTask;volume:create' (9a9cf909-8190-4a76-beb3-250110c7478d) transitioned into state 'RUNNING' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-03 10:52:06.676 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:52:06.677 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:52:06.677 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:52:06.677 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:52:06.678 31434 DEBUG cinder.scheduler.manager [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.scheduler.flows.create_volume.ExtractSchedulerSpecTask;volume:create' (9a9cf909-8190-4a76-beb3-250110c7478d) transitioned into state 'SUCCESS' with result '{'request_spec': {u'source_replicaid': None, u'volume_properties': {u'status': u'creating', u'volume_type_id': None, u'display_name': u'test02', u'availability_zone': u'nova', u'reservations': [u'4f741e55-4623-4d97-bf7d-9e55fa4b3788', u'd2e1431e-1620-403e-b541-c4f2c28351d7'], u'attach_status': u'detached', u'display_description': u'', u'id': u'c280978f-2f57-4552-a035-7692ad233c35', u'metadata': {}, u'source_replicaid': None, u'encryption_key_id': None, u'volume_metadata': [], u'replication_status': u'disabled', u'snapshot_id': None, u'source_volid': None, u'user_id': u'2a1a6dee31a0461c95a5ba8e7e1f30e1', u'project_id': u'516953ba1481400fb69642e9ffe14b02', u'consistencygroup_id': None, u'size': 10}, u'volume_type': {}, u'image_id': None, u'snapshot_id': None, u'consistencygroup_id': None, u'source_volid': None, u'volume_id': u'c280978f-2f57-4552-a035-7692ad233c35'}}' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:113
2017-01-03 10:52:06.679 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:52:06.679 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:52:06.680 31434 DEBUG cinder.scheduler.manager [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.scheduler.flows.create_volume.ScheduleCreateVolumeTask;volume:create' (f99f9898-fdcd-47ab-90e8-bcb8270b0fbb) transitioned into state 'RUNNING' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-03 10:52:06.690 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host dzc-o-control01v#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.690 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host sjhl-o-compute07#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.691 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host sjhl-o-compute09#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.691 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host sjhl-o-compute10#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.692 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host sjhl-o-compute03#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.692 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host sjhl-o-compute04#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.693 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host dzc-o-control02#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.693 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host sjhl-o-compute08#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.694 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host sjhl-o-compute05#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.694 31434 DEBUG cinder.scheduler.filters.capacity_filter [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Sufficient free space for volume creation on host sjhl-o-compute06#RBD (requested / avail): 10/5127.0 host_passes /usr/lib/python2.7/site-packages/cinder/scheduler/filters/capacity_filter.py:68
2017-01-03 10:52:06.695 31434 DEBUG cinder.scheduler.filter_scheduler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Filtered [host 'dzc-o-control01v#RBD': free_capacity_gb: 5127, pools: None, host 'sjhl-o-compute07#RBD': free_capacity_gb: 5127, pools: None, host 'sjhl-o-compute09#RBD': free_capacity_gb: 5127, pools: None, host 'sjhl-o-compute10#RBD': free_capacity_gb: 5127, pools: None, host 'sjhl-o-compute03#RBD': free_capacity_gb: 5127, pools: None, host 'sjhl-o-compute04#RBD': free_capacity_gb: 5127, pools: None, host 'dzc-o-control02#RBD': free_capacity_gb: 5127, pools: None, host 'sjhl-o-compute08#RBD': free_capacity_gb: 5127, pools: None, host 'sjhl-o-compute05#RBD': free_capacity_gb: 5127, pools: None, host 'sjhl-o-compute06#RBD': free_capacity_gb: 5127, pools: None] _get_weighted_candidates /usr/lib/python2.7/site-packages/cinder/scheduler/filter_scheduler.py:300

2017-01-03 10:52:06.695 31434 DEBUG cinder.scheduler.filter_scheduler [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Choosing dzc-o-control01v#RBD _choose_top_host /usr/lib/python2.7/site-packages/cinder/scheduler/filter_scheduler.py:419
选中了dzc-o-control01v
2017-01-03 10:52:06.744 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:52:06.745 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:52:06.745 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:52:06.746 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:52:06.746 31434 DEBUG cinder.scheduler.manager [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Task 'cinder.scheduler.flows.create_volume.ScheduleCreateVolumeTask;volume:create' (f99f9898-fdcd-47ab-90e8-bcb8270b0fbb) transitioned into state 'SUCCESS' with result 'None' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:113
2017-01-03 10:52:06.747 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'ANALYZING' in response to event 'finished' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:52:06.747 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'GAME_OVER' in response to event 'finished' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:52:06.748 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Exiting old state 'GAME_OVER' in response to event 'success' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-03 10:52:06.748 31434 DEBUG taskflow.engines.action_engine.runner [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Entering new state 'SUCCESS' in response to event 'success' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-03 10:52:06.749 31434 DEBUG cinder.scheduler.manager [req-426c3441-f768-407b-a87e-b6cd5653d10a 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Flow 'volume_create_scheduler' (68feef60-3a04-4122-a321-768e1453ad32) transitioned into state 'SUCCESS' from state 'RUNNING' _flow_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:83
2017-01-03 10:52:08.888 31434 DEBUG cinder.scheduler.host_manager [req-49e44367-ef64-4e44-a859-01a30d97f6b5 - - - - -] Received volume service update from sjhl-o-compute08: {u'volume_backend_name': u'RBD', u'free_capacity_gb': 5127, u'driver_version': u'1.1.0', u'total_capacity_gb': 7070, u'reserved_percentage': 0, u'vendor_name': u'Open Source', u'storage_protocol': u'ceph'} update_service_capabilities /usr/lib/python2.7/site-packages/cinder/scheduler/host_manager.py:434
2017-01-03 10:52:13.042 31434 DEBUG cinder.scheduler.host_manager [req-f2e48d42-fa98-468e-b49c-99b6e3d7344d - - - - -] Received volume service update from sjhl-o-compute04: {u'volume_backend_name': u'RBD', u'free_capacity_gb': 5127, u'driver_version': u'1.1.0', u'total_capacity_gb': 7070, u'reserved_percentage': 0, u'vendor_name': u'Open Source', u'storage_protocol': u'ceph'} update_service_capabilities /usr/lib/python2.7/site-packages/cinder/scheduler/host_manager.py:434
2017-01-03 10:52:15.832 31434 DEBUG cinder.scheduler.host_manager [req-6cdc6d90-7bad-4c4f-89d0-40c91dcbccef - - - - -] Received volume service update from sjhl-o-compute03: {u'volume_backend_name': u'RBD', u'free_capacity_gb': 5127, u'driver_version': u'1.1.0', u'total_capacity_gb': 7070, u'reserved_percentage': 0, u'vendor_name': u'Open Source', u'storage_protocol': u'ceph'} update_service_capabilities /usr/lib/python2.7/site-packages/cinder/scheduler/host_manager.py:434


attach volume
那么问题来了：存储节点上本地的 LV 如何挂载到计算节点的 instance 上呢？通常情况存储节点和计算节点是不同的物理节点。
Target
提供 iSCSI 存储资源的设备，简单的说，就是 iSCSI 服务器。
Initiator
使用 iSCSI 存储资源的设备，也就是 iSCSI 客户端。
下面来看一下 attach volume的操作
1.user向 cinder-api 发送attach请求
2.cinder-api想msg发送请求
3.cinder-volume初始化volume的连接
4.nova-compute 将 volume attach 到 instance


cinder-api 接收到attach volume的请求，attach实际包含两个步骤
1.初始化volume的连接
volume创建后，只是在后端的volume provider中创建了相应的存储对象（如lvm），这时候compute无法使用
cinder-volume必须以某种方式将volume export出来，计算节点才能够访问到，这个export的过程叫做
初始化volume的连接


2017-01-03 11:38:49.448 31482 DEBUG cinder.volume.manager [req-360e6104-b698-4caf-8fb8-e8a163d3e5dd 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Volume 46fe22d1-0352-4156-9cf6-08442ec93f64: creating export initialize_connection /usr/lib/python2.7/site-packages/cinder/volume/manager.py:900
2017-01-03 11:38:49.449 31482 DEBUG cinder.openstack.common.processutils [req-360e6104-b698-4caf-8fb8-e8a163d3e5dd 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Running cmd (subprocess): ceph mon dump --format=json --id cinder --conf /etc/ceph/ceph.conf execute /usr/lib/python2.7/site-packages/cinder/openstack/common/processutils.py:158
2017-01-03 11:38:49.780 31482 DEBUG cinder.openstack.common.processutils [req-360e6104-b698-4caf-8fb8-e8a163d3e5dd 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Result was 0 execute /usr/lib/python2.7/site-packages/cinder/openstack/common/processutils.py:192
2017-01-03 11:38:49.782 31482 DEBUG cinder.volume.drivers.rbd [req-360e6104-b698-4caf-8fb8-e8a163d3e5dd 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] connection data: {'driver_volume_type': 'rbd', 'data': {'secret_type': 'ceph', 'name': u'volumes/volume-46fe22d1-0352-4156-9cf6-08442ec93f64', 'secret_uuid': '3c97b1d3-1153-4454-8ffb-0fbf9ff01311', 'auth_enabled': True, 'hosts': [u'10.20.8.31', u'10.20.8.32', u'10.20.8.33'], 'auth_username': 'cinder', 'ports': [u'6789', u'6789', u'6789']}} initialize_connection /usr/lib/python2.7/site-packages/cinder/volume/drivers/rbd.py:728
2017-01-03 11:38:50.358 31482 DEBUG cinder.openstack.common.lockutils [req-28a6aee2-1bb1-4bf1-af36-2864de85a698 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Got semaphore "46fe22d1-0352-4156-9cf6-08442ec93f64" for method "do_attach"... inner /usr/lib/python2.7/site-packages/cinder/openstack/common/lockutils.py:191
2017-01-03 11:38:50.359 31482 DEBUG cinder.openstack.common.lockutils [req-28a6aee2-1bb1-4bf1-af36-2864de85a698 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Attempting to grab file lock "46fe22d1-0352-4156-9cf6-08442ec93f64" for method "do_attach"... inner /usr/lib/python2.7/site-packages/cinder/openstack/common/lockutils.py:202
2017-01-03 11:38:50.361 31482 DEBUG cinder.openstack.common.lockutils [req-28a6aee2-1bb1-4bf1-af36-2864de85a698 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Got file lock "46fe22d1-0352-4156-9cf6-08442ec93f64" at /var/lib/cinder/tmp/cinder-46fe22d1-0352-4156-9cf6-08442ec93f64 for method "do_attach"... inner /usr/lib/python2.7/site-packages/cinder/openstack/common/lockutils.py:232
2017-01-03 11:38:50.741 31482 DEBUG cinder.openstack.common.lockutils [req-28a6aee2-1bb1-4bf1-af36-2864de85a698 2a1a6dee31a0461c95a5ba8e7e1f30e1 516953ba1481400fb69642e9ffe14b02 - - -] Released file lock "46fe22d1-0352-4156-9cf6-08442ec93f64" at /var/lib/cinder/tmp/cinder-46fe22d1-0352-4156-9cf6-08442ec93f64 for method "do_attach"... inner /usr/lib/python2.7/site-packages/cinder/openstack/common/lockutils.py:239


sjhl-o-compute06上 挂载磁盘
2017-01-03 11:53:19.654 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Created new semaphore "b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 11:53:19.654 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Acquired semaphore "b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 11:53:19.654 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Got semaphore / lock "do_reserve" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2017-01-03 11:53:19.679 6024 DEBUG nova.compute.utils [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] Using /dev/vd instead of None get_next_device_name /usr/lib/python2.7/site-packages/nova/compute/utils.py:175
2017-01-03 11:53:19.699 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Releasing semaphore "b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 11:53:19.699 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Semaphore / lock released "do_reserve" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2017-01-03 11:53:19.998 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Created new semaphore "b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 11:53:19.998 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Acquired semaphore "b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 11:53:19.999 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Got semaphore / lock "do_attach_volume" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2017-01-03 11:53:19.999 6024 AUDIT nova.compute.manager [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] [instance: b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d] Attaching volume c280978f-2f57-4552-a035-7692ad233c35 to /dev/vdd
2017-01-03 11:53:20.000 6024 DEBUG nova.volume.cinder [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] Cinderclient connection created using URL: http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02 get_cinder_client_version /usr/lib/python2.7/site-packages/nova/volume/cinder.py:255
2017-01-03 11:53:20.139 6024 DEBUG nova.openstack.common.processutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf systool -c fc_host -v execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-03 11:53:20.216 6024 DEBUG nova.openstack.common.processutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] Result was 1 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2017-01-03 11:53:20.216 6024 DEBUG nova.virt.libvirt.driver [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] [instance: b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d] Could not determine fibre channel world wide node names get_volume_connector /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:1280
2017-01-03 11:53:20.217 6024 DEBUG nova.openstack.common.processutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf systool -c fc_host -v execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-03 11:53:20.285 6024 DEBUG nova.openstack.common.processutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] Result was 1 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2017-01-03 11:53:20.286 6024 DEBUG nova.virt.libvirt.driver [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] [instance: b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d] Could not determine fibre channel world wide port names get_volume_connector /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:1287
2017-01-03 11:53:20.286 6024 DEBUG nova.volume.cinder [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] Cinderclient connection created using URL: http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02 get_cinder_client_version /usr/lib/python2.7/site-packages/nova/volume/cinder.py:255
更新sjhl-o-compute06  instance 的xml文件
    <disk type='network' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <auth username='cinder'>
        <secret type='ceph' uuid='3c97b1d3-1153-4454-8ffb-0fbf9ff01311'/>
      </auth>
      <source protocol='rbd' name='volumes/volume-c280978f-2f57-4552-a035-7692ad233c35'>
        <host name='10.20.8.31' port='6789'/>
        <host name='10.20.8.32' port='6789'/>
        <host name='10.20.8.33' port='6789'/>
      </source>
      <target dev='vdd' bus='virtio'/>
      <serial>c280978f-2f57-4552-a035-7692ad233c35</serial>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </disk>


2017-01-03 11:53:20.826 6024 DEBUG nova.virt.libvirt.config [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] Generated XML ('<disk type="network" device="disk">\n  <driver name="qemu" type="raw" cache="none"/>\n  <source protocol="rbd" name="volumes/volume-c280978f-2f57-4552-a035-7692ad233c35">\n    <host name="10.20.8.31" port="6789"/>\n    <host name="10.20.8.32" port="6789"/>\n    <host name="10.20.8.33" port="6789"/>\n  </source>\n  <auth username="cinder">\n    <secret type="ceph" uuid="3c97b1d3-1153-4454-8ffb-0fbf9ff01311"/>\n  </auth>\n  <target bus="virtio" dev="vdd"/>\n  <serial>c280978f-2f57-4552-a035-7692ad233c35</serial>\n</disk>\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82
2017-01-03 11:53:20.893 6024 DEBUG nova.volume.cinder [req-d60a7af7-0689-4e2d-875d-e4448a706efe None] Cinderclient connection created using URL: http://controller.light.fang.com:8776/v1/516953ba1481400fb69642e9ffe14b02 get_cinder_client_version /usr/lib/python2.7/site-packages/nova/volume/cinder.py:255
2017-01-03 11:53:21.406 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Releasing semaphore "b99e1bc5-0fb9-4df8-bd84-ab0f10346a0d" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 11:53:21.406 6024 DEBUG nova.openstack.common.lockutils [req-d60a7af7-0689-4e2d-875d-e4448a706efe ] Semaphore / lock released "do_attach_volume" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275


[root@dzc-o-control01v cinder]# ceph df 
GLOBAL:
    SIZE      AVAIL     RAW USED     %RAW USED 
    7070G     5127G        1943G         27.48 
POOLS:
    NAME        ID     USED     %USED     MAX AVAIL     OBJECTS 
    rbd         0         0         0         1519G           0 
    volumes     1      645G      9.12         1519G      167463 
    images      2         0         0         1519G           0 
这个ceph集群中有三个pool  rbd volumes  images

不管你是想为云平台提供Ceph 对象存储和/或 Ceph 块设备，还是想部署一个 Ceph 文件系统或者把 Ceph 作为他用，所有 Ceph 存储集群的部署都始于部署一个个 Ceph 节点、网络和 Ceph 存储集群。 Ceph 存储集群至少需要一个 Ceph Monitor 和两个 OSD 守护进程。而运行 Ceph 文件系统客户端时，则必须要有元数据服务器（ Metadata Server ）。
Ceph OSDs: Ceph OSD 守护进程（ Ceph OSD ）的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD 守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当 Ceph 存储集群设定为有2个副本时，至少需要2个 OSD 守护进程，集群才能达到 active+clean 状态（ Ceph 默认有3个副本，但你可以调整副本数）。
Monitors: Ceph Monitor维护着展示集群状态的各种图表，包括监视器图、 OSD 图、归置组（ PG ）图、和 CRUSH 图。 Ceph 保存着发生在Monitors 、 OSD 和 PG上的每一次状态变更的历史信息（称为 epoch ）。
MDSs: Ceph 元数据服务器（ MDS ）为 Ceph 文件系统存储元数据（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）。元数据服务器使得 POSIX 文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 ls、find 等基本命令。
一个ceph集群可以有多个pool，每个pool是逻辑上的隔离单位，不同的pool可以有完全不一样的数据处理方式，比如Replica Size（副本数）、Placement Groups、CRUSH Rules、快照、所属者等。
[root@dzc-o-control01v cinder]# ceph osd lspools
0 rbd,1 volumes,2 images,
[root@dzc-o-control01v cinder]# rados df 
pool name                 KB      objects       clones     degraded      unfound           rd        rd KB           wr        wr KB
images                     0            0            0            0           0            0            0            0            0
rbd                        0            0            0            0           0            0            0            0            0
volumes            676332769       167463          836            0           0    781455510    379838415   3110494490  39269228795
  total used      2037755072       167463
  total avail     5376344960
  total space     7414100032

池
 

当您第一次部署集群不创建一个存储池，Ceph将使用默认的存储池来存储数据。存储池为了提供了一些额外的功能,包括:

复制: 你可以设置一个对象期望的副本数量。典型配置存储一个对象和一个它的副本（如 size = 2），但你可以更改副本的数量。
配置组: 你可以设置一个存储池的配置组数量。典型配置在每个 OSD 上使用大约 100 个归置组，这样，不用过多计算资源就得到了较优的均衡。设置多个存储池的时候，要注意为这些存储池和集群设置合理的配置组数量。
CRUSH规则:当你在存储池里存数据的时候，映射到存储池的 CRUSH 规则集使得 CRUSH 确定一条规则，用于集群内主对象的归置和其副本的复制。你可以给存储池定制 CRUSH 规则。
快照: 你用 ceph osd pool mksnap 创建快照的时候，实际上创建了一小部分存储池的快照。
设置所有者:你可以设置一个用户 ID 为一个存储池的所有者。

要把数据组织到存储池里，你可以列出、创建、删除存储池，也可以查看每个存储池的利用率。
Placement Groups【配置组】

total  pgs =  osd*100/replica

[root@dzc-o-control01v cinder]# ceph osd dump  --format plain
epoch 2543
fsid 2502730f-0856-4b99-b305-76ca43ebbbe4
created 2016-01-15 14:44:49.524898
modified 2017-01-03 12:27:36.541316
flags 
pool 0 'rbd' replicated size 3 min_size 2 crush_ruleset 0 object_hash rjenkins pg_num 256 pgp_num 256 last_change 67 flags hashpspool stripe_width 0
pool 1 'volumes' replicated size 3 min_size 2 crush_ruleset 0 object_hash rjenkins pg_num 128 pgp_num 128 last_change 2540 flags hashpspool stripe_width 0
	removed_snaps [1~1]
pool 2 'images' replicated size 3 min_size 2 crush_ruleset 0 object_hash rjenkins pg_num 128 pgp_num 128 last_change 71 flags hashpspool stripe_width 0
pool 3 'test-pool' replicated size 3 min_size 2 crush_ruleset 0 object_hash rjenkins pg_num 8 pgp_num 8 last_change 2543 flags hashpspool stripe_width 0
max_osd 16
osd.0 up   in  weight 1 up_from 2395 up_thru 2541 down_at 1736 last_clean_interval [330,1709) 10.20.8.38:6800/19809 192.168.32.10:6800/19809 192.168.32.10:6801/19809 10.20.8.38:6801/19809 exists,up f47fea10-1dae-46d5-b4a7-f0ec6b1a3d6c
osd.1 up   in  weight 1 up_from 2389 up_thru 2526 down_at 2385 last_clean_interval [323,2384) 10.20.8.23:6801/17815 192.168.32.12:6800/17815 192.168.32.12:6801/17815 10.20.8.23:6803/17815 exists,up 5ac66b8b-48e9-4612-9520-b62a682d6685
osd.2 up   in  weight 1 up_from 2526 up_thru 2538 down_at 2524 last_clean_interval [2513,2523) 10.20.8.41:6801/2925 192.168.32.13:6800/2925 192.168.32.13:6801/2925 10.20.8.41:6802/2925 exists,up 689dea8b-a446-42ce-8207-7215455f5404
osd.3 up   in  weight 1 up_from 1702 up_thru 2541 down_at 1696 last_clean_interval [367,1695) 10.20.8.42:6802/3401 192.168.32.14:6802/3401 192.168.32.14:6803/3401 10.20.8.42:6803/3401 exists,up d7f8008c-7c54-44b3-8832-9e414f893edd
osd.4 up   in  weight 1 up_from 2373 up_thru 2538 down_at 1710 last_clean_interval [308,1697) 10.20.8.44:6800/16776 192.168.32.16:6800/16776 192.168.32.16:6801/16776 10.20.8.44:6801/16776 exists,up fc06b497-e09d-4ee3-9a7b-2ffe9a0b3c80
osd.5 up   in  weight 1 up_from 2367 up_thru 2541 down_at 2365 last_clean_interval [1287,2364) 10.20.8.45:6800/22754 192.168.32.17:6800/22754 192.168.32.17:6802/22754 10.20.8.45:6804/22754 exists,up 847f4e5a-d425-4b8a-9052-00916ffdb3d2
osd.6 up   in  weight 1 up_from 2395 up_thru 2538 down_at 1732 last_clean_interval [330,1709) 10.20.8.38:6802/19962 192.168.32.10:6802/19962 192.168.32.10:6803/19962 10.20.8.38:6803/19962 exists,up 2b256b1b-158f-4ec7-a774-d45f6ffeed1b
osd.7 up   in  weight 1 up_from 2389 up_thru 2541 down_at 2383 last_clean_interval [326,2382) 10.20.8.23:6804/17957 192.168.32.12:6802/17957 192.168.32.12:6803/17957 10.20.8.23:6805/17957 exists,up 95c6e7cb-94da-49ac-9fc1-1205d9026c5e
osd.8 up   in  weight 1 up_from 2526 up_thru 2541 down_at 2522 last_clean_interval [2514,2521) 10.20.8.41:6803/3089 192.168.32.13:6802/3089 192.168.32.13:6803/3089 10.20.8.41:6804/3089 exists,up 65a0d9ec-8328-4865-b351-01124f44c6d7
osd.9 up   in  weight 1 up_from 1700 up_thru 2538 down_at 1696 last_clean_interval [364,1695) 10.20.8.42:6800/3078 192.168.32.14:6800/3078 192.168.32.14:6801/3078 10.20.8.42:6801/3078 exists,up b082dc23-ce0a-4718-9f36-c10dbc2fc4ac
osd.10 up   in  weight 1 up_from 2373 up_thru 2538 down_at 1710 last_clean_interval [308,1697) 10.20.8.44:6802/16922 192.168.32.16:6802/16922 192.168.32.16:6803/16922 10.20.8.44:6803/16922 exists,up 6ddda7b6-9619-4d76-9c32-43493c30e7a4
osd.11 up   in  weight 1 up_from 2368 up_thru 2541 down_at 2363 last_clean_interval [1286,2362) 10.20.8.45:6805/22912 192.168.32.17:6804/22912 192.168.32.17:6805/22912 10.20.8.45:6806/22912 exists,up 6580d75e-0621-46d5-aa53-8b01f18fca78
osd.12 up   in  weight 1 up_from 2403 up_thru 2538 down_at 2106 last_clean_interval [2078,2097) 10.20.8.39:6800/16613 192.168.32.11:6800/16613 192.168.32.11:6801/16613 10.20.8.39:6801/16613 exists,up 886cbe62-036b-4ae3-9201-26b7673fa45a
osd.13 up   in  weight 1 up_from 2379 up_thru 2538 down_at 2148 last_clean_interval [2080,2362) 10.20.8.39:6802/16762 192.168.32.11:6802/16762 192.168.32.11:6803/16762 10.20.8.39:6803/16762 exists,up 22e3c816-ecc2-4904-8fbc-bdbfe8f210e6
osd.14 up   in  weight 1 up_from 2520 up_thru 2538 down_at 2518 last_clean_interval [1360,2517) 10.20.8.43:6801/5493 192.168.32.15:6800/5493 192.168.32.15:6801/5493 10.20.8.43:6802/5493 exists,up aa9259a7-7ebb-41cd-8f56-4f1abc03d539
osd.15 up   in  weight 1 up_from 2520 up_thru 2526 down_at 2516 last_clean_interval [1360,2515) 10.20.8.43:6803/5652 192.168.32.15:6802/5652 192.168.32.15:6803/5652 10.20.8.43:6804/5652 exists,up 401c2b55-4298-49ec-b15c-ac9217bd1d01
  
########################################################################################################################
热迁移报错
[root@sjhl-o-compute05 nova]# grep "2017-01-03 18:15:14.432 9535 ERROR" -C10 nova-compute.log 
2017-01-03 18:15:04.956 9535 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-03 18:15:05.022 9535 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2017-01-03 18:15:05.024 9535 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-03 18:15:05.091 9535 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2017-01-03 18:15:05.113 9535 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-03 18:15:05.175 9535 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2017-01-03 18:15:05.177 9535 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-03 18:15:05.237 9535 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2017-01-03 18:15:07.977 9535 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_volume_usage run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2017-01-03 18:15:07.977 9535 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3048f10>> sleeping for 9.92 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2017-01-03 18:15:14.432 9535 ERROR nova.virt.libvirt.driver [-] [instance: a26b2dde-525b-4038-8937-76e51ce754ed] Live Migration failure: internal error: unable to execute QEMU command 'migrate': this feature or command is not currently supported
2017-01-03 18:15:17.904 9535 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rescued_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2017-01-03 18:15:17.904 9535 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3048f10>> sleeping for 5.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2017-01-03 18:15:22.904 9535 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2017-01-03 18:15:22.905 9535 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3048f10>> sleeping for 2.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2017-01-03 18:15:24.901 9535 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._run_pending_deletes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2017-01-03 18:15:24.902 9535 DEBUG nova.compute.manager [-] Cleaning up deleted instances _run_pending_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:6223
2017-01-03 18:15:24.932 9535 DEBUG nova.compute.manager [-] There are 0 instances to clean _run_pending_deletes /usr/lib/python2.7/site-packages/nova/compute/manager.py:6232
2017-01-03 18:15:24.933 9535 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x3048f10>> sleeping for 1.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2017-01-03 18:15:25.933 9535 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager.update_available_resource run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2017-01-03 18:15:25.934 9535 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources



[root@sjhl-o-compute07 nova]# grep -C10 "2017-01-03 18:15:12.888 13992 DEBUG nova.virt.disk.api \[req-b412b3fa-4fd1-42e" nova-compute.log
2017-01-03 18:15:05.860 13992 DEBUG nova.virt.disk.vfs.api [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Instance for image imgfile=/var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk imgfmt=qcow2 partition=None instance_for_image /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/api.py:45
2017-01-03 18:15:05.860 13992 DEBUG nova.virt.disk.vfs.api [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Using primary VFSGuestFS instance_for_image /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/api.py:49
2017-01-03 18:15:09.351 13992 DEBUG nova.virt.disk.vfs.guestfs [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Setting up appliance for /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk qcow2 setup /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:137
2017-01-03 18:15:09.352 13992 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_rebooting_instances run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2017-01-03 18:15:09.353 13992 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2584f10>> sleeping for 0.10 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2017-01-03 18:15:09.450 13992 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._check_instance_build_time run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2017-01-03 18:15:09.451 13992 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2584f10>> sleeping for 5.00 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2017-01-03 18:15:12.745 13992 DEBUG nova.virt.disk.vfs.guestfs [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Mount guest OS image /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk partition None setup_os_static /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:83
2017-01-03 18:15:12.834 13992 DEBUG nova.virt.disk.vfs.guestfs [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Tearing down appliance teardown /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:185
2017-01-03 18:15:12.836 13992 WARNING nova.virt.disk.vfs.guestfs [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Failed to close augeas aug_close: do_aug_close: you must call 'aug-init' first to initialize Augeas
2017-01-03 18:15:12.888 13992 DEBUG nova.virt.disk.api [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Unable to mount image /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk with error Error mounting /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk with libguestfs (mount_options: /dev/sda on / (options: ''): mount: /dev/sda is write-protected, mounting read-only
mount: unknown filesystem type '(null)'). Cannot resize. is_image_partitionless /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:218
2017-01-03 18:15:12.888 13992 DEBUG nova.openstack.common.lockutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb ] Released file lock "/var/lib/nova/instances/locks/nova-d2c8da7000ccecc9fff5da7b843148f0465b785a" release /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:115
2017-01-03 18:15:12.889 13992 DEBUG nova.openstack.common.lockutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb ] Releasing semaphore "d2c8da7000ccecc9fff5da7b843148f0465b785a" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 18:15:12.889 13992 DEBUG nova.openstack.common.lockutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb ] Semaphore / lock released "copy_qcow2_image" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:275
2017-01-03 18:15:12.889 13992 DEBUG nova.virt.libvirt.vif [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=False,config_drive='',created_at=2017-01-03T09:53:13Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='tet22',display_name='tet22-a26b2dde-525b-4038-8937-76e51ce754ed',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute05',hostname='tet22-a26b2dde-525b-4038-8937-76e51ce754ed',id=239,image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',info_cache=InstanceInfoCache,instance_type_id=2,kernel_id='',key_data=None,key_name=None,launch_index=7,launched_at=2017-01-03T09:53:24Z,launched_on='sjhl-o-compute05',locked=False,locked_by=None,memory_mb=512,metadata={},node='sjhl-o-compute05',numa_topology=<?>,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='e599088c985f42e7948b12f601705cd3',ramdisk_id='',reservation_id='r-34tpofjr',root_device_name='/dev/vda',root_gb=1,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='db5bc6ea-2b2d-4091-8461-e4aa9d7b2be5',image_container_format='bare',image_disk_format='qcow2',image_min_disk='1',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='1',instance_type_id='2',instance_type_memory_mb='512',instance_type_name='m1.tiny',instance_type_root_gb='1',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='1',network_allocated='True'},task_state='migrating',terminated_at=None,updated_at=2017-01-03T10:15:04Z,user_data=None,user_id='7d5b5abc30ea463690567e5f8cc794f9',uuid=a26b2dde-525b-4038-8937-76e51ce754ed,vcpus=1,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'57b34687-1629-4ae1-8247-4cef5d84b4be', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:2e:38:74', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.125', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tap57b34687-16', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:2e:38:74', 'active': True, 'type': u'ovs', 'id': u'57b34687-1629-4ae1-8247-4cef5d84b4be', 'qbg_params': None}) plug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:531
2017-01-03 18:15:12.891 13992 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl addbr qbr57b34687-16 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-03 18:15:12.981 13992 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2017-01-03 18:15:12.982 13992 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl setfd qbr57b34687-16 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-03 18:15:13.053 13992 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
2017-01-03 18:15:13.053 13992 DEBUG nova.openstack.common.processutils [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl stp qbr57b34687-16 off execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161



[root@sjhl-o-compute07 nova]# grep e6585ae5-8b63-48af-a763-68198fba6565 nova-compute.log
2017-01-04 11:10:42.923 13992 DEBUG nova.openstack.common.lockutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] Created new semaphore "refresh_cache-e6585ae5-8b63-48af-a763-68198fba6565" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-04 11:10:42.923 13992 DEBUG nova.openstack.common.lockutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] Acquired semaphore "refresh_cache-e6585ae5-8b63-48af-a763-68198fba6565" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-04 11:10:42.924 13992 DEBUG nova.network.neutronv2.api [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] [instance: e6585ae5-8b63-48af-a763-68198fba6565] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=e599088c985f42e7948b12f601705cd3&device_id=e6585ae5-8b63-48af-a763-68198fba6565 -X GET -H "X-Auth-Token: 648a128422b94f33947ccf7b38a73578" -H "User-Agent: python-neutronclient"
2017-01-04 11:10:42.965 13992 DEBUG neutronclient.client [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] RESP:200 {'date': 'Wed, 04 Jan 2017 03:10:42 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '744', 'x-openstack-request-id': 'req-dc51b214-5814-4692-9570-1a23b9c20dc6'} {"ports": [{"status": "ACTIVE", "binding:host_id": "sjhl-o-compute05", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.147"}], "id": "36e20199-ca08-4d08-b5a7-29e88a755ee4", "security_groups": ["b20cac25-ec58-4830-b7f3-888a54ce1391"], "device_id": "e6585ae5-8b63-48af-a763-68198fba6565", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:21:48:f9"}]}
2017-01-04 11:10:43.080 13992 DEBUG nova.openstack.common.lockutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] Releasing semaphore "refresh_cache-e6585ae5-8b63-48af-a763-68198fba6565" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-04 11:10:43.081 13992 DEBUG nova.openstack.common.lockutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] Created new semaphore "/var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk.info" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-04 11:10:43.082 13992 DEBUG nova.openstack.common.lockutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] Acquired semaphore "/var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-04 11:10:43.082 13992 DEBUG nova.openstack.common.lockutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] Releasing semaphore "/var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk.info" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-04 11:10:43.217 13992 DEBUG nova.openstack.common.processutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Running cmd (subprocess): qemu-img create -f qcow2 -o backing_file=/var/lib/nova/instances/_base/93ba25f6d23f8419a1c88ca3a07af197dbd365f0 /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-04 11:10:43.308 13992 DEBUG nova.virt.disk.api [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Checking if we can resize image /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk. size=32212254720 can_resize_image /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:192
2017-01-04 11:10:43.308 13992 DEBUG nova.openstack.common.processutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-04 11:10:43.368 13992 DEBUG nova.openstack.common.processutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Running cmd (subprocess): qemu-img resize /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk 32212254720 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-04 11:10:43.528 13992 DEBUG nova.virt.disk.api [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Checking if we can resize filesystem inside /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk. CoW=True is_image_partitionless /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:206
2017-01-04 11:10:43.529 13992 DEBUG nova.virt.disk.vfs.api [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Instance for image imgfile=/var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk imgfmt=qcow2 partition=None instance_for_image /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/api.py:45
2017-01-04 11:10:43.529 13992 DEBUG nova.virt.disk.vfs.guestfs [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Setting up appliance for /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk qcow2 setup /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:137
2017-01-04 11:10:46.972 13992 DEBUG nova.virt.disk.vfs.guestfs [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Mount guest OS image /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk partition None setup_os_static /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:83
2017-01-04 11:10:47.071 13992 DEBUG nova.virt.disk.api [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Unable to mount image /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk with error Error mounting /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565/disk with libguestfs (mount_options: /dev/sda on / (options: ''): mount: /dev/sda is already mounted or /sysroot busy). Cannot resize. is_image_partitionless /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:218
2017-01-04 11:10:47.073 13992 DEBUG nova.virt.libvirt.vif [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=False,config_drive='',created_at=2017-01-04T03:10:13Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='asdsad',display_name='asdsad',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute05',hostname='asdsad',id=256,image_ref='1ca0181d-24f4-4c13-938e-e05ade3d1ada',info_cache=InstanceInfoCache,instance_type_id=21,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2017-01-04T03:10:19Z,launched_on='sjhl-o-compute05',locked=False,locked_by=None,memory_mb=8192,metadata={},node='sjhl-o-compute05',numa_topology=<?>,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='e599088c985f42e7948b12f601705cd3',ramdisk_id='',reservation_id='r-da8pvvu5',root_device_name='/dev/vda',root_gb=30,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='1ca0181d-24f4-4c13-938e-e05ade3d1ada',image_container_format='bare',image_disk_format='qcow2',image_min_disk='30',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='ab132230-f7ae-4a1b-a3cf-cb9674fd004b',instance_type_id='21',instance_type_memory_mb='8192',instance_type_name='fang-kibanna',instance_type_root_gb='30',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='8',network_allocated='True'},task_state='migrating',terminated_at=None,updated_at=2017-01-04T03:10:42Z,user_data=None,user_id='7d5b5abc30ea463690567e5f8cc794f9',uuid=e6585ae5-8b63-48af-a763-68198fba6565,vcpus=8,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'36e20199-ca08-4d08-b5a7-29e88a755ee4', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:21:48:f9', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.147', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tap36e20199-ca', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:21:48:f9', 'active': True, 'type': u'ovs', 'id': u'36e20199-ca08-4d08-b5a7-29e88a755ee4', 'qbg_params': None}) plug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:531
2017-01-04 11:10:47.856 13992 DEBUG nova.openstack.common.processutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl --timeout=120 -- --if-exists del-port qvo36e20199-ca -- add-port br-int qvo36e20199-ca -- set Interface qvo36e20199-ca external-ids:iface-id=36e20199-ca08-4d08-b5a7-29e88a755ee4 external-ids:iface-status=active external-ids:attached-mac=fa:16:3e:21:48:f9 external-ids:vm-uuid=e6585ae5-8b63-48af-a763-68198fba6565 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-04 11:10:48.293 13992 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483499448.29, e6585ae5-8b63-48af-a763-68198fba6565 => Started> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2017-01-04 11:10:48.294 13992 INFO nova.compute.manager [-] [instance: e6585ae5-8b63-48af-a763-68198fba6565] VM Started (Lifecycle Event)
2017-01-04 11:10:48.369 13992 DEBUG nova.compute.manager [-] [instance: e6585ae5-8b63-48af-a763-68198fba6565] Synchronizing instance power state after lifecycle event "Started"; current vm_state: active, current task_state: migrating, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2017-01-04 11:10:48.439 13992 INFO nova.compute.manager [-] [instance: e6585ae5-8b63-48af-a763-68198fba6565] During the sync_power process the instance has moved from host sjhl-o-compute05 to host sjhl-o-compute07
2017-01-04 11:10:48.505 13992 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483499448.51, e6585ae5-8b63-48af-a763-68198fba6565 => Stopped> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2017-01-04 11:10:48.505 13992 INFO nova.compute.manager [-] [instance: e6585ae5-8b63-48af-a763-68198fba6565] VM Stopped (Lifecycle Event)
2017-01-04 11:10:48.563 13992 DEBUG nova.compute.manager [-] [instance: e6585ae5-8b63-48af-a763-68198fba6565] Synchronizing instance power state after lifecycle event "Stopped"; current vm_state: active, current task_state: migrating, current DB power_state: 1, VM power_state: 4 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2017-01-04 11:10:48.624 13992 DEBUG nova.openstack.common.lockutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] Created new semaphore "refresh_cache-e6585ae5-8b63-48af-a763-68198fba6565" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-04 11:10:48.624 13992 DEBUG nova.openstack.common.lockutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] Acquired semaphore "refresh_cache-e6585ae5-8b63-48af-a763-68198fba6565" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-04 11:10:48.624 13992 DEBUG nova.network.neutronv2.api [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] [instance: e6585ae5-8b63-48af-a763-68198fba6565] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=e599088c985f42e7948b12f601705cd3&device_id=e6585ae5-8b63-48af-a763-68198fba6565 -X GET -H "X-Auth-Token: 648a128422b94f33947ccf7b38a73578" -H "User-Agent: python-neutronclient"
2017-01-04 11:10:48.631 13992 INFO nova.compute.manager [-] [instance: e6585ae5-8b63-48af-a763-68198fba6565] During the sync_power process the instance has moved from host sjhl-o-compute05 to host sjhl-o-compute07
2017-01-04 11:10:48.656 13992 DEBUG neutronclient.client [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] RESP:200 {'date': 'Wed, 04 Jan 2017 03:10:48 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '744', 'x-openstack-request-id': 'req-da03c06f-5e9a-4736-9d66-c7c39b5fb16f'} {"ports": [{"status": "ACTIVE", "binding:host_id": "sjhl-o-compute05", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "b7dba8f8-62ac-4286-ba2f-093cc3d95bc1", "ip_address": "192.168.34.147"}], "id": "36e20199-ca08-4d08-b5a7-29e88a755ee4", "security_groups": ["b20cac25-ec58-4830-b7f3-888a54ce1391"], "device_id": "e6585ae5-8b63-48af-a763-68198fba6565", "name": "", "admin_state_up": true, "network_id": "d647355e-b019-4de0-999e-00107531edc0", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:21:48:f9"}]}
2017-01-04 11:10:48.792 13992 DEBUG nova.openstack.common.lockutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e ] Releasing semaphore "refresh_cache-e6585ae5-8b63-48af-a763-68198fba6565" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-04 11:10:48.815 13992 WARNING nova.virt.libvirt.driver [-] [instance: e6585ae5-8b63-48af-a763-68198fba6565] During wait destroy, instance disappeared.
2017-01-04 11:10:48.816 13992 DEBUG nova.virt.libvirt.vif [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=False,config_drive='',created_at=2017-01-04T03:10:13Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='asdsad',display_name='asdsad',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute05',hostname='asdsad',id=256,image_ref='1ca0181d-24f4-4c13-938e-e05ade3d1ada',info_cache=InstanceInfoCache,instance_type_id=21,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2017-01-04T03:10:19Z,launched_on='sjhl-o-compute05',locked=False,locked_by=None,memory_mb=8192,metadata={},node='sjhl-o-compute05',numa_topology=<?>,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='e599088c985f42e7948b12f601705cd3',ramdisk_id='',reservation_id='r-da8pvvu5',root_device_name='/dev/vda',root_gb=30,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='1ca0181d-24f4-4c13-938e-e05ade3d1ada',image_container_format='bare',image_disk_format='qcow2',image_min_disk='30',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='ab132230-f7ae-4a1b-a3cf-cb9674fd004b',instance_type_id='21',instance_type_memory_mb='8192',instance_type_name='fang-kibanna',instance_type_root_gb='30',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='8',network_allocated='True'},task_state=None,terminated_at=None,updated_at=2017-01-04T03:10:48Z,user_data=None,user_id='7d5b5abc30ea463690567e5f8cc794f9',uuid=e6585ae5-8b63-48af-a763-68198fba6565,vcpus=8,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'36e20199-ca08-4d08-b5a7-29e88a755ee4', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:21:48:f9', 'floating_ips': [], 'label': u'DMZ_NET', 'meta': {}, 'address': u'192.168.34.147', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.34.3'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.34.254'})})], 'cidr': u'192.168.34.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'd647355e-b019-4de0-999e-00107531edc0', 'label': u'DMZ_NET'}), 'devname': u'tap36e20199-ca', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:21:48:f9', 'active': True, 'type': u'ovs', 'id': u'36e20199-ca08-4d08-b5a7-29e88a755ee4', 'qbg_params': None}) unplug /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:678
2017-01-04 11:10:49.290 13992 DEBUG nova.objects.instance [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Lazy-loading `system_metadata' on Instance uuid e6585ae5-8b63-48af-a763-68198fba6565 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2017-01-04 11:10:49.356 13992 DEBUG nova.openstack.common.processutils [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] Running cmd (subprocess): mv /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565 /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565_del execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-04 11:10:49.364 13992 INFO nova.virt.libvirt.driver [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] [instance: e6585ae5-8b63-48af-a763-68198fba6565] Deleting instance files /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565_del
2017-01-04 11:10:49.365 13992 INFO nova.virt.libvirt.driver [req-68ef486b-7c70-4ff2-93e6-9b13a4137c5e None] [instance: e6585ae5-8b63-48af-a763-68198fba6565] Deletion of /var/lib/nova/instances/e6585ae5-8b63-48af-a763-68198fba6565_del complete



2017-01-03 18:15:12.888 13992 DEBUG nova.virt.disk.api [req-b412b3fa-4fd1-42ec-9779-14b06b3e12eb None] Unable to mount image /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk with error Error mounting /var/lib/nova/instances/a26b2dde-525b-4038-8937-76e51ce754ed/disk with libguestfs (mount_options: /dev/sda on / (options: ''): mount: /dev/sda is write-protected, mounting read-only
2017-01-03 18:40:35.166 13992 DEBUG nova.virt.disk.api [req-c4a20294-578c-476a-aed0-a45c878f1e0c None] Unable to mount image /var/lib/nova/instances/e49a43bb-6e46-4ac1-94eb-9310ed8786e1/disk with error Error mounting /var/lib/nova/instances/e49a43bb-6e46-4ac1-94eb-9310ed8786e1/disk with libguestfs (mount_options: /dev/sda on / (options: ''): mount: /dev/sda is already mounted or /sysroot busy). Cannot resize. is_image_partitionless /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:218

######################################################################################################################## 
新建volume launch instance   报错
创建一个新的 volume，将 image 的数据 copy 到 volume，然后从该 volume launch


2017-01-03 19:00:28.834 13766 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 19:00:28.835 13766 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 19:00:28.835 13766 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 19:00:28.835 13766 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 19:00:28.835 13766 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 19:00:28.835 13766 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 19:00:28.835 13766 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 19:00:28.836 13766 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 19:00:28.836 13766 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 19:00:28.836 13766 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 19:00:28.836 13766 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 19:00:28.836 13766 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 19:00:28.836 13766 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 19:00:28.836 13766 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 19:00:28.837 13766 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 19:00:28.837 13766 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 19:00:28.837 13766 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 19:00:28.837 13766 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 19:00:28.837 13766 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 19:00:28.837 13766 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 19:00:28.838 13766 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 19:00:28.838 13766 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 19:00:28.838 13766 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 19:00:28.838 13766 DEBUG nova.network.base_api [-] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'dfeeba7b-f32c-43f4-b790-a4de12d4b3d2', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.33.26'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.33.4'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.33.254'})})], 'cidr': u'192.168.33.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'ba31704e-c1b6-4a43-8eb6-6162d25e7b29', 'label': u'INSIDE_NET'}), 'devname': u'tapdfeeba7b-f3', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:01:d0:37', 'active': True, 'type': u'ovs', 'id': u'dfeeba7b-f32c-43f4-b790-a4de12d4b3d2', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2017-01-03 19:00:28.843 13766 DEBUG nova.compute.manager [req-3ba67862-7781-4595-95a6-2361eab37c33 None] [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] Getting RDP console get_rdp_console /usr/lib/python2.7/site-packages/nova/compute/manager.py:4418
2017-01-03 19:00:28.858 13766 DEBUG nova.openstack.common.lockutils [-] Releasing semaphore "refresh_cache-62d621fb-bd7b-4922-be0a-9eb26f079d96" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-03 19:00:28.858 13766 DEBUG nova.compute.manager [-] [instance: 62d621fb-bd7b-4922-be0a-9eb26f079d96] Updated the network info_cache for instance _heal_instance_info_cache /usr/lib/python2.7/site-packages/nova/compute/manager.py:5350
2017-01-03 19:00:28.859 13766 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x31e2f10>> sleeping for 3.99 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
2017-01-03 19:00:29.037 13766 WARNING nova.compute.manager [-] Volume id: 90dca8df-7080-4ede-8836-a259c9676f4c finished being created but was not set as 'available'
2017-01-03 19:00:29.038 13766 DEBUG nova.volume.cinder [-] Cinderclient connection created using URL: http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3 get_cinder_client_version /usr/lib/python2.7/site-packages/nova/volume/cinder.py:255
2017-01-03 19:00:29.202 13766 ERROR nova.compute.manager [-] [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] Instance failed block device setup
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] Traceback (most recent call last):
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1822, in _prep_block_device
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]     do_check_attach=do_check_attach) +
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]   File "/usr/lib/python2.7/site-packages/nova/virt/block_device.py", line 407, in attach_block_devices
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]     map(_log_and_attach, block_device_mapping)
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]   File "/usr/lib/python2.7/site-packages/nova/virt/block_device.py", line 405, in _log_and_attach
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]     bdm.attach(*attach_args, **attach_kwargs)
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]   File "/usr/lib/python2.7/site-packages/nova/virt/block_device.py", line 339, in attach
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]     do_check_attach=do_check_attach)
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]   File "/usr/lib/python2.7/site-packages/nova/virt/block_device.py", line 46, in wrapped
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]     ret_val = method(obj, context, *args, **kwargs)
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]   File "/usr/lib/python2.7/site-packages/nova/virt/block_device.py", line 229, in attach
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]     volume_api.check_attach(context, volume, instance=instance)
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]   File "/usr/lib/python2.7/site-packages/nova/volume/cinder.py", line 305, in check_attach
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]     raise exception.InvalidVolume(reason=msg)
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] InvalidVolume: Invalid volume: status must be 'available'
2017-01-03 19:00:29.202 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] 
2017-01-03 19:00:29.215 13766 ERROR nova.compute.manager [-] [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] Failure prepping block device
2017-01-03 19:00:29.215 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] Traceback (most recent call last):
2017-01-03 19:00:29.215 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2221, in _build_resources
2017-01-03 19:00:29.215 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]     block_device_mapping)
2017-01-03 19:00:29.215 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1850, in _prep_block_device
2017-01-03 19:00:29.215 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe]     raise exception.InvalidBDM()
2017-01-03 19:00:29.215 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] InvalidBDM: Block Device Mapping is Invalid.
2017-01-03 19:00:29.215 13766 TRACE nova.compute.manager [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] 
2017-01-03 19:00:29.216 13766 DEBUG nova.compute.claims [-] [instance: 1db745d1-f44a-4f51-888e-a05edc07b6fe] Aborting claim: [Claim: 16000 MB memory, 100 GB disk] abort /usr/lib/python2.7/site-packages/nova/compute/claims.py:128
2017-01-03 19:00:29.216 13766 DEBUG nova.openstack.common.lockutils [-] Created new semaphore "compute_resources" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-03 19:00:29.216 13766 DEBUG nova.openstack.common.lockutils [-] Acquired semaphore "compute_resources" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-03 19:00:29.217 13766 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock "abort_instance_claim" inner /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:271
2017-01-03 19:00:29.247 13766 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute06', 'sjhl-o-compute06')
 
########################################################################################################################
在日志中可以看到               centos6.5 镜像
Attempting download of 651a3d42-d699-45b8-92d3-2ca51a9d15df ((None, None)) to volume 5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e
[root@dzc-o-control01v images]# qemu-img  info 651a3d42-d699-45b8-92d3-2ca51a9d15df
image: 651a3d42-d699-45b8-92d3-2ca51a9d15df
file format: qcow2
virtual size: 15G (16106127360 bytes)
disk size: 15G
cluster_size: 65536
Format specific information:
    compat: 1.1
    lazy refcounts: true


将磁盘下载到
qemu-img info /var/lib/cinder/conversion/tmpswVzk1 
开始的时候/var/lib/cinder/空间不够 做了一个软连接 ln -sv /data/ceph-tmp-volume  /var/lib/cinder  && chown -R cinder.cinder /data/ceph-tmp-volume 
Image fetch details: dest /var/lib/cinder/conversion/tmpswVzk1, size 15362.00 MB, duration 120.05 sec
Image download 15362.00 MB at 127.96 MB/s  可以在zabbix中看到网卡 eth0 10.20.8.31 有流量突高
651a3d42-d699-45b8-92d3-2ca51a9d15df was qcow2, converting to raw

#######注意这里的convert 目标目录是 /tmp/  /tmp没法做软连接

Running cmd (subprocess): sudo cinder-rootwrap /etc/cinder/rootwrap.conf qemu-img convert -O raw /var/lib/cinder/conversion/tmpswVzk1 /tmp/tmprSVeNb
convert的目的目录可以在 /etc/cinder/cinder.conf 中设置
# Directory where temporary image files are stored when the volume driver does not write them directly to the volume.(string value)
#volume_tmp_dir=<None> 
volume_tmp_dir=/data/tmp   #默认的目录是 /tmp 

2017-01-06 11:36:55.476 28812 DEBUG nova.scheduler.filter_scheduler [req-c28913a8-7d78-423d-84b9-fc7807df3247 None] Weighed [WeighedHost [host: (sjhl-o-compute09, sjhl-o-compute09) ram:86288 disk:308224 io_ops:0 instances:5, weight: 1.0]
然后查看nova-scheduler日志可以看到选中的nova机器是compute09，但是09上还是报错了




2017-01-05 21:03:17.910 24073 DEBUG cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.CreateVolumeFromSpecTask;volume:create' (21320db4-e806-4fd2-828b-aa08340c079c) transitioned into state 'RUNNING' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-05 21:03:17.911 24073 INFO cinder.volume.flows.manager.create_volume [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Volume 5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e: being created as image with specification: {'status': u'creating', 'image_location': (None, None), 'volume_size': 30, 'volume_name': u'volume-5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e', 'image_id': u'651a3d42-d699-45b8-92d3-2ca51a9d15df', 'image_service': <cinder.image.glance.GlanceImageService object at 0x4f94450>, 'image_meta': {'status': u'active', 'name': u'CentOS6.5-x86_64', 'deleted': None, 'container_format': u'bare', 'created_at': datetime.datetime(2015, 12, 29, 7, 50, 46, tzinfo=<iso8601.iso8601.Utc object at 0x4a209d0>), 'disk_format': u'qcow2', 'updated_at': datetime.datetime(2015, 12, 29, 7, 54, 12, tzinfo=<iso8601.iso8601.Utc object at 0x4a209d0>), 'id': u'651a3d42-d699-45b8-92d3-2ca51a9d15df', 'owner': u'e599088c985f42e7948b12f601705cd3', 'min_ram': 0, 'checksum': u'2b5aebaf12562bf4b4608b1657c5109f', 'min_disk': 0, 'is_public': None, 'deleted_at': None, 'properties': {}, 'size': 16108814336}}
2017-01-05 21:03:17.911 24073 DEBUG cinder.volume.flows.manager.create_volume [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Cloning 5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e from image 651a3d42-d699-45b8-92d3-2ca51a9d15df  at location (None, None). _create_from_image /usr/lib/python2.7/site-packages/cinder/volume/flows/manager/create_volume.py:562
2017-01-05 21:03:17.912 24073 DEBUG cinder.volume.drivers.rbd [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] creating volume 'volume-5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e' create_volume /usr/lib/python2.7/site-packages/cinder/volume/drivers/rbd.py:491
2017-01-05 21:03:17.912 24073 DEBUG cinder.volume.drivers.rbd [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] opening connection to ceph cluster (timeout=-1). _connect_to_rados /usr/lib/python2.7/site-packages/cinder/volume/drivers/rbd.py:291
2017-01-05 21:03:18.064 24073 DEBUG cinder.volume.flows.manager.create_volume [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Attempting download of 651a3d42-d699-45b8-92d3-2ca51a9d15df ((None, None)) to volume 5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e. _copy_image_to_volume /usr/lib/python2.7/site-packages/cinder/volume/flows/manager/create_volume.py:484


2017-01-05 21:03:18.118 24073 DEBUG cinder.openstack.common.processutils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Running cmd (subprocess): sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C qemu-img info /var/lib/cinder/conversion/tmpswVzk1 execute /usr/lib/python2.7/site-packages/cinder/openstack/common/processutils.py:158
2017-01-05 21:03:18.247 24073 DEBUG cinder.openstack.common.processutils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Result was 0 execute /usr/lib/python2.7/site-packages/cinder/openstack/common/processutils.py:192


2017-01-05 21:05:18.302 24073 DEBUG cinder.image.image_utils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Image fetch details: dest /var/lib/cinder/conversion/tmpswVzk1, size 15362.00 MB, duration 120.05 sec fetch /usr/lib/python2.7/site-packages/cinder/image/image_utils.py:139
2017-01-05 21:05:18.303 24073 INFO cinder.image.image_utils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Image download 15362.00 MB at 127.96 MB/s


2017-01-05 21:05:18.358 24073 DEBUG cinder.openstack.common.processutils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Running cmd (subprocess): sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C qemu-img info /var/lib/cinder/conversion/tmpswVzk1 execute /usr/lib/python2.7/site-packages/cinder/openstack/common/processutils.py:158
2017-01-05 21:05:18.627 24073 DEBUG cinder.openstack.common.processutils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Result was 0 execute /usr/lib/python2.7/site-packages/cinder/openstack/common/processutils.py:192
2017-01-05 21:05:18.629 24073 DEBUG cinder.image.image_utils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] 651a3d42-d699-45b8-92d3-2ca51a9d15df was qcow2, converting to raw  fetch_to_volume_format /usr/lib/python2.7/site-packages/cinder/image/image_utils.py:277
2017-01-05 21:05:18.630 24073 DEBUG cinder.volume.utils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Not using bps rate limiting on volume copy setup_blkio_cgroup /usr/lib/python2.7/site-packages/cinder/volume/utils.py:215
2017-01-05 21:05:18.631 24073 DEBUG cinder.openstack.common.processutils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Running cmd (subprocess): sudo cinder-rootwrap /etc/cinder/rootwrap.conf qemu-img convert -O raw /var/lib/cinder/conversion/tmpswVzk1 /tmp/tmprSVeNb execute /usr/lib/python2.7/site-packages/cinder/openstack/common/processutils.py:158
2017-01-05 21:05:42.474 24073 DEBUG cinder.openstack.common.processutils [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Result was 1 execute /usr/lib/python2.7/site-packages/cinder/openstack/common/processutils.py:192
2017-01-05 21:05:43.745 24073 ERROR cinder.volume.flows.manager.create_volume [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Failed to copy image 651a3d42-d699-45b8-92d3-2ca51a9d15df to volume: 5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e, error: qemu-img: error while writing sector 2834432: No space left on device


2017-01-05 21:05:43.748 24073 WARNING cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.CreateVolumeFromSpecTask;volume:create' (21320db4-e806-4fd2-828b-aa08340c079c) transitioned into state 'FAILURE'
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager Traceback (most recent call last):
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager   File "/usr/lib/python2.7/site-packages/taskflow/engines/action_engine/executor.py", line 35, in _execute_task
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager     result = task.execute(**arguments)
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager   File "/usr/lib/python2.7/site-packages/cinder/volume/flows/manager/create_volume.py", line 638, in execute
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager     **volume_spec)
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager   File "/usr/lib/python2.7/site-packages/cinder/volume/flows/manager/create_volume.py", line 590, in _create_from_image
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager     image_id, image_location, image_service)
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager   File "/usr/lib/python2.7/site-packages/cinder/volume/flows/manager/create_volume.py", line 492, in _copy_image_to_volume
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager     raise exception.ImageCopyFailure(reason=ex.stderr)
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager ImageCopyFailure: Failed to copy image to volume: qemu-img: error while writing sector 2834432: No space left on device
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager 
2017-01-05 21:05:43.748 24073 TRACE cinder.volume.manager 
2017-01-05 21:05:43.749 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.750 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.750 24073 WARNING cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.CreateVolumeFromSpecTask;volume:create' (21320db4-e806-4fd2-828b-aa08340c079c) transitioned into state 'REVERTING'
2017-01-05 21:05:43.751 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.751 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.752 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.752 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.753 24073 DEBUG cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.CreateVolumeFromSpecTask;volume:create' (21320db4-e806-4fd2-828b-aa08340c079c) transitioned into state 'REVERTED' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-05 21:05:43.753 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.753 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.754 24073 WARNING cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.NotifyVolumeActionTask;volume:create, create.start' (d508d764-d67f-481d-ad94-56edea2324e7) transitioned into state 'REVERTING'
2017-01-05 21:05:43.754 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.755 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.755 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.755 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.756 24073 DEBUG cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.NotifyVolumeActionTask;volume:create, create.start' (d508d764-d67f-481d-ad94-56edea2324e7) transitioned into state 'REVERTED' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-05 21:05:43.756 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.757 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.757 24073 WARNING cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.ExtractVolumeSpecTask;volume:create' (ec1c5511-0e8a-4c9f-932b-2853ad15e515) transitioned into state 'REVERTING'
2017-01-05 21:05:43.758 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.758 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.759 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.759 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.760 24073 DEBUG cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.ExtractVolumeSpecTask;volume:create' (ec1c5511-0e8a-4c9f-932b-2853ad15e515) transitioned into state 'REVERTED' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-05 21:05:43.760 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.760 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.761 24073 WARNING cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.OnFailureRescheduleTask;volume:create' (04ed0df5-4428-439b-bb7c-fc4b4970dd89) transitioned into state 'REVERTING'
2017-01-05 21:05:43.762 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.762 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.762 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.763 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.763 24073 DEBUG cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.OnFailureRescheduleTask;volume:create' (04ed0df5-4428-439b-bb7c-fc4b4970dd89) transitioned into state 'REVERTED' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-05 21:05:43.764 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'ANALYZING' in response to event 'schedule' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.764 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'SCHEDULING' in response to event 'schedule' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.764 24073 WARNING cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.ExtractVolumeRefTask;volume:create' (40528ab4-b812-4a9f-8d22-5bbed777070c) transitioned into state 'REVERTING'
2017-01-05 21:05:43.765 24073 DEBUG cinder.volume.flows.common [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Updating volume: 5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e with {'status': 'error'} due to: ??? error_out_volume /usr/lib/python2.7/site-packages/cinder/volume/flows/common.py:88
2017-01-05 21:05:43.806 24073 ERROR cinder.volume.flows.manager.create_volume [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Volume 5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e: create failed
2017-01-05 21:05:43.806 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'SCHEDULING' in response to event 'wait' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.807 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'WAITING' in response to event 'wait' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.807 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'WAITING' in response to event 'analyze' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.807 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'ANALYZING' in response to event 'analyze' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.808 24073 DEBUG cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Task 'cinder.volume.flows.manager.create_volume.ExtractVolumeRefTask;volume:create' (40528ab4-b812-4a9f-8d22-5bbed777070c) transitioned into state 'REVERTED' _task_receiver /usr/lib/python2.7/site-packages/cinder/flow_utils.py:131
2017-01-05 21:05:43.808 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'ANALYZING' in response to event 'finished' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.809 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'GAME_OVER' in response to event 'finished' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.809 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exiting old state 'GAME_OVER' in response to event 'reverted' on_exit /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:156
2017-01-05 21:05:43.810 24073 DEBUG taskflow.engines.action_engine.runner [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Entering new state 'REVERTED' in response to event 'reverted' on_enter /usr/lib/python2.7/site-packages/taskflow/engines/action_engine/runner.py:160
2017-01-05 21:05:43.810 24073 WARNING cinder.volume.manager [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Flow 'volume_create_manager' (4f0cfeaf-d9e0-4172-be5d-5b7c8b7f1aef) transitioned into state 'REVERTED' from state 'RUNNING'
2017-01-05 21:05:43.811 24073 ERROR oslo.messaging.rpc.dispatcher [req-e5bcdac5-0487-409d-b946-09cd3f759986 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Exception during message handling: Failed to copy image to volume: qemu-img: error while writing sector 2834432: No space left on device
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py", line 134, in _dispatch_and_reply
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py", line 177, in _dispatch
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py", line 123, in _do_dispatch
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/osprofiler/profiler.py", line 105, in wrapper
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     return f(*args, **kwargs)
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/cinder/volume/manager.py", line 381, in create_volume
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     _run_flow()
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/cinder/volume/manager.py", line 374, in _run_flow
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     flow_engine.run()
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/taskflow/engines/action_engine/engine.py", line 99, in run
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     for _state in self.run_iter():
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/taskflow/engines/action_engine/engine.py", line 156, in run_iter
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     misc.Failure.reraise_if_any(failures.values())
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/taskflow/utils/misc.py", line 733, in reraise_if_any
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     failures[0].reraise()
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/taskflow/utils/misc.py", line 740, in reraise
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     six.reraise(*self._exc_info)
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/taskflow/engines/action_engine/executor.py", line 35, in _execute_task
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     result = task.execute(**arguments)
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/cinder/volume/flows/manager/create_volume.py", line 638, in execute
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     **volume_spec)
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/cinder/volume/flows/manager/create_volume.py", line 590, in _create_from_image
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     image_id, image_location, image_service)
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher   File "/usr/lib/python2.7/site-packages/cinder/volume/flows/manager/create_volume.py", line 492, in _copy_image_to_volume
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher     raise exception.ImageCopyFailure(reason=ex.stderr)
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher ImageCopyFailure: Failed to copy image to volume: qemu-img: error while writing sector 2834432: No space left on device
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher 
2017-01-05 21:05:43.811 24073 TRACE oslo.messaging.rpc.dispatcher 
2017-01-05 21:05:45.141 24073 DEBUG cinder.openstack.common.lockutils [req-95ec92c4-019a-4bfe-b8d0-61c8fec67931 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Got semaphore "5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e-delete_volume" for method "lvo_inner2"... inner /usr/lib/python2.7/site-packages/cinder/openstack/common/lockutils.py:191
2017-01-05 21:05:45.142 24073 DEBUG cinder.openstack.common.lockutils [req-95ec92c4-019a-4bfe-b8d0-61c8fec67931 7d5b5abc30ea463690567e5f8cc794f9 e599088c985f42e7948b12f601705cd3 - - -] Attempting to grab file lock "5e1d0838-0797-4a09-9ea0-3ff3f2f14c0e-delete_volume" for method "lvo_inner2"... inner /usr/lib/python2.7/site-packages/cinder/openstack/common/lockutils.py:202


########################################################################################################################
查看09的日志发现
VolumeNotCreated: Volume b6c79c02-6c12-4bf1-a3bd-fa285aa007d1 did not finish being created even after we waited 192 seconds or 61 attempts.
虚拟机的 volume 没有在限定时间内准备好  
这个可以配置的，可以看到是/usr/lib/python2.7/site-packages/nova/compute/manager.py文件的_await_block_device_map_created方法
这个方法中可以看到两个CONF的配置
一个是block_device_allocate_retries     重试总次数
# Number of times to retry block device allocation on failures
# (integer value)
block_device_allocate_retries=600

一个是 block_device_allocate_retries_interval 重试间隔
# Waiting time interval (seconds) between block device
# allocation retries on failures (integer value)
#block_device_allocate_retries_interval=3

def _await_block_device_map_created(self, context, vol_id):
    # TODO(yamahata): creating volume simultaneously
    #                 reduces creation time?
    # TODO(yamahata): eliminate dumb polling
    start = time.time()
    retries = CONF.block_device_allocate_retries
    if retries < 0:
        LOG.warn(_LW("Treating negative config value (%(retries)s) for "
                     "'block_device_retries' as 0."),
                 {'retries': retries})
    # (1) treat  negative config value as 0
                 {'retries': retries})
    # (1) treat  negative config value as 0
    # (2) the configured value is 0, one attempt should be made
    # (3) the configured value is > 0, then the total number attempts
    #      is (retries + 1)
    attempts = 1
    if retries >= 1:
        attempts = retries + 1
    for attempt in range(1, attempts + 1):
        volume = self.volume_api.get(context, vol_id)
        volume_status = volume['status']
        if volume_status not in ['creating', 'downloading']:
            if volume_status != 'available':
                LOG.warn(_("Volume id: %s finished being created but was"
                           " not set as 'available'"), vol_id)
            return attempt
        greenthread.sleep(CONF.block_device_allocate_retries_interval)
    # NOTE(harlowja): Should only happen if we ran out of attempts
    raise exception.VolumeNotCreated(volume_id=vol_id,
                                     seconds=int(time.time() - start),
                                     attempts=attempts)



2017-01-06 11:00:20.300 20180 AUDIT nova.compute.manager [req-86d8e572-2d72-4261-a6ee-2c9e6ba0cbf6 None] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Starting instance...
2017-01-06 11:00:20.385 20180 AUDIT nova.compute.claims [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Attempting claim: memory 8192 MB, disk 30 GB
2017-01-06 11:00:20.385 20180 AUDIT nova.compute.claims [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Total memory: 193424 MB, used: 107648.00 MB
2017-01-06 11:00:20.385 20180 AUDIT nova.compute.claims [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] memory limit: 290136.00 MB, free: 182488.00 MB
2017-01-06 11:00:20.386 20180 AUDIT nova.compute.claims [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Total disk: 547 GB, used: 242.00 GB
2017-01-06 11:00:20.386 20180 AUDIT nova.compute.claims [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] disk limit not specified, defaulting to unlimited
2017-01-06 11:00:20.400 20180 AUDIT nova.compute.claims [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Claim successful
2017-01-06 11:00:20.519 20180 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute09', 'sjhl-o-compute09')
2017-01-06 11:00:20.636 20180 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute09', 'sjhl-o-compute09')
2017-01-06 11:00:20.908 20180 AUDIT nova.virt.block_device [req-86d8e572-2d72-4261-a6ee-2c9e6ba0cbf6 None] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Booting with volume None at /dev/vda
2017-01-06 11:00:21.149 20180 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute09', 'sjhl-o-compute09')
2017-01-06 11:00:25.096 20180 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 115840
2017-01-06 11:00:25.097 20180 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 275
2017-01-06 11:00:25.097 20180 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 54
2017-01-06 11:00:25.097 20180 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2017-01-06 11:00:25.119 20180 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute09', 'sjhl-o-compute09')
2017-01-06 11:00:25.119 20180 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute09:sjhl-o-compute09
2017-01-06 11:01:14.066 20180 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2017-01-06 11:01:25.055 20180 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 115840
2017-01-06 11:01:25.055 20180 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 275
2017-01-06 11:01:25.055 20180 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 54
2017-01-06 11:01:25.055 20180 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2017-01-06 11:01:25.056 20180 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute09:sjhl-o-compute09
2017-01-06 11:02:15.098 20180 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2017-01-06 11:02:26.160 20180 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 115840
2017-01-06 11:02:26.161 20180 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 275
2017-01-06 11:02:26.161 20180 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 54
2017-01-06 11:02:26.161 20180 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2017-01-06 11:02:26.161 20180 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute09:sjhl-o-compute09
2017-01-06 11:03:16.065 20180 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2017-01-06 11:03:27.134 20180 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 115840
2017-01-06 11:03:27.134 20180 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 275
2017-01-06 11:03:27.134 20180 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 54
2017-01-06 11:03:27.135 20180 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2017-01-06 11:03:27.135 20180 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute09:sjhl-o-compute09
2017-01-06 11:03:33.936 20180 ERROR nova.compute.manager [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Instance failed block device setup
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Traceback (most recent call last):
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1822, in _prep_block_device
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     do_check_attach=do_check_attach) +
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/virt/block_device.py", line 407, in attach_block_devices
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     map(_log_and_attach, block_device_mapping)
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/virt/block_device.py", line 405, in _log_and_attach
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     bdm.attach(*attach_args, **attach_kwargs)
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/virt/block_device.py", line 333, in attach
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     wait_func(context, vol['id'])
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1266, in _await_block_device_map_created
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     attempts=attempts)
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d] VolumeNotCreated: Volume b6c79c02-6c12-4bf1-a3bd-fa285aa007d1 did not finish being created even after we waited 192 seconds or 61 attempts.
2017-01-06 11:03:33.936 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d] 
2017-01-06 11:03:33.937 20180 ERROR nova.compute.manager [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Failure prepping block device
2017-01-06 11:03:33.937 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Traceback (most recent call last):
2017-01-06 11:03:33.937 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2221, in _build_resources
2017-01-06 11:03:33.937 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     block_device_mapping)
2017-01-06 11:03:33.937 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1850, in _prep_block_device
2017-01-06 11:03:33.937 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     raise exception.InvalidBDM()
2017-01-06 11:03:33.937 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d] InvalidBDM: Block Device Mapping is Invalid.
2017-01-06 11:03:33.937 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d] 
2017-01-06 11:03:33.973 20180 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute09', 'sjhl-o-compute09')
2017-01-06 11:03:33.975 20180 ERROR nova.compute.manager [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Build of instance e105cc80-50e6-42d1-a195-efde652cd27d aborted: Failure prepping block device.
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Traceback (most recent call last):
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2033, in _do_build_and_run_instance
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     filter_properties)
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2132, in _build_and_run_instance
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     'create.error', fault=e)
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     six.reraise(self.type_, self.value, self.tb)
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2105, in _build_and_run_instance
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     block_device_mapping) as resources:
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib64/python2.7/contextlib.py", line 17, in __enter__
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     return self.gen.next()
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2243, in _build_resources
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d]     reason=msg)
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d] BuildAbortException: Build of instance e105cc80-50e6-42d1-a195-efde652cd27d aborted: Failure prepping block device.
2017-01-06 11:03:33.975 20180 TRACE nova.compute.manager [instance: e105cc80-50e6-42d1-a195-efde652cd27d] 
2017-01-06 11:03:34.024 20180 INFO nova.network.neutronv2.api [-] [instance: e105cc80-50e6-42d1-a195-efde652cd27d] Unable to reset device ID for port None
########################################################################################################################
放在ceph中的instance 的xml文件中磁盘是network类型的
    <disk type="network" device="disk">
      <driver name="qemu" type="raw" cache="none"/>
      <source protocol="rbd" name="volumes/volume-8740b14b-6ed7-4a2b-b441-683606cef933">
        <host name="10.20.8.31" port="6789"/>
        <host name="10.20.8.32" port="6789"/>
        <host name="10.20.8.33" port="6789"/>
      </source>
      <auth username="cinder">
        <secret type="ceph" uuid="3c97b1d3-1153-4454-8ffb-0fbf9ff01311"/>
      </auth>
      <target bus="virtio" dev="vda"/>
      <serial>8740b14b-6ed7-4a2b-b441-683606cef933</serial>
    </disk>
	
而放在本机的磁盘类型是file
    <disk type="file" device="disk">
      <driver name="qemu" type="qcow2" cache="none"/>
      <source file="/var/lib/nova/instances/28b3e84c-2477-432a-93be-4fe7fe43e047/disk"/>
      <target bus="virtio" dev="vda"/>
    </disk>


然后针对 放在ceph中的机器和放在计算节点本地机器磁盘性能的测试

可以看到dd顺序读写的区别还是挺大的
ceph-local  是本地的 192.168.33.51   boot from image 
ceph-share  是ceph的 192.168.33.44   create a new  volume 


[root@ceph-local opt]#  sync;/usr/bin/time -p bash -c "(dd if=/dev/zero of=/opt/test.dd  bs=1M count=3000;sync)"
3000+0 records in
3000+0 records out
3145728000 bytes (3.1 GB) copied, 15.0479 s, 209 MB/s
real 25.33
user 0.00
sys 5.23



[root@ceph-share opt]#  sync;/usr/bin/time -p bash -c "(dd if=/dev/zero of=/opt/test.dd  bs=1M count=3000;sync)"
3000+0 records in
3000+0 records out
3145728000 bytes (3.1 GB) copied, 17.8667 s, 176 MB/s
real 27.72
user 0.00
sys 2.66



[root@ceph-local ~]# hdparm -t /dev/vda

/dev/vda:
 Timing buffered disk reads: 846 MB in  3.00 seconds = 281.58 MB/sec

 
[root@ceph-share ~]# hdparm -t /dev/vda

/dev/vda:
 Timing buffered disk reads: 564 MB in  3.05 seconds = 185.14 MB/sec 
 
 
[root@ceph-local ~]#  hdparm -T /dev/vda

/dev/vda:
 Timing cached reads:   16868 MB in  2.00 seconds = 8442.88 MB/sec

[root@ceph-share ~]#  hdparm -T /dev/vda

/dev/vda:
 Timing cached reads:   17774 MB in  2.00 seconds = 8896.92 MB/sec 
########################################################################################################################
下面进行热迁移实验
dzc-elk-ceph01  在 sjhl-o-compute09上  热迁移至07上








2017-01-06 15:12:02.877 21335 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] VM Paused (Lifecycle Event)
2017-01-06 15:12:03.018 21335 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] During sync_power_state the instance has a pending task (migrating). Skip.
2017-01-06 15:12:03.339 21335 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] VM Stopped (Lifecycle Event)
2017-01-06 15:12:03.343 21335 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] _post_live_migration() is started..
2017-01-06 15:12:03.478 21335 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] During sync_power_state the instance has a pending task (migrating). Skip.
2017-01-06 15:12:04.545 21335 INFO nova.virt.libvirt.driver [req-60c75966-a0c1-403a-840c-038309a7cc62 None] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] Deleting instance files /var/lib/nova/instances/7ff81f1d-7562-4692-a159-5459fa05ce04_del
2017-01-06 15:12:04.546 21335 INFO nova.virt.libvirt.driver [req-60c75966-a0c1-403a-840c-038309a7cc62 None] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] Deletion of /var/lib/nova/instances/7ff81f1d-7562-4692-a159-5459fa05ce04_del complete
2017-01-06 15:12:04.623 21335 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2017-01-06 15:12:05.579 21335 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 107136
2017-01-06 15:12:05.580 21335 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 306
2017-01-06 15:12:05.580 21335 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 53
2017-01-06 15:12:05.580 21335 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2017-01-06 15:12:05.612 21335 INFO nova.scheduler.client.report [-] Compute_service record updated for ('sjhl-o-compute09', 'sjhl-o-compute09')
2017-01-06 15:12:05.613 21335 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute09:sjhl-o-compute09
2017-01-06 15:12:05.684 21335 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] Migrating instance to sjhl-o-compute07 finished successfully.
2017-01-06 15:12:05.684 21335 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] You may see the error "libvirt: QEMU error: Domain not found: no domain with matching name." This error can be safely ignored.
2017-01-06 15:12:08.187 21335 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources
2017-01-06 15:12:09.172 21335 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 193424, total allocated virtual ram (MB): 107136
2017-01-06 15:12:09.173 21335 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 306
2017-01-06 15:12:09.173 21335 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 24, total allocated vcpus: 53
2017-01-06 15:12:09.173 21335 AUDIT nova.compute.resource_tracker [-] PCI stats: []
2017-01-06 15:12:09.174 21335 INFO nova.compute.resource_tracker [-] Compute_service record updated for sjhl-o-compute09:sjhl-o-compute09







2017-01-06 15:12:02.933 13992 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483686722.93, 7ff81f1d-7562-4692-a159-5459fa05ce04 => Resumed> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2017-01-06 15:12:02.933 13992 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] VM Resumed (Lifecycle Event)
2017-01-06 15:12:02.993 13992 DEBUG nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] Synchronizing instance power state after lifecycle event "Resumed"; current vm_state: active, current task_state: migrating, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2017-01-06 15:12:03.061 13992 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] During the sync_power process the instance has moved from host sjhl-o-compute09 to host sjhl-o-compute07
2017-01-06 15:12:03.061 13992 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1483686722.93, 7ff81f1d-7562-4692-a159-5459fa05ce04 => Resumed> emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1298
2017-01-06 15:12:03.061 13992 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] VM Resumed (Lifecycle Event)
2017-01-06 15:12:03.128 13992 DEBUG nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] Synchronizing instance power state after lifecycle event "Resumed"; current vm_state: active, current task_state: migrating, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1108
2017-01-06 15:12:03.211 13992 INFO nova.compute.manager [-] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] During the sync_power process the instance has moved from host sjhl-o-compute09 to host sjhl-o-compute07
2017-01-06 15:12:04.422 13992 INFO nova.compute.manager [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] Post operation of migration started
2017-01-06 15:12:04.422 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/extensions.json -X GET -H "X-Auth-Token: 7998f338c94b468e9ab886df023ec864" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2017-01-06 15:12:04.431 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] RESP:200 {'date': 'Fri, 06 Jan 2017 07:12:04 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '4111', 'x-openstack-request-id': 'req-2b69fe6c-a99e-4ed6-9013-42476cddf80f'} {"extensions": [{"updated": "2012-10-05T10:00:00-00:00", "name": "security-group", "links": [], "namespace": "http://docs.openstack.org/ext/securitygroups/api/v2.0", "alias": "security-group", "description": "The security groups extension."}, {"updated": "2013-02-07T10:00:00-00:00", "name": "L3 Agent Scheduler", "links": [], "namespace": "http://docs.openstack.org/ext/l3_agent_scheduler/api/v1.0", "alias": "l3_agent_scheduler", "description": "Schedule routers among l3 agents"}, {"updated": "2013-03-28T10:00:00-00:00", "name": "Neutron L3 Configurable external gateway mode", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/ext-gw-mode/api/v1.0", "alias": "ext-gw-mode", "description": "Extension of the router abstraction for specifying whether SNAT should occur on the external gateway"}, {"updated": "2014-02-03T10:00:00-00:00", "name": "Port Binding", "links": [], "namespace": "http://docs.openstack.org/ext/binding/api/v1.0", "alias": "binding", "description": "Expose port bindings of a virtual port to external application"}, {"updated": "2012-09-07T10:00:00-00:00", "name": "Provider Network", "links": [], "namespace": "http://docs.openstack.org/ext/provider/api/v1.0", "alias": "provider", "description": "Expose mapping of virtual networks to physical networks"}, {"updated": "2013-02-03T10:00:00-00:00", "name": "agent", "links": [], "namespace": "http://docs.openstack.org/ext/agent/api/v2.0", "alias": "agent", "description": "The agent management extension."}, {"updated": "2012-07-29T10:00:00-00:00", "name": "Quota management support", "links": [], "namespace": "http://docs.openstack.org/network/ext/quotas-sets/api/v2.0", "alias": "quotas", "description": "Expose functions for quotas management per tenant"}, {"updated": "2013-02-07T10:00:00-00:00", "name": "DHCP Agent Scheduler", "links": [], "namespace": "http://docs.openstack.org/ext/dhcp_agent_scheduler/api/v1.0", "alias": "dhcp_agent_scheduler", "description": "Schedule networks among dhcp agents"}, {"updated": "2014-04-26T00:00:00-00:00", "name": "HA Router extension", "links": [], "namespace": "", "alias": "l3-ha", "description": "Add HA capability to routers."}, {"updated": "2013-06-27T10:00:00-00:00", "name": "Multi Provider Network", "links": [], "namespace": "http://docs.openstack.org/ext/multi-provider/api/v1.0", "alias": "multi-provider", "description": "Expose mapping of virtual networks to multiple physical networks"}, {"updated": "2013-01-14T10:00:00-00:00", "name": "Neutron external network", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/external_net/api/v1.0", "alias": "external-net", "description": "Adds external network attribute to network resource."}, {"updated": "2012-07-20T10:00:00-00:00", "name": "Neutron L3 Router", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/router/api/v1.0", "alias": "router", "description": "Router abstraction for basic L3 forwarding between L2 Neutron networks and access to external networks via a NAT gateway."}, {"updated": "2013-07-23T10:00:00-00:00", "name": "Allowed Address Pairs", "links": [], "namespace": "http://docs.openstack.org/ext/allowedaddresspairs/api/v2.0", "alias": "allowed-address-pairs", "description": "Provides allowed address pairs"}, {"updated": "2013-02-01T10:00:00-00:00", "name": "Neutron Extra Route", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/extraroutes/api/v1.0", "alias": "extraroute", "description": "Extra routes configuration for L3 router"}, {"updated": "2013-03-17T12:00:00-00:00", "name": "Neutron Extra DHCP opts", "links": [], "namespace": "http://docs.openstack.org/ext/neutron/extra_dhcp_opt/api/v1.0", "alias": "extra_dhcp_opt", "description": "Extra options configuration for DHCP. For example PXE boot options to DHCP clients can be specified (e.g. tftp-server, server-ip-address, bootfile-name)"}, {"updated": "2014-06-1T10:00:00-00:00", "name": "Distributed Virtual Router", "links": [], "namespace": "http://docs.openstack.org/ext/dvr/api/v1.0", "alias": "dvr", "description": "Enables configuration of Distributed Virtual Routers."}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2017-01-06 15:12:04.431 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.432 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.432 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.432 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.432 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.432 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.433 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.433 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.433 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.433 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.434 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.434 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.434 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=e599088c985f42e7948b12f601705cd3&device_id=7ff81f1d-7562-4692-a159-5459fa05ce04 -X GET -H "X-Auth-Token: 3a720c5e86d84e92b1c1654c1fff133c" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2017-01-06 15:12:04.462 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] RESP:200 {'date': 'Fri, 06 Jan 2017 07:12:04 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-57f408f5-0e79-497c-9d38-9948c1b8a8fe'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute09", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "e347242a-4fdb-40d8-901d-77987644ecca", "ip_address": "192.168.33.43"}], "id": "90434480-21dc-436e-8d5c-a353948c362a", "security_groups": ["b20cac25-ec58-4830-b7f3-888a54ce1391"], "device_id": "7ff81f1d-7562-4692-a159-5459fa05ce04", "name": "", "admin_state_up": true, "network_id": "ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:1e:da:eb"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2017-01-06 15:12:04.463 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.463 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.463 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.463 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.464 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.464 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.464 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.464 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.465 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.465 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.465 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.465 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.465 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.466 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.466 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.466 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.466 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.466 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.467 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.467 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.467 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.467 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.468 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.468 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.468 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.468 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.468 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.469 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.469 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.469 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.469 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.470 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.470 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.470 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.470 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.470 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.471 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.471 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.471 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.471 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports/90434480-21dc-436e-8d5c-a353948c362a.json -X PUT -H "X-Auth-Token: 3a720c5e86d84e92b1c1654c1fff133c" -H "User-Agent: python-neutronclient" -d '{"port": {"binding:host_id": "sjhl-o-compute07"}}'
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2017-01-06 15:12:04.626 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] RESP:200 {'date': 'Fri, 06 Jan 2017 07:12:04 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '739', 'x-openstack-request-id': 'req-f544e1c4-8fa7-491b-8c7b-a91b10d0b862'} {"port": {"status": "BUILD", "binding:host_id": "sjhl-o-compute07", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "e347242a-4fdb-40d8-901d-77987644ecca", "ip_address": "192.168.33.43"}], "id": "90434480-21dc-436e-8d5c-a353948c362a", "security_groups": ["b20cac25-ec58-4830-b7f3-888a54ce1391"], "device_id": "7ff81f1d-7562-4692-a159-5459fa05ce04", "name": "", "admin_state_up": true, "network_id": "ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:1e:da:eb"}}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2017-01-06 15:12:04.627 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.627 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.627 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.627 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.627 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.628 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.628 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.628 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.628 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.629 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.629 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.629 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.629 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.630 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.630 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.630 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.630 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.630 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.631 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.631 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.631 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.631 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "refresh_cache-7ff81f1d-7562-4692-a159-5459fa05ce04" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.631 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "refresh_cache-7ff81f1d-7562-4692-a159-5459fa05ce04" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.632 13992 DEBUG nova.network.neutronv2.api [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:610
2017-01-06 15:12:04.632 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.632 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.632 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.633 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.633 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.633 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.633 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.633 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.634 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.634 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Created new semaphore "neutron_admin_auth_token_lock" internal_lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:206
2017-01-06 15:12:04.634 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Acquired semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:229
2017-01-06 15:12:04.634 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "neutron_admin_auth_token_lock" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.635 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?tenant_id=e599088c985f42e7948b12f601705cd3&device_id=7ff81f1d-7562-4692-a159-5459fa05ce04 -X GET -H "X-Auth-Token: 3a720c5e86d84e92b1c1654c1fff133c" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2017-01-06 15:12:04.662 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] RESP:200 {'date': 'Fri, 06 Jan 2017 07:12:04 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '742', 'x-openstack-request-id': 'req-96ed30d1-fcc7-4d04-aba5-51686d296c09'} {"ports": [{"status": "BUILD", "binding:host_id": "sjhl-o-compute07", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "compute:dzc", "binding:profile": {}, "fixed_ips": [{"subnet_id": "e347242a-4fdb-40d8-901d-77987644ecca", "ip_address": "192.168.33.43"}], "id": "90434480-21dc-436e-8d5c-a353948c362a", "security_groups": ["b20cac25-ec58-4830-b7f3-888a54ce1391"], "device_id": "7ff81f1d-7562-4692-a159-5459fa05ce04", "name": "", "admin_state_up": true, "network_id": "ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:1e:da:eb"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2017-01-06 15:12:04.720 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/subnets.json?id=e347242a-4fdb-40d8-901d-77987644ecca -X GET -H "X-Auth-Token: 7998f338c94b468e9ab886df023ec864" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2017-01-06 15:12:04.748 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] RESP:200 {'date': 'Fri, 06 Jan 2017 07:12:04 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '522', 'x-openstack-request-id': 'req-358d260d-d5e4-4402-bb23-b1a4aebf1965'} {"subnets": [{"name": "INSIDE_SUBNET", "enable_dhcp": true, "network_id": "ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "tenant_id": "e599088c985f42e7948b12f601705cd3", "dns_nameservers": ["218.30.110.17", "8.8.8.8"], "gateway_ip": null, "ipv6_ra_mode": null, "allocation_pools": [{"start": "192.168.33.3", "end": "192.168.33.250"}], "host_routes": [{"nexthop": "192.168.33.254", "destination": "0.0.0.0/0"}], "ip_version": 4, "ipv6_address_mode": null, "cidr": "192.168.33.0/24", "id": "e347242a-4fdb-40d8-901d-77987644ecca"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2017-01-06 15:12:04.749 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] 
REQ: curl -i http://controller.light.fang.com:9696/v2.0/ports.json?network_id=ba31704e-c1b6-4a43-8eb6-6162d25e7b29&device_owner=network%3Adhcp -X GET -H "X-Auth-Token: 7998f338c94b468e9ab886df023ec864" -H "User-Agent: python-neutronclient"
 http_log_req /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:140
2017-01-06 15:12:04.787 13992 DEBUG neutronclient.client [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] RESP:200 {'date': 'Fri, 06 Jan 2017 07:12:04 GMT', 'connection': 'keep-alive', 'content-type': 'application/json; charset=UTF-8', 'content-length': '746', 'x-openstack-request-id': 'req-9abd5e1c-d3eb-4fa8-9d17-3b994e8983b2'} {"ports": [{"status": "ACTIVE", "binding:host_id": "dzc-o-neutron01v", "allowed_address_pairs": [], "extra_dhcp_opts": [], "device_owner": "network:dhcp", "binding:profile": {}, "fixed_ips": [{"subnet_id": "e347242a-4fdb-40d8-901d-77987644ecca", "ip_address": "192.168.33.4"}], "id": "905ede5c-4d14-4653-a78a-d0c11a3c0e1b", "security_groups": [], "device_id": "dhcp95fc7688-e5d9-5615-b6e9-fc077300258a-ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "name": "", "admin_state_up": true, "network_id": "ba31704e-c1b6-4a43-8eb6-6162d25e7b29", "tenant_id": "e599088c985f42e7948b12f601705cd3", "binding:vif_details": {"port_filter": true, "ovs_hybrid_plug": true}, "binding:vnic_type": "normal", "binding:vif_type": "ovs", "mac_address": "fa:16:3e:1d:e6:ac"}]}
 http_log_resp /usr/lib/python2.7/site-packages/neutronclient/common/utils.py:149
2017-01-06 15:12:04.788 13992 DEBUG nova.network.base_api [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Updating cache with info: [VIF({'profile': {}, 'ovs_interfaceid': u'90434480-21dc-436e-8d5c-a353948c362a', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.33.43'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.33.4'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.33.254'})})], 'cidr': u'192.168.33.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'ba31704e-c1b6-4a43-8eb6-6162d25e7b29', 'label': u'INSIDE_NET'}), 'devname': u'tap90434480-21', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:1e:da:eb', 'active': False, 'type': u'ovs', 'id': u'90434480-21dc-436e-8d5c-a353948c362a', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:40
2017-01-06 15:12:04.808 13992 DEBUG nova.openstack.common.lockutils [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Releasing semaphore "refresh_cache-7ff81f1d-7562-4692-a159-5459fa05ce04" lock /usr/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:238
2017-01-06 15:12:04.831 13992 DEBUG glanceclient.common.http [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] curl -i -X HEAD -H 'X-Service-Catalog: [{"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "id": "37eb9ae73b72467b8695247873083335", "internalURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3"}], "type": "volumev2", "name": "cinderv2"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "region": "regionOne", "id": "30d052b7f62344dabab8c29928d29f99", "serviceName": "cinder", "internalURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "publicURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3"}], "type": "volume", "name": "cinder"}]' -H 'X-Auth-Token: {SHA1}1426169249e0ed74af1b24c91da1ce09ffd44b7e' -H 'Accept-Encoding: gzip, deflate' -H 'Connection: keep-alive' -H 'Accept: */*' -H 'X-Roles: admin' -H 'User-Agent: python-glanceclient' -H 'X-Tenant-Id: e599088c985f42e7948b12f601705cd3' -H 'X-User-Id: 7d5b5abc30ea463690567e5f8cc794f9' -H 'X-Identity-Status: Confirmed' -H 'Content-Type: application/octet-stream' http://controller.light.fang.com:9292/v1/images/ log_curl_request /usr/lib/python2.7/site-packages/glanceclient/common/http.py:122
2017-01-06 15:12:04.839 13992 DEBUG glanceclient.common.http [req-a3103d74-9608-4edd-99a4-bf21f815a055 ] Request returned failure status 404. _request /usr/lib/python2.7/site-packages/glanceclient/common/http.py:226
2017-01-06 15:12:04.840 13992 WARNING nova.compute.utils [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] Can't access image : Image  could not be found.
2017-01-06 15:12:04.840 13992 DEBUG nova.virt.libvirt.driver [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] Start _get_guest_xml network_info=[VIF({'profile': {}, 'ovs_interfaceid': u'90434480-21dc-436e-8d5c-a353948c362a', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:1e:da:eb', 'floating_ips': [], 'label': u'INSIDE_NET', 'meta': {}, 'address': u'192.168.33.43', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.33.4'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.33.254'})})], 'cidr': u'192.168.33.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'ba31704e-c1b6-4a43-8eb6-6162d25e7b29', 'label': u'INSIDE_NET'}), 'devname': u'tap90434480-21', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:1e:da:eb', 'active': False, 'type': u'ovs', 'id': u'90434480-21dc-436e-8d5c-a353948c362a', 'qbg_params': None})] disk_info={'disk_bus': 'virtio', 'cdrom_bus': 'ide', 'mapping': {u'/dev/vda': {'bus': u'virtio', 'boot_index': '1', 'type': u'disk', 'dev': u'vda'}, 'root': {'bus': u'virtio', 'boot_index': '1', 'type': u'disk', 'dev': u'vda'}}} image_meta={u'min_disk': u'30', u'container_format': u'bare', u'min_ram': u'0', u'disk_format': u'qcow2', 'properties': {u'instance_type_memory_mb': u'8192', u'instance_type_swap': u'0', u'instance_type_root_gb': u'30', u'instance_type_name': u'fang-kibanna', u'instance_type_id': u'21', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'instance_type_flavorid': u'ab132230-f7ae-4a1b-a3cf-cb9674fd004b', u'instance_type_vcpus': u'8', u'base_image_ref': u''}} rescue=None block_device_info={'swap': None, 'ephemerals': [], 'block_device_mapping': [{'guest_format': None, 'boot_index': 0, 'mount_device': u'/dev/vda', 'connection_info': {u'driver_volume_type': u'rbd', u'serial': u'8740b14b-6ed7-4a2b-b441-683606cef933', u'data': {u'secret_type': u'ceph', u'name': u'volumes/volume-8740b14b-6ed7-4a2b-b441-683606cef933', u'secret_uuid': u'3c97b1d3-1153-4454-8ffb-0fbf9ff01311', u'qos_specs': None, u'hosts': [u'10.20.8.31', u'10.20.8.32', u'10.20.8.33'], u'auth_enabled': True, u'access_mode': u'rw', u'auth_username': u'cinder', u'ports': [u'6789', u'6789', u'6789']}}, 'disk_bus': u'virtio', 'device_type': u'disk', 'delete_on_termination': True}]} _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4156
2017-01-06 15:12:04.864 13992 DEBUG nova.objects.instance [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Lazy-loading `numa_topology' on Instance uuid 7ff81f1d-7562-4692-a159-5459fa05ce04 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2017-01-06 15:12:04.875 13992 DEBUG nova.virt.libvirt.driver [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] CPU mode 'host-model' model '' was chosen _get_guest_cpu_model_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:3362
2017-01-06 15:12:04.876 13992 DEBUG nova.virt.hardware [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Getting desirable topologies for flavor Flavor(created_at=2016-08-25T08:41:19Z,deleted=False,deleted_at=None,disabled=False,ephemeral_gb=0,extra_specs={},flavorid='ab132230-f7ae-4a1b-a3cf-cb9674fd004b',id=21,is_public=True,memory_mb=8192,name='fang-kibanna',projects=<?>,root_gb=30,rxtx_factor=1.0,swap=0,updated_at=None,vcpu_weight=0,vcpus=8) and image_meta {u'min_disk': u'30', u'container_format': u'bare', u'min_ram': u'0', u'disk_format': u'qcow2', 'properties': {u'instance_type_memory_mb': u'8192', u'instance_type_swap': u'0', u'instance_type_root_gb': u'30', u'instance_type_name': u'fang-kibanna', u'instance_type_id': u'21', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'instance_type_flavorid': u'ab132230-f7ae-4a1b-a3cf-cb9674fd004b', u'instance_type_vcpus': u'8', u'base_image_ref': u''}} get_desirable_configs /usr/lib/python2.7/site-packages/nova/virt/hardware.py:502
2017-01-06 15:12:04.876 13992 DEBUG nova.virt.hardware [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Flavor limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:296
2017-01-06 15:12:04.876 13992 DEBUG nova.virt.hardware [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Image limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:309
2017-01-06 15:12:04.877 13992 DEBUG nova.virt.hardware [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Flavor pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:332
2017-01-06 15:12:04.877 13992 DEBUG nova.virt.hardware [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Image pref -1:-1:-1 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:354
2017-01-06 15:12:04.877 13992 DEBUG nova.virt.hardware [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Chosen -1:-1:-1 limits 65536:65536:65536 get_topology_constraints /usr/lib/python2.7/site-packages/nova/virt/hardware.py:383
2017-01-06 15:12:04.877 13992 DEBUG nova.virt.hardware [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Build topologies for 8 vcpu(s) 8:8:8 get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:419
2017-01-06 15:12:04.878 13992 DEBUG nova.virt.hardware [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Got 10 possible topologies get_possible_topologies /usr/lib/python2.7/site-packages/nova/virt/hardware.py:442
2017-01-06 15:12:04.898 13992 DEBUG nova.virt.libvirt.vif [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=True,availability_zone='dzc',cell_name=None,cleaned=False,config_drive='',created_at=2017-01-06T03:36:54Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='asdasdas',display_name='asdasdas',ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,host='sjhl-o-compute09',hostname='asdasdas',id=285,image_ref='',info_cache=InstanceInfoCache,instance_type_id=21,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2017-01-06T03:42:54Z,launched_on='sjhl-o-compute09',locked=False,locked_by=None,memory_mb=8192,metadata={},node='sjhl-o-compute09',numa_topology=None,os_type=None,pci_devices=<?>,power_state=1,progress=0,project_id='e599088c985f42e7948b12f601705cd3',ramdisk_id='',reservation_id='r-2jp1vsmi',root_device_name='/dev/vda',root_gb=30,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='',image_container_format='bare',image_disk_format='qcow2',image_min_disk='30',image_min_ram='0',instance_type_ephemeral_gb='0',instance_type_flavorid='ab132230-f7ae-4a1b-a3cf-cb9674fd004b',instance_type_id='21',instance_type_memory_mb='8192',instance_type_name='fang-kibanna',instance_type_root_gb='30',instance_type_rxtx_factor='1.0',instance_type_swap='0',instance_type_vcpu_weight=None,instance_type_vcpus='8'},task_state='migrating',terminated_at=None,updated_at=2017-01-06T07:11:51Z,user_data=None,user_id='7d5b5abc30ea463690567e5f8cc794f9',uuid=7ff81f1d-7562-4692-a159-5459fa05ce04,vcpus=8,vm_mode=None,vm_state='active') vif=VIF({'profile': {}, 'ovs_interfaceid': u'90434480-21dc-436e-8d5c-a353948c362a', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:1e:da:eb', 'floating_ips': [], 'label': u'INSIDE_NET', 'meta': {}, 'address': u'192.168.33.43', 'type': 'fixed'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.33.4'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'218.30.110.17'}), IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [Route({'interface': None, 'cidr': u'0.0.0.0/0', 'meta': {}, 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.33.254'})})], 'cidr': u'192.168.33.0/24', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'e599088c985f42e7948b12f601705cd3'}, 'id': u'ba31704e-c1b6-4a43-8eb6-6162d25e7b29', 'label': u'INSIDE_NET'}), 'devname': u'tap90434480-21', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:1e:da:eb', 'active': False, 'type': u'ovs', 'id': u'90434480-21dc-436e-8d5c-a353948c362a', 'qbg_params': None}) virt_typekvm get_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:342
2017-01-06 15:12:04.900 13992 DEBUG nova.objects.instance [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Lazy-loading `pci_devices' on Instance uuid 7ff81f1d-7562-4692-a159-5459fa05ce04 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:579
2017-01-06 15:12:04.961 13992 DEBUG nova.virt.libvirt.config [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] Generated XML ('<domain type="kvm">\n  <uuid>7ff81f1d-7562-4692-a159-5459fa05ce04</uuid>\n  <name>instance-0000011d</name>\n  <memory>8388608</memory>\n  <vcpu cpuset="0-5,12-17">8</vcpu>\n  <metadata>\n    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">\n      <nova:package version="2014.2.2-1.el7"/>\n      <nova:name>asdasdas</nova:name>\n      <nova:creationTime>2017-01-06 07:12:04</nova:creationTime>\n      <nova:flavor name="fang-kibanna">\n        <nova:memory>8192</nova:memory>\n        <nova:disk>30</nova:disk>\n        <nova:swap>0</nova:swap>\n        <nova:ephemeral>0</nova:ephemeral>\n        <nova:vcpus>8</nova:vcpus>\n      </nova:flavor>\n      <nova:owner>\n        <nova:user uuid="7d5b5abc30ea463690567e5f8cc794f9">admin</nova:user>\n        <nova:project uuid="e599088c985f42e7948b12f601705cd3">admin</nova:project>\n      </nova:owner>\n    </nova:instance>\n  </metadata>\n  <sysinfo type="smbios">\n    <system>\n      <entry name="manufacturer">Fedora Project</entry>\n      <entry name="product">OpenStack Nova</entry>\n      <entry name="version">2014.2.2-1.el7</entry>\n      <entry name="serial">9b8aea6b-f8cc-483d-a613-8a9a1b9e12bd</entry>\n      <entry name="uuid">7ff81f1d-7562-4692-a159-5459fa05ce04</entry>\n    </system>\n  </sysinfo>\n  <os>\n    <type>hvm</type>\n    <boot dev="hd"/>\n    <smbios mode="sysinfo"/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n  </features>\n  <clock offset="utc">\n    <timer name="pit" tickpolicy="delay"/>\n    <timer name="rtc" tickpolicy="catchup"/>\n    <timer name="hpet" present="no"/>\n  </clock>\n  <cpu mode="host-model" match="exact">\n    <topology sockets="8" cores="1" threads="1"/>\n  </cpu>\n  <devices>\n    <disk type="network" device="disk">\n      <driver name="qemu" type="raw" cache="none"/>\n      <source protocol="rbd" name="volumes/volume-8740b14b-6ed7-4a2b-b441-683606cef933">\n        <host name="10.20.8.31" port="6789"/>\n        <host name="10.20.8.32" port="6789"/>\n        <host name="10.20.8.33" port="6789"/>\n      </source>\n      <auth username="cinder">\n        <secret type="ceph" uuid="3c97b1d3-1153-4454-8ffb-0fbf9ff01311"/>\n      </auth>\n      <target bus="virtio" dev="vda"/>\n      <serial>8740b14b-6ed7-4a2b-b441-683606cef933</serial>\n    </disk>\n    <interface type="bridge">\n      <mac address="fa:16:3e:1e:da:eb"/>\n      <model type="virtio"/>\n      <source bridge="qbr90434480-21"/>\n      <target dev="tap90434480-21"/>\n    </interface>\n    <serial type="file">\n      <source path="/var/lib/nova/instances/7ff81f1d-7562-4692-a159-5459fa05ce04/console.log"/>\n    </serial>\n    <serial type="pty"/>\n    <input type="tablet" bus="usb"/>\n    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>\n    <video>\n      <model type="cirrus"/>\n    </video>\n    <memballoon model="virtio">\n      <stats period="10"/>\n    </memballoon>\n  </devices>\n</domain>\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82
2017-01-06 15:12:04.961 13992 DEBUG nova.virt.libvirt.driver [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] End _get_guest_xml xml=<domain type="kvm">
  <uuid>7ff81f1d-7562-4692-a159-5459fa05ce04</uuid>
  <name>instance-0000011d</name>
  <memory>8388608</memory>
  <vcpu cpuset="0-5,12-17">8</vcpu>
  <metadata>
    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0">
      <nova:package version="2014.2.2-1.el7"/>
      <nova:name>asdasdas</nova:name>
      <nova:creationTime>2017-01-06 07:12:04</nova:creationTime>
      <nova:flavor name="fang-kibanna">
        <nova:memory>8192</nova:memory>
        <nova:disk>30</nova:disk>
        <nova:swap>0</nova:swap>
        <nova:ephemeral>0</nova:ephemeral>
        <nova:vcpus>8</nova:vcpus>
      </nova:flavor>
      <nova:owner>
        <nova:user uuid="7d5b5abc30ea463690567e5f8cc794f9">admin</nova:user>
        <nova:project uuid="e599088c985f42e7948b12f601705cd3">admin</nova:project>
      </nova:owner>
    </nova:instance>
  </metadata>
  <sysinfo type="smbios">
    <system>
      <entry name="manufacturer">Fedora Project</entry>
      <entry name="product">OpenStack Nova</entry>
      <entry name="version">2014.2.2-1.el7</entry>
      <entry name="serial">9b8aea6b-f8cc-483d-a613-8a9a1b9e12bd</entry>
      <entry name="uuid">7ff81f1d-7562-4692-a159-5459fa05ce04</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev="hd"/>
    <smbios mode="sysinfo"/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset="utc">
    <timer name="pit" tickpolicy="delay"/>
    <timer name="rtc" tickpolicy="catchup"/>
    <timer name="hpet" present="no"/>
  </clock>
  <cpu mode="host-model" match="exact">
    <topology sockets="8" cores="1" threads="1"/>
  </cpu>
  <devices>
    <disk type="network" device="disk">
      <driver name="qemu" type="raw" cache="none"/>
      <source protocol="rbd" name="volumes/volume-8740b14b-6ed7-4a2b-b441-683606cef933">
        <host name="10.20.8.31" port="6789"/>
        <host name="10.20.8.32" port="6789"/>
        <host name="10.20.8.33" port="6789"/>
      </source>
      <auth username="cinder">
        <secret type="ceph" uuid="3c97b1d3-1153-4454-8ffb-0fbf9ff01311"/>
      </auth>
      <target bus="virtio" dev="vda"/>
      <serial>8740b14b-6ed7-4a2b-b441-683606cef933</serial>
    </disk>
    <interface type="bridge">
      <mac address="fa:16:3e:1e:da:eb"/>
      <model type="virtio"/>
      <source bridge="qbr90434480-21"/>
      <target dev="tap90434480-21"/>
    </interface>
    <serial type="file">
      <source path="/var/lib/nova/instances/7ff81f1d-7562-4692-a159-5459fa05ce04/console.log"/>
    </serial>
    <serial type="pty"/>
    <input type="tablet" bus="usb"/>
    <graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/>
    <video>
      <model type="cirrus"/>
    </video>
    <memballoon model="virtio">
      <stats period="10"/>
    </memballoon>
  </devices>
</domain>
 _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4168
2017-01-06 15:12:04.973 13992 DEBUG nova.compute.manager [req-a3103d74-9608-4edd-99a4-bf21f815a055 None] [instance: 7ff81f1d-7562-4692-a159-5459fa05ce04] Checking state _get_power_state /usr/lib/python2.7/site-packages/nova/compute/manager.py:1159
2017-01-06 15:12:15.458 13992 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_unconfirmed_resizes run_periodic_tasks /usr/lib/python2.7/site-packages/nova/openstack/common/periodic_task.py:193
2017-01-06 15:12:15.458 13992 DEBUG nova.openstack.common.loopingcall [-] Dynamic looping call <bound method Service.periodic_tasks of <nova.service.Service object at 0x2584f10>> sleeping for 11.99 seconds _inner /usr/lib/python2.7/site-packages/nova/openstack/common/loopingcall.py:132
########################################################################################################################

sjhl-o-compute08之前的 创建虚拟机失败
重启dbus-org.freedesktop.machine1.service 这个服务恢复，可以在主机上创建虚拟机了，但是迁移还是有问题，又进行了模拟compute08
宿主机断电实验 在控制节点上进行了 nova evacuate  0001 --on-shared-storage ,状态一直是Rebuilding
systemctl restart dbus-org.freedesktop.machine1.service
This is a tiny daemon that tracks locally running Virtual Machines and Containers in various ways.


[root@sjhl-o-compute08 nova]# systemctl  status dbus-org.freedesktop.machine1.service  -l
● systemd-machined.service - Virtual Machine and Container Registration Service
   Loaded: loaded (/usr/lib/systemd/system/systemd-machined.service; static; vendor preset: disabled)
   Active: active (running) since Tue 2017-01-17 15:01:28 CST; 1h 8min ago
     Docs: man:systemd-machined.service(8)
           http://www.freedesktop.org/wiki/Software/systemd/machined
 Main PID: 28347 (systemd-machine)
   Status: "Processing requests..."
   CGroup: /system.slice/systemd-machined.service
           └─28347 /usr/lib/systemd/systemd-machined

Jan 17 15:01:28 sjhl-o-compute08 systemd[1]: Starting Virtual Machine and Container Registration Service...
Jan 17 15:01:28 sjhl-o-compute08 systemd[1]: Started Virtual Machine and Container Registration Service.
Jan 17 15:01:28 sjhl-o-compute08 systemd-machined[28347]: New machine qemu-instance-00000120.
Jan 17 15:20:20 sjhl-o-compute08 systemd-machined[28347]: New machine qemu-instance-00000124.
Jan 17 15:26:14 sjhl-o-compute08 systemd-machined[28347]: Machine qemu-instance-00000124 terminated.


2017-01-17 14:46:15.181 20385 ERROR nova.compute.manager [-] [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2] Instance failed to spawn
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2] Traceback (most recent call last):
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2246, in _build_resources
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     yield resources
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2116, in _build_and_run_instance
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     block_device_info=block_device_info)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 2622, in spawn
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     block_device_info, disk_info=disk_info)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4425, in _create_domain_and_network
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     power_on=power_on)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4349, in _create_domain
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     LOG.error(err)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py", line 82, in __exit__
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     six.reraise(self.type_, self.value, self.tb)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4339, in _create_domain
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     domain.createWithFlags(launch_flags)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 183, in doit
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     result = proxy_call(self._autowrap, f, *args, **kwargs)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 141, in proxy_call
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     rv = execute(f, *args, **kwargs)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 122, in execute
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     six.reraise(c, e, tb)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib/python2.7/site-packages/eventlet/tpool.py", line 80, in tworker
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     rv = meth(*args, **kwargs)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]   File "/usr/lib64/python2.7/site-packages/libvirt.py", line 1059, in createWithFlags
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2]     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2] libvirtError: Activation of org.freedesktop.machine1 timed out
2017-01-17 14:46:15.181 20385 TRACE nova.compute.manager [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2] 
2017-01-17 14:46:15.213 20385 AUDIT nova.compute.manager [req-8b5b56d2-d416-45c8-9220-a86e40c7d489 None] [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2] Terminating instance
2017-01-17 14:46:15.225 20385 INFO nova.virt.libvirt.driver [-] [instance: f9feb419-d49d-4460-9cce-8bc3f3d5c8b2] Instance destroyed successfully.
########################################################################################################################
########################################################################################################################
########################################################################################################################
########################################################################################################################
########################################################################################################################
########################################################################################################################
########################################################################################################################
########################################################################################################################
openstack热迁移
OpenStack有两种在线迁移类型：live migration和block migration。Livemigration需要实例保存在NFS共享存储中，这种迁移主要是实例的内存状态的迁移，速度应该会很快。Block migration除了实例内存状态要迁移外，还得迁移磁盘文件，速度会慢些，但是它不要求实例存储在共享文件系统中。
* NFS允许一个系统在网络上与他人共享目录和文件。通过使用NFS，用户和程序可以像访问本地文件一样访问远端系统上的文件。
 
########################################################################################################################
Neutron Server
对外提供 OpenStack 网络 API，接收请求，并调用 Plugin 处理请求。
Plugin
处理 Neutron Server 发来的请求，维护 OpenStack 逻辑网络的状态， 并调用 Agent 处理请求。
Agent
处理 Plugin 的请求，负责在 network provider 上真正实现各种网络功能。
network provider
提供网络服务的虚拟或物理网络设备，例如 Linux Bridge，Open vSwitch 或者其他支持 Neutron 的物理交换机。
Queue
Neutron Server，Plugin 和 Agent 之间通过 Messaging Queue 通信和调用。
Database
存放 OpenStack 的网络状态信息，包括 Network, Subnet, Port, Router 等。

以创建一个 VLAN100 的 network 为例，假设 network provider 是 linux bridge， 流程如下：
Neutron Server 接收到创建 network 的请求，通过 Message Queue（RabbitMQ）通知已注册的 Linux Bridge Plugin。
Plugin 将要创建的 network 的信息（例如名称、VLAN ID等）保存到数据库中，并通过 Message Queue 通知运行在各节点上的 Agent。
Agent 收到消息后会在节点上的物理网卡（比如 eth2）上创建 VLAN 设备（比如 eth2.100），并创建 bridge （比如 brqXXX） 桥接 VLAN 设备。

1.plugin 解决的是 What 的问题，即网络要配置成什么样子？而至于如何配置 How 的工作则交由 agent 完成。
2.plugin，agent 和 network provider 是配套使用的，比如上例中 network provider 是 linux bridge，那么就得使用 linux bridge 的 plungin 和 agent；如果 network provider 换成了 OVS 或者物理交换机，plugin 和 agent 也得替换。
3.plugin 的一个主要的职责是在数据库中维护 Neutron 网络的状态信息，这就造成一个问题：所有 network provider 的 plugin 都要编写一套非常类似的数据库访问代码。为了解决这个问题，Neutron 在 Havana 版本实现了一个 ML2（Modular Layer 2）plugin，对 plgin 的功能进行抽象和封装。有了 ML2 plugin，各种 network provider 无需开发自己的 plugin，只需要针对 ML2 开发相应的 driver 就可以了，工作量和难度都大大减少。ML2 会在后面详细讨论。
4.plugin 按照功能分为两类： core plugin 和 service plugin。core plugin 维护 Neutron 的 netowrk, subnet 和 port 相关资源的信息，与 core plugin 对应的 agent 包括 linux bridge, OVS 等； service plugin 提供 routing, firewall, load balance 等服务，也有相应的 agent。后面也会分别详细讨论。

########################################################################################################################
部署方案一：控制节点和计算节点

部署的服务包括 neutron-server,core plugin 的agent和service plugin的agent
计算节点部署core plugin的agent 负责提供二层网络功能
1. core plugin 和 service plugin 已经集成在neutron server 中不需要再单独运行plugin服务
2. 控制节点和计算节点都需要部署 core plugin 的agent ，这样才能使控制节点和计算节点在二层上打通

部署方案二：控制节点和计算节点和网络节点
控制节点
部署 neutron server 服务。
网络节点
部署 core plugin 的agent 和service plugin 的plugin
计算机点 部署 core plugin 的agent
这个方案的要点是将所有的 agent 从控制节点分离出来，部署到独立的网络节点上。

1.控制节点只负责通过 neutron server 响应 API 请求。

2.由独立的网络节点实现数据的交换，路由以及 load balance等高级网络服务。

3.可以通过增加网络节点承担更大的负载。
可以部署多个控制节点、网络节点和计算节点。

########################################################################################################################  
neutron server =APi +plugin 
Neutron Server 包括两部分： 1. 提供 API 服务。 2. 运行 Plugin。
Core API
对外提供管理 network, subnet 和 port 的 RESTful API。

Extension API
对外提供管理 router, load balance, firewall 等资源 的 RESTful API。

Commnon Service
认证和校验 API 请求。

Neutron Core
Neutron server 的核心处理程序，通过调用相应的 Plugin 处理请求。

Core Plugin API
定义了 Core Plugin 的抽象功能集合，Neutron Core 通过该 API 调用相应的 Core Plgin。

Extension Plugin API
定义了 Service Plugin 的抽象功能集合，Neutron Core 通过该 API 调用相应的 Service Plugin。

Core Plugin
实现了 Core Plugin API，在数据库中维护 network, subnet 和 port 的状态，并负责调用相应的 agent 在 network provider 上执行相关操作，比如创建 network。
Service Plugin
实现了 Extension Plugin API，在数据库中维护 router, load balance, security group 等资源的状态，并负责调用相应的 agent 在 network provider 上执行相关操作，比如创建 router。

8.Ml2对二层网络进行抽象和建模，引入了type driver和 mechansim driver 
这两类driver解耦了Neutron所支持的网络类型与访问这些网络类型的机制
其结果就是使得 ML2 具有非常好的弹性，易于扩展，能够灵活支持多种 type 和 mechanism。

TYpe Drive
neutron 支持的每一种网络类型都有一个对应的ML2 type driver,type driver 负责维护网络类型状态，
执行验证，创建网络等，ML2支持的网络类型包括local,flat,vlan gre vxlan

Mechansim Driver
neutron 支持的每一种网络机制都一个对应的 ML2 mechansim driver。 mechanism driver 负责获取由 type driver 维护的网络状态，并确保在相应的网络设备（物理或虚拟）上正确实现这些状态。
type 和 mechanisim 都太抽象，现在我们举一个具体的例子： type driver 为 vlan，mechansim driver 为 linux bridge，我们要完成的操作是创建 network vlan100，那么：

vlan type driver 会确保将 vlan100 的信息保存到 Neutron 数据库中，包括 network 的名称，vlan ID 等。

linux bridge mechanism driver 会确保各节点上的 linux brige agent 在物理网卡上创建 ID 为 100 的 vlan 设备 和 brige 设备，并将两者进行桥接。
mechanism driver 有三种类型：
Agent-based
包括 linux bridge, open vswitch 等。
Controller-based
包括 OpenDaylight, VMWare NSX 等。
基于物理交换机
包括 Cisco Nexus, Arista, Mellanox 等。 比如前面那个例子如果换成 Cisco 的 mechanism driver，则会在 Cisco 物理交换机的指定 trunk 端口上添加 vlan100。

########################################################################################################################
core plugin/agent 负责管理核心实体:net subnet port 更高级的网络服务则由service plugin/Agent管理
service plugin/agent 及其Agent提供更丰富的扩展功能，路由，load balance firewall等
DHCP
dhcp agent 通过 dnsmasq 为 instance 提供 dhcp 服务。
Routing
l3 agent 可以为 project（租户）创建 router，提供 Neutron subnet 之间的路由服务。路由功能默认通过 IPtables 实现。
Firewall
l3 agent 可以在route上配置防火墙策略，提供网络安全防护
另一个与安全相关的功能是 Security Group，也是通过 IPtables 实现。 Firewall 与 Security Group 的区别在于：
1.Firewall安全策略位于route 保护的是某个project的所有network
2.Security group安全策略位于instance 保护的是单个instance
Load Balance
Neutron 默认通过 HAProxy 为 project 中的多个 instance 提供 load balance 服务。

metadata-agent 之前没有讲到，这里做个补充：

instance 在启动时需要访问 nova-metadata-api 服务获取 metadata 和 userdata，这些 data 是该 instance 的定制化信息，比如 hostname, ip， public key 等。

但 instance 启动时并没有 ip，如何能够通过网络访问到 nova-metadata-api 服务呢？

答案就是 neutron-metadata-agent
该 agent 让 instance 能够通过 dhcp-agent 或者 l3-agent 与 nova-metadata-api 通信
instance 通过 neutron-metadata-agent 和 nova-metadata-api通信
########################################################################################################################  
openstack 网络流量类型
1.managetment网络
用户节点之间message queue 内部通信以及访问db，所有节点都需要连接到managent网络
2.APi网络
 OpenStack 各组件通过该网络向用户暴露 API 服务。Keystone, Nova, Neutron, Glance, Cinder, Horizon 的 endpoints 均配置在 API 网络上。
 通常，管理员也通过 API 网络 SSH 管理各个节点。
3. VM 网络
VM网络也叫tenant网络，用户instance 间通信VM 网络可以选择的类型包括 local, flat, vlan, vxlan 和 gre。 VM 网络由 Neutron 配置和管理。
External 网络
4.External 网络指的是 VM 网络之外的网络，该网络不由 Neutron 管理。 Neutron 可以将 router attach 到 External 网络，为 instance 提供访问外部网络的能力。 External 网络可能是企业的 intranet，也可能是 internet。
这几类网络只是逻辑上的划分，物理实现上有非常大的自由度
########################################################################################################################
/etc/neutron/neutron.conf
core_plugin=ml2
/etc/neutron/plugins/ml2/ml2_conf.ini
type_drivers = flat,vlan
tenant_network_types = vlan
mechanism_drivers = openvswitch
控制节点和计算节点都需要在各自的 ml2_conf.ini 中配置 mechanism_drivers 选项。


########################################################################################################################  
到这里 local network 的知识点已经讨论完毕，做个小结吧。

1. 位于同一 local network 的 instance 可以通信。
2. 位于不同 local network 的 instance 无法通信。
3. 一个 local network 只能位于一个物理节点，无法跨节点。

虽然在实际应用中极少使用 local network，但学习 local network 的意义在于：
local network 可以作为学习 flat, vlan, vxlan 等更复杂网络类型的起点，降低 Neutron 的学习难度。
flat network 是不带 tag 的网络，要求宿主机的物理网卡直接与 linux bridge 连接，这意味着：
每个 flat network 都会独占一个物理网卡。
支持多个 flat

如果要创建多个 flat 网络，需要定义多个 label，用逗号隔开，当然也需要用到多个物理网卡，如下所示：

[ml2_type_flat]
flat_networks = flat1,flat2
[linux_bridge]
physical_interface_mappings = flat1:eth1,flat2:eth2

DHCP agent 
/etc/neutron/dhcp-agent.ini 

dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
dhcp_driver
使用 dnsmasq 实现 DHCP。
interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver
使用ovs连接dhcp namespace interface
当创建 network 并在 subnet 上 enable DHCP 时，网络节点上的 DHCP agent 会启动一个 dnsmasq 进程为该 network 提供 DHCP 服务。
dnsmasq 是一个提供 DHCP 和 DNS 服务的开源软件。 dnsmasq 与 network 是一对一关系，一个 dnsmasq 进程可以为同一 netowrk 中所有 enable 了 DHCP 的 subnet 提供服务。

搜房openstack -juno上面有两个网络 对应着网络节点上的两个dnsmasq进程
[root@dzc-o-control01v cinder]# neutron net-list
+--------------------------------------+------------+------------------------------------------------------+
| id                                   | name       | subnets                                              |
+--------------------------------------+------------+------------------------------------------------------+
| ba31704e-c1b6-4a43-8eb6-6162d25e7b29 | INSIDE_NET | e347242a-4fdb-40d8-901d-77987644ecca 192.168.33.0/24 |
| d647355e-b019-4de0-999e-00107531edc0 | DMZ_NET    | b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 192.168.34.0/24 |
+--------------------------------------+------------+------------------------------------------------------+
[root@dzc-o-control01v cinder]# neutron subnet-list
+--------------------------------------+---------------+-----------------+----------------------------------------------------+
| id                                   | name          | cidr            | allocation_pools                                   |
+--------------------------------------+---------------+-----------------+----------------------------------------------------+
| b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 | DMZ_SUBNET    | 192.168.34.0/24 | {"start": "192.168.34.3", "end": "192.168.34.250"} |
| e347242a-4fdb-40d8-901d-77987644ecca | INSIDE_SUBNET | 192.168.33.0/24 | {"start": "192.168.33.3", "end": "192.168.33.250"} |
+--------------------------------------+---------------+-----------------+----------------------------------------------------+

[root@dzc-o-neutron01v ~]# ps -ef |grep dns
root      4646 23239  0 14:10 pts/0    00:00:00 grep --color=auto dns
nobody   19367     1  0  2016 ?        00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=tapdb21d3d0-99 --except-interface=lo --pid-file=/var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/host --addn-hosts=/var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/opts --leasefile-ro --dhcp-authoritative --dhcp-range=set:tag0,192.168.34.0,static,86400s --dhcp-lease-max=256 --conf-file=/etc/neutron/dnsmasq-neutron.conf --domain=openstacklocal
nobody   19369     1  0  2016 ?        00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=tap905ede5c-4d --except-interface=lo --pid-file=/var/lib/neutron/dhcp/ba31704e-c1b6-4a43-8eb6-6162d25e7b29/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/ba31704e-c1b6-4a43-8eb6-6162d25e7b29/host --addn-hosts=/var/lib/neutron/dhcp/ba31704e-c1b6-4a43-8eb6-6162d25e7b29/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/ba31704e-c1b6-4a43-8eb6-6162d25e7b29/opts --leasefile-ro --dhcp-authoritative --dhcp-range=set:tag0,192.168.33.0,static,86400s --dhcp-lease-max=256 --conf-file=/etc/neutron/dnsmasq-neutron.conf --domain=openstacklocal

/var/lib/neutron/dhcp/目录下会保存有两个dhcp进程的文件
[root@dzc-o-neutron01v dhcp]# ll
total 0
drwxr-xr-x 2 neutron neutron 71 Jan  3 15:27 ba31704e-c1b6-4a43-8eb6-6162d25e7b29
drwxr-xr-x 2 neutron neutron 71 Jan  4 11:26 d647355e-b019-4de0-999e-00107531edc0

[root@dzc-o-neutron01v d647355e-b019-4de0-999e-00107531edc0]# cat host 
fa:16:3e:ff:4b:ac,host-192-168-34-14.openstacklocal,192.168.34.14
fa:16:3e:47:3e:55,host-192-168-34-5.openstacklocal,192.168.34.5
fa:16:3e:a9:15:53,host-192-168-34-11.openstacklocal,192.168.34.11
fa:16:3e:9d:64:50,host-192-168-34-16.openstacklocal,192.168.34.16
fa:16:3e:bd:43:88,host-192-168-34-19.openstacklocal,192.168.34.19
fa:16:3e:e0:6f:c9,host-192-168-34-12.openstacklocal,192.168.34.12
fa:16:3e:42:27:d5,host-192-168-34-15.openstacklocal,192.168.34.15
fa:16:3e:96:f0:bf,host-192-168-34-3.openstacklocal,192.168.34.3
fa:16:3e:d5:53:a1,host-192-168-34-51.openstacklocal,192.168.34.51
fa:16:3e:1a:98:12,host-192-168-34-54.openstacklocal,192.168.34.54
fa:16:3e:35:2b:e1,host-192-168-34-57.openstacklocal,192.168.34.57
fa:16:3e:45:e9:0e,host-192-168-34-86.openstacklocal,192.168.34.86
fa:16:3e:5c:00:d3,host-192-168-34-87.openstacklocal,192.168.34.87
存放 DHCP host 信息的文件，这里的 host 在我们这里实际上就是 instance。 dnsmasq 从该文件获取 host 的 IP 与 MAC 的对应关系。 每个 host 对应一个条目，信息来源于 Neutron 数据库。
Linux Network Namespace
Neutron 通过 dnsmasq 提供 DHCP 服务，而 dnsmasq 如何独立的为每个 network 服务呢？
答案是通过 Linux Network Namespace 隔离，本节将详细讨论。在二层网络上，VLAN 可以将一个物理交换机分割成几个独立的虚拟交换机。 类似地，在三层网络上，Linux network namespace 可以将一个物理三层网络分割成几个独立的虚拟三层网络。
Neutron 通过 namespace 为每个 network 提供独立的 DHCP 和路由服务，从而允许租户创建重叠的网络。
如果没有 namespace，网络就不能重叠，这样就失去了很多灵活性。
每个 dnsmasq 进程都位于独立的 namespace, 命名为 qdhcp-<network id>，例如 flat_net，我们有：

[root@dzc-o-neutron01v d647355e-b019-4de0-999e-00107531edc0]# ip netns
qdhcp-d647355e-b019-4de0-999e-00107531edc0
qdhcp-ba31704e-c1b6-4a43-8eb6-6162d25e7b29
其实，宿主机本身也有一个 namespace，叫 root namespace，拥有所有物理和虚拟 interface device。 物理 interface 只能位于 root namespace。
新创建的 namespace 默认只有一个 loopback device。 管理员可以将虚拟 interface，例如 bridge，tap 等设备添加到某个 namespace。

ip netns exec qdhcp-d647355e-b019-4de0-999e-00107531edc0  bash
[root@dzc-o-neutron01v ~]# ifconfig 
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 0  (Local Loopback)
        RX packets 289  bytes 103064 (100.6 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 289  bytes 103064 (100.6 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

tapdb21d3d0-99: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.34.3  netmask 255.255.255.0  broadcast 192.168.34.255
        inet6 fe80::f816:3eff:fe96:f0bf  prefixlen 64  scopeid 0x20<link>
        ether fa:16:3e:96:f0:bf  txqueuelen 0  (Ethernet)
        RX packets 1913296  bytes 117567892 (112.1 MiB)
        RX errors 0  dropped 30  overruns 0  frame 0
        TX packets 5642  bytes 587752 (573.9 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

对于 flat_net 的 DHCP 设备 tap19a0ed3d-fe，需要将其放到 namespace qdhcp-f153b42f-c3a1-4b6c-8865-c09b5b2aa274 中，但这样会带来一个问题： tap19a0ed3d-fe 将无法直接与 root namespace 中的 bridge 设备 brqf153b42f-c3 连接。
Neutron 使用 veth pair 解决了这个问题。
veth pair 是一种成对出现的特殊网络设备，它们象一根虚拟的网线，可用于连接两个 namespace。向 veth pair 一端输入数据，在另一端就能读到此数据。
tap19a0ed3d-fe 与 ns-19a0ed3d-fe 就是一对 veth pair，它们将 qdhcp-f153b42f-c3a1-4b6c-8865-c09b5b2aa274 连接到 brqf153b42f-c3。


下图显示launch instance时 dnsmasq的工作

1.neutron分配port里面包含了MAC和ip信息，这些信息会同步到dnsmasq的host文件，同时nova-compute会设置vm -vif的mac地址 fa:16:3e:d0:35:ea
2.vm开机 会发出 DHCPDISCOVER(tapdb21d3d0-99) fa:16:3e:d0:35:ea 广播
3.广播到达 veth tapdb21d3d0-99，然后传送给 veth pair 的另一端 tapdb21d3d0-99。dnsmasq 在它上面监听，dnsmasq 检查其 host 文件，发现有对应项，于是dnsmasq 以  DHCPOFFER 消息将 IP（192.168.34.148）、子网掩码（255.255.255.0）、地址租用期限等信息发送给 cirros-vm1。
4.vm 发送DHCPREQUEST确认消息接受此DHCPOFFER
5.DHCPACK(tapdb21d3d0-99) 192.168.34.148 fa:16:3e:d0:35:ea host-192-168-34-148

[root@dzc-o-neutron01v ~]# tail -f /var/log/messages |grep dnsmasq
Jan  4 14:31:10 dzc-o-neutron01v dnsmasq[19367]: read /var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/addn_hosts - 14 addresses
Jan  4 14:31:10 dzc-o-neutron01v dnsmasq-dhcp[19367]: read /var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/host
Jan  4 14:31:10 dzc-o-neutron01v dnsmasq-dhcp[19367]: read /var/lib/neutron/dhcp/d647355e-b019-4de0-999e-00107531edc0/opts
Jan  4 14:31:21 dzc-o-neutron01v dnsmasq-dhcp[19367]: DHCPDISCOVER(tapdb21d3d0-99) fa:16:3e:d0:35:ea
Jan  4 14:31:21 dzc-o-neutron01v dnsmasq-dhcp[19367]: DHCPOFFER(tapdb21d3d0-99) 192.168.34.148 fa:16:3e:d0:35:ea
Jan  4 14:31:21 dzc-o-neutron01v dnsmasq-dhcp[19367]: DHCPDISCOVER(tapdb21d3d0-99) fa:16:3e:d0:35:ea
Jan  4 14:31:21 dzc-o-neutron01v dnsmasq-dhcp[19367]: DHCPOFFER(tapdb21d3d0-99) 192.168.34.148 fa:16:3e:d0:35:ea
Jan  4 14:31:21 dzc-o-neutron01v dnsmasq-dhcp[19367]: DHCPREQUEST(tapdb21d3d0-99) 192.168.34.148 fa:16:3e:d0:35:ea
Jan  4 14:31:21 dzc-o-neutron01v dnsmasq-dhcp[19367]: DHCPACK(tapdb21d3d0-99) 192.168.34.148 fa:16:3e:d0:35:ea host-192-168-34-148
Jan  4 14:31:21 dzc-o-neutron01v dnsmasq-dhcp[19367]: DHCPREQUEST(tapdb21d3d0-99) 192.168.34.148 fa:16:3e:d0:35:ea
Jan  4 14:31:21 dzc-o-neutron01v dnsmasq-dhcp[19367]: DHCPACK(tapdb21d3d0-99) 192.168.34.148 fa:16:3e:d0:35:ea host-192-168-34-148



14:34:33.984087 IP 0.0.0.0.bootpc > 255.255.255.255.bootps: BOOTP/DHCP, Request from fa:16:3e:4e:39:e9 (oui Unknown), length 290
14:34:33.984149 IP 0.0.0.0.bootpc > 255.255.255.255.bootps: BOOTP/DHCP, Request from fa:16:3e:4e:39:e9 (oui Unknown), length 290
14:34:33.984475 IP dzc-o-neutron01v.bootps > 192.168.34.149.bootpc: BOOTP/DHCP, Reply, length 316
14:34:33.984602 IP dzc-o-neutron01v.bootps > 192.168.34.149.bootpc: BOOTP/DHCP, Reply, length 316
14:34:33.985281 IP 0.0.0.0.bootpc > 255.255.255.255.bootps: BOOTP/DHCP, Request from fa:16:3e:4e:39:e9 (oui Unknown), length 302
14:34:33.985303 IP 0.0.0.0.bootpc > 255.255.255.255.bootps: BOOTP/DHCP, Request from fa:16:3e:4e:39:e9 (oui Unknown), length 302
14:34:33.985570 IP dzc-o-neutron01v.bootps > 192.168.34.149.bootpc: BOOTP/DHCP, Reply, length 337
14:34:33.985691 IP dzc-o-neutron01v.bootps > 192.168.34.149.bootpc: BOOTP/DHCP, Reply, length 337
14:34:34.065331 ARP, Request who-has 192.168.34.254 tell 192.168.34.149, length 46
14:34:34.065351 ARP, Request who-has 192.168.34.254 tell 192.168.34.149, length 46

########################################################################################################################
openstack -vlan 模式 网络解析

1.三个 instance 通过 TAP 设备连接到名为 “brqXXXX” linux bridge。
2. 在物理网卡 eth1 上创建了 eth1.100 的 vlan interface，eth1.100 连接到 brqXXXX。
3. instance 通过 eth1.100 发送到 eth1 的数据包就会打上 vlan100 的 tag。
如果再创建一个 network vlan101，eth1 上会相应的创建 vlan interface eth1.101，并且连接的新的 lingux bridge “brqYYYY” 。
每个 vlan network 有自己的 bridge，从而也就实现了基于 vlan 的隔离。

[ovs]
local_ip = 10.20.8.33
tenant_network_type = vlan
integration_bridge = br-int
network_vlan_ranges = physnet1:500:599
bridge_mappings = physnet1:br-ex
搜房用的  ovs 映射的网卡是 br-ex 这个是 bond的网桥
physnet1只是一个label 起到标识作用
    Bridge br-ex
        Port br-ex
            Interface br-ex
                type: internal
        Port "bond0"
            Interface "bond0"
        Port phy-br-ex
            Interface phy-br-ex
                type: patch
                options: {peer=int-br-ex}
				
ifenslave bond0 eth2 eth3

ovs-vsctl add-port br-ex bond0


Neutron 的路由服务是由 l3 agent 提供的。 除此之外，l3 agent 通过 iptables 提供 firewall 和 floating ip 服务。
/etc/neutron/ls-agent.ini
interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver

[root@dzc-o-control01v cinder]# neutron agent-list
+--------------------------------------+--------------------+------------------+-------+----------------+---------------------------+
| id                                   | agent_type         | host             | alive | admin_state_up | binary                    |
+--------------------------------------+--------------------+------------------+-------+----------------+---------------------------+
| 092d23a0-7681-4750-9343-1dbaaac0ee53 | Open vSwitch agent | sjhl-o-compute10 | :-)   | True           | neutron-openvswitch-agent |
| 0a72462b-ee92-4ee0-beab-58dda19845e0 | Open vSwitch agent | dzc-o-neutron01v | :-)   | True           | neutron-openvswitch-agent |
| 101a0260-686a-4cf6-8f09-bc8e7d45c006 | DHCP agent         | dzc-o-neutron01v | :-)   | True           | neutron-dhcp-agent        |
| 21e4221d-0358-40ea-a7db-531c2e391ce8 | Open vSwitch agent | sjhl-o-compute03 | :-)   | True           | neutron-openvswitch-agent |
| 52429ac1-80bf-4acf-ba8c-919ee95c8fbf | Open vSwitch agent | dzc-o-compute01  | :-)   | True           | neutron-openvswitch-agent |
| 6254bbd0-d7d4-4a56-9225-437be50778f9 | Open vSwitch agent | sjhl-o-compute05 | :-)   | True           | neutron-openvswitch-agent |
| 739662e4-0e38-4d56-9c8e-a27693549a07 | Open vSwitch agent | sjhl-o-compute08 | :-)   | True           | neutron-openvswitch-agent |
| 874dffde-eecd-45bd-9343-93484928a5f4 | Open vSwitch agent | sjhl-o-compute07 | :-)   | True           | neutron-openvswitch-agent |
| a65d8b06-9c29-4a8e-988b-dc3f01238207 | Metadata agent     | dzc-o-neutron01v | :-)   | True           | neutron-metadata-agent    |
| cb446aa6-257c-476e-bb51-4096db3eb74d | Open vSwitch agent | sjhl-o-compute04 | :-)   | True           | neutron-openvswitch-agent |
| cd4cc1d5-70df-4d0f-b9be-77eb60e95c55 | Open vSwitch agent | sjhl-o-compute06 | :-)   | True           | neutron-openvswitch-agent |
| dbad5f8c-8890-4d3a-9731-6f17554d8aeb | Open vSwitch agent | sjhl-o-compute09 | :-)   | True           | neutron-openvswitch-agent |
| e63177f9-e4c5-462e-92a2-8f5016df318f | Open vSwitch agent | dzc-o-compute02  | :-)   | True           | neutron-openvswitch-agent |
| f6ea1599-6288-408e-ab2d-b6d4745fba1b | L3 agent           | dzc-o-neutron01v | :-)   | True           | neutron-l3-agent          |
+--------------------------------------+--------------------+------------------+-------+----------------+---------------------------+


[ovs]
local_ip = 192.168.32.11
#local_ip = 192.168.2.39
tenant_network_type = vlan
integration_bridge = br-int
network_vlan_ranges = physnet1:500:599
bridge_mappings = physnet1:br-ex




答案是： l3 agent 会为每个 router 创建了一个 namespace，通过 veth pair 与 TAP 相连，然后将 Gateway IP 配置在位于 namespace 里面的 veth interface 上，这样就能提供路由了。
########################################################################################################################  

Floating IP 是相对于Fixed IP而言的，它一般是在VM创建后分配给VM的，可以达到的目的就是，外界可以访问通过这个Floating Ip访问这个VM，VM也可以通过这个IP访问外界。

在OpenStack中，这个Floating IP使用了namespace内的iptables建立NAT 转发机制来达到VM与外界的通讯的
1. floating IP 能够让外网直接访问租户网络中的 instance。这是通过在 router 上应用 iptalbes 的 NAT 规则实现的。
2. floating IP 是配置在 router 的外网 interface 上的，而非 instance，这一点需要特别注意。

overlay network 是指建立在其他网络上的网络。 该网络中的节点可以看作通过虚拟（或逻辑）链路连接起来的。 
########################################################################################################################
为openstack虚拟机添加网卡
1.查看网络，创建网络
[root@dzc-o-control01v ~]# neutron net-list
+--------------------------------------+------------+------------------------------------------------------+
| id                                   | name       | subnets                                              |
+--------------------------------------+------------+------------------------------------------------------+
| 50f5cafb-91c2-48f2-8af2-8f85a1b3d31c | vlan503    | 113b0d21-9dfc-47c3-8960-9ea44ae82270 192.168.35.0/24 |
| ba31704e-c1b6-4a43-8eb6-6162d25e7b29 | INSIDE_NET | e347242a-4fdb-40d8-901d-77987644ecca 192.168.33.0/24 |
| d647355e-b019-4de0-999e-00107531edc0 | DMZ_NET    | b7dba8f8-62ac-4286-ba2f-093cc3d95bc1 192.168.34.0/24 |
+--------------------------------------+------------+------------------------------------------------------+
2.创建端口
[root@dzc-o-control01v ~]# neutron port-create 50f5cafb-91c2-48f2-8af2-8f85a1b3d31c
Created a new port:
+-----------------------+--------------------------------------------------------------------------------------+
| Field                 | Value                                                                                |
+-----------------------+--------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                 |
| allowed_address_pairs |                                                                                      |
| binding:host_id       |                                                                                      |
| binding:profile       | {}                                                                                   |
| binding:vif_details   | {}                                                                                   |
| binding:vif_type      | unbound                                                                              |
| binding:vnic_type     | normal                                                                               |
| device_id             |                                                                                      |
| device_owner          |                                                                                      |
| fixed_ips             | {"subnet_id": "113b0d21-9dfc-47c3-8960-9ea44ae82270", "ip_address": "192.168.35.12"} |
| id                    | 87b41a99-a444-4dca-bfc9-40b18d56eb51                                                 |
| mac_address           | fa:16:3e:c6:2a:c7                                                                    |
| name                  |                                                                                      |
| network_id            | 50f5cafb-91c2-48f2-8af2-8f85a1b3d31c                                                 |
| security_groups       | b20cac25-ec58-4830-b7f3-888a54ce1391                                                 |
| status                | DOWN                                                                                 |
| tenant_id             | e599088c985f42e7948b12f601705cd3                                                     |

3.attach给instance 
[root@dzc-o-control01v ~]# nova interface-attach  --port-id 87b41a99-a444-4dca-bfc9-40b18d56eb51 7c2385a2-bacd-4c3c-aa73-0186df05dc17
[root@dzc-o-control01v ~]# nova interface-list 7c2385a2-bacd-4c3c-aa73-0186df05dc17
+------------+--------------------------------------+--------------------------------------+----------------+-------------------+
| Port State | Port ID                              | Net ID                               | IP addresses   | MAC Addr          |
+------------+--------------------------------------+--------------------------------------+----------------+-------------------+
| ACTIVE     | 4fcff239-ecb3-4cec-9d5e-9c4bbde61b3a | ba31704e-c1b6-4a43-8eb6-6162d25e7b29 | 192.168.33.31  | fa:16:3e:19:c1:69 |
| ACTIVE     | 87b41a99-a444-4dca-bfc9-40b18d56eb51 | 50f5cafb-91c2-48f2-8af2-8f85a1b3d31c | 192.168.35.12  | fa:16:3e:c6:2a:c7 |
| ACTIVE     | c879659d-9fd3-48a3-98c4-f8eb81df46bf | d647355e-b019-4de0-999e-00107531edc0 | 192.168.34.153 | fa:16:3e:91:4e:00 |
+------------+--------------------------------------+--------------------------------------+----------------+-------------------+

########################################################################################################################  
openstack vxlan迁移  适配 cisico acpi 


A --Configure the controller node.

a) Install the apicapi module.
pip install  apicapi

b) add the following configuration files to the neutron service
vim /etc/systemd/system/multi-user.target.wants/neutron-server.service
[Unit]
Description=OpenStack Neutron Server
After=syslog.target network.target

[Service]
Type=notify
User=neutron
#ExecStart=/usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini --log-file /var/log/neutron/server.log
ExecStart=/usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf --config-file
/etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini --log-file
/var/log/neutron/server.log --config-file /etc/neutron/plugins/ml2/ml2_conf.ini --config-file
/etc/neutron/plugins/ml2/ml2_conf_cisco.ini
PrivateTmp=true
NotifyAccess=all
KillMode=process

[Install]
WantedBy=multi-user.target

c) Create the APIC service agent as follows
cat >/usr/bin/neutron-cisco-apic-service-agent <<EOF
import sys
from neutron.plugins.ml2.drivers.cisco.apic.apic_topology import service_main
if __name__ == "__main__":
sys.exit(service_main())
EOF

d) Set up the APIC service agent.
cat >/etc/systemd/system/multi-user.target.wants/neutron-cisco-apic-service-agent.service <<EOF
[Unit]
Description=OpenStack APIC Service Agent
After=syslog.target network.target
[Service]
Type=simple
User=neutron
ExecStart=/usr/bin/neutron-cisco-apic-service-agent --config-file=/etc/neutron/neutron.conf
--config-file=/etc/neutron/plugins/ml2/ml2_conf_cisco.ini
--log-file=/var/log/neutron/cisco-apic-service-agent.log
PrivateTmp=false
KillMode=process
[Install]
WantedBy=multi-user.target
EOF

###############################################################

B--Configure the network and compute nodes.
a) Install and set up LLDP if you plan to use automatic host discovery.
wget http://media.luffy.cx/files/lldpd/lldpd-0.7.11.tar.gz --no-check-certificate
tar zxvf lldpd-0.7.11.tar.gz
cd lldpd-0.7.11
mkdir build
cd build
../configure
make install
cp /usr/local/sbin/lldp* /usr/sbin/

a) Create the LLDP runtime environment.
Example:
useradd -s /sbin/nologin _lldpd
mkdir -p /var/run/lldpd
chown root:root /var/run/lldpd
mkdir -p /usr/local/var/run
chown root:root /usr/local/var/run

b) Set up the LLDP service.
sed -ie 's/\@sbindir\@\/lldpd/\/usr\/local\/sbin\/lldpd/' /usr/lib/systemd/system/lldpd.service
systemctl daemon-reload
chkconfig lldpd on
systemctl start lldpd


cat >/usr/bin/neutron-cisco-apic-host-agent <<EOF
import sys
from neutron.plugins.ml2.drivers.cisco.apic.apic_topology import aget_main
if __name__ == "__main__":
sys.exit(service_main())
EOF

c) Set up the APIC host agent service.
cat >/etc/systemd/system/multi-user.target.wants/neutron-cisco-apic-host-agent.service <<EOF
[Unit]
Description=OpenStack APIC Host Agent
After=syslog.target network.target
[Service]
Type=simple
User=neutron
ExecStart=/usr/bin/neutron-cisco-apic-host-agent --config-file=/etc/neutron/neutron.conf
--config-file=/etc/neutron/plugins/ml2/ml2_conf_cisco.ini
--log-file=/var/log/neutron/cisco-apic-host-agent.log
PrivateTmp=false
KillMode=process
[Install]
Wanted=multi-user.target
EOF


d) Update the /etc/systemd/system/multi-user.target.wants/neutron-openvswitch-agent.service file to look like the following.
cat >/etc/systemd/system/multi-user.target.wants/neutron-openvswitch-agent.service << EOF
[Unit]
Description=OpenStack Neutron Open vSwitch Agent
After=syslog.target network.target
[Service]
Type=simple
User=neutron
#ExecStart=/usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf 
#--config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini --log-file /var/log/neutron/openvswitch-agent.log


ExecStart=/usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf
--config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini
--log-file /var/log/neutron/openvswitch-agent.log
PrivateTmp=true
KillMode=process
[Install]
WantedBy=multi-user.target
EOF

########################################################################################################################
Configuring the Cisco APIC Driver
 /etc/neutron/plugins/ml2/ml2_conf.ini
 
[ml2]
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
mechanism_drivers =  openvswitch,cisco_apic

[ml2_type_vlan]
vni_ranges = 1001:2000

[ovs]
local_ip = 10.20.8.33
tenant_network_type = vxlan
integration_bridge = br-int
network_vxlan_ranges = physnet1:500:599
#bridge_mappings = physnet1:br-eth0
bridge_mappings = physnet1:br-ex


Setup the /etc/neutron/plugins/ml2/ml2_conf_cisco.ini. This can include the following parameters.
[DEFAULT]
apic_system_id=openstack
[ml2_cisco_apic]
apic_hosts=10.10.0.123
apic_username=admin
apic_password=password
apic_name_mapping=use_name
apic_vpc_pairs=201:202,203:204
root_helper='sudo'
[apic_external_network:ext]
switch=203
port=1/34
cidr_exposed=192.168.0.2/24
gateway_ip=192.168.0.1
#note: optional and needed only for manual configuration
[apic_switch:201]
compute11,compute21=1/10
compute12=1/11
[apic_switch:202]
compute11,compute21=1/20
compute12=1/21


[ml2_cisco_apic]

# Hostname:port list of APIC controllers
# apic_hosts = 1.1.1.1:80, 1.1.1.2:8080, 1.1.1.3:80

# Username for the APIC controller
# apic_username = user

# Password for the APIC controller
# apic_password = password

apic_system_id=openstack


[apic_external_network:network_ext]
switch=203
port=1/34
encap=vlan-100
cidr_exposed=10.10.40.2/16
gateway_ip=10.10.40.1


a) Create an external network through Openstack (note the shared parameter is optional but the name must match the
name specified in the configuration file). The name of the network MUST match the name you use with
apic_external_network field in the configuration file.

neutron net-create network_ext --shared --router:external=True  

b) Create a subnet for the network. It would contain the gateway_ip you selected.
neutron subnet-create network_ext --name extsub 192.168.0.0/24

c) Create a network for the tenant on the routed subnet.
neutron net-create net1
Neutron subnet-create net1 –name net1sub 10.100.0.0/24

d) Create a neutron router and set its gateway. Note you should use net1's ID rather than name.

neutron router-create myrouter
neutron router-gateway-set myrouter <ext-ID>
neturon router-interface-add myrouter <net1-ID>

e)Configure the ML2 file for the network/compute nodes. The parameter "physnet1" in the ml2 configuration file may be
anythingyouchoosebutmustbethesameintheml2_type_vlanandovssectionsofthefile.Checkthatthebr-eth
interface is already created and add an external interface to it (example is ens33 here). It is assumed ens33 is UP and
attached to the ACI fabric.
ovs-vsctl add-port br-eth ens33

########################################################################################################################  
ovs 和linux-bridge 的区别是 linux-bridge是将不通的tap连接到不同的网桥上面而ovs是全部连接到br-int上
namespace:
  1.宿主机本身也是一个namespace 叫做root namespace 拥有所有物理和虚拟interface device 物理interface只能位于rootnamespace中
  2.新建的namespace默认只有一个lookback device 管理员可以将虚拟interface 例如bridge tap设备放到某个namespace中
  3.对于 flat_net 的 DHCP 设备 tap19a0ed3d-fe，需要将其放到 namespace qdhcp-f153b42f-c3a1-4b6c-8865-c09b5b2aa274 中，但这样会带来一个问题： tap19a0ed3d-fe 将无法直接与 root namespace 中的 bridge 设备 brqf153b42f-c3 连接。
  4.Neutron 使用veth pair解决了这个问题
    veth  pair是一种成对出现的特殊网络设备，他们像一根虚拟网线，可以用于连接两个namespace
  5.向veth pair的一段输入数据在另一端就能读到此数据
  
########################################################################################################################
虚拟路由
[root@dzc-o-neutron01v ~]# ip netns ls
qrouter-d68a4bca-58f2-4242-b626-ae86bc9f427a
qdhcp-50f5cafb-91c2-48f2-8af2-8f85a1b3d31c
qdhcp-d647355e-b019-4de0-999e-00107531edc0
qdhcp-ba31704e-c1b6-4a43-8eb6-6162d25e7b29

ip netns exec qrouter-d68a4bca-58f2-4242-b626-ae86bc9f427a bash

[root@dzc-o-neutron01v ~]# ifconfig 
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 0  (Local Loopback)
        RX packets 8  bytes 784 (784.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 8  bytes 784 (784.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

qr-ef171f95-74: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.34.254  netmask 255.255.255.0  broadcast 192.168.34.255
        inet6 fe80::f816:3eff:fe65:381  prefixlen 64  scopeid 0x20<link>
        ether fa:16:3e:65:03:81  txqueuelen 0  (Ethernet)
        RX packets 238  bytes 15990 (15.6 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 51  bytes 2586 (2.5 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

qr-fd30d844-47: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.35.1  netmask 255.255.255.0  broadcast 192.168.35.255
        inet6 fe80::f816:3eff:fe2d:2cf8  prefixlen 64  scopeid 0x20<link>
        ether fa:16:3e:2d:2c:f8  txqueuelen 0  (Ethernet)
        RX packets 146  bytes 10362 (10.1 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 10  bytes 864 (864.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

		
[root@dzc-o-neutron01v ~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.34.0    0.0.0.0         255.255.255.0   U     0      0        0 qr-ef171f95-74
192.168.35.0    0.0.0.0         255.255.255.0   U     0      0        0 qr-fd30d844-47		


########################################################################################################################  
neutron external network
当租户网络连接到neutron router 通常将router 作为默认网关
当 router 接收到 instance 的数据包，并将其转发到外网时:
1.router会修改包的源地址为自己的外网地址 ，这样确保数据包这样确保数据包转发到外网，并能够从外网返回。
2.router 修改返回的数据包，并转发给真正的 instance。
这个行为被称作 Source NAT。
如果需要从外网直接访问 instance，则可以利用 floating IP。
下面是关于 floating IP 必须知道的事实：

1. floating IP 提供静态 NAT 功能，建立外网 IP 与 instance 租户网络 IP 的一对一映射。
2. floating IP 是配置在 router 提供网关的外网 interface 上的，而非 instance 中。
3. router 会根据通信的方向修改数据包的源或者目的地址。

########################################################################################################################
L2 Population 的作用是在 VTEP 上提供 Porxy ARP 功能，使得 VTEP 能够预先获知 VXLAN 网络中如下信息：
1. VM IP -- MAC 对应关系
2. VM -- VTEP 的对应关系

当 VM A 需要与 VM G 通信时：
1. Host 1 上的 VTEP 直接响应 VM A 的 APR 请求，告之 VM G 的 MAC 地址。
2. 因为 Host 1 上的 VTEP 知道 VM G 位于 Host 4，会将封装好的 VXLAN 数据包直接发送给 Host 4 的 VTEP。

这样就解决了 MAC 地址学习和 APR 广播的问题，从而保证了 VXLAN 的 Scalability。

那么下一个关键问题是：
VTEP 是如何提前获知 IP -- MAC -- VTEP 相关信息的呢？
答案是：

1.Neutron 知道每一个 port 的状态和信息； port 保存了 IP，MAC 相关数据。

2.instance 启动时，其 port 状态变化过程为：down -> build -> active。

3.每当 port 状态发生变化时，Neutron 都会通过 RPC 消息通知各节点上的 Neutron agent，使得 VTEP 能够更新 VM 和 port 的相关信息。

4.VTEP 可以根据这些信息判断出其他 Host 上都有哪些 VM，以及它们的 MAC 地址，这样就能直接与之通信，从而避免了不必要的隧道连接和广播。

理解了工作原理，下节我们学习如何在 Neutorn 中配置 L2 Population。

在 /etc/neutron/plugins/ml2/ml2_conf.ini 设置 l2population mechanism driver。
mechanism_drivers = linuxbridge,l2population
同时在 [VXLAN] 中配置 enable L2 Population。
l2_population =True
L2 Population 生效后，创建的 vxlan-100 会多一个 Proxy ARP 功能。
########################################################################################################################  
安全组的应用对象是虚拟网卡，由 L2 Agent 实现，比如 neutron_openvswitch_agent 和 neutron_linuxbridge_agent。
安全组会在计算节点上通过 iptables 规则来控制进出 instance 虚拟网卡的流量。
也就是说：安全组保护的是 instance。

FWaaS 的应用对象是 router，可以在安全组之前控制外部过来的流量，但是对于同一个 subnet 内的流量不作限制。
也就是说：FWaaS 保护的是 subnet。

所以，可以同时部署 FWaaS 和安全组实现双重防护。
########################################################################################################################
那问题来了，为什么 tapfc1c6ebb-71 不能像左边的 DHCP 设备 tap7970bdcd-f2 那样直接连接到 br-int 呢？

其原因是： Open vSwitch 目前还不支持将 iptables 规则放在与它直接相连的 tap 设备上。

如果做不到这一点，就无法实现 Security Group 功能。 为了支持 Security Group，不得不多引入一个 Linux Bridge 支持 iptables。

这样的后果就是网络结构更复杂了，路径上多了一个 linux bridge 和 一对 veth pair 设备。

first_local_net 相关 port 其 tag 为 1； second_local_net 相关 port 其 tag 为 2。

玄机就在这里了： Open vSwitch 的每个网桥都可以看作一个真正的交换机，可以支持 VLAN，这里的 tag 就是 VLAN ID。

br-int 中标记 tag 1 的 port 和 标记 tag 2 的 port 分别属于不同的 VLAN，它们之间是隔离的。

需要特别说明的是： Open vSwitch 中的 tag 是内部 VLAN，用于隔离网桥中的 port，与物理网络中的 VLAN 没有关系。
看来 veth pair 和 patch port 都可以连接网桥，使用的时候如何选择呢？

patch port 是 ovs bridge 自己特有的 port 类型，只能在 ovs 中使用。
如果是连接两个 ovs bridge，优先使用 patch port，因为性能更好。
所以：
1. 连接两个 ovs bridge，优先使用 patch port。技术上veth pair 也能实现，但性能不如 patch port。
2. 连接 ovs bridge 和 linux bridge，只能使用 veth pair。
3. 连接两个 linux bridge，只能使用 veth pair。

########################################################################################################################  
要使nova console-log 能将实例启动过程输出到实例启动日志中，要在文件/boot/grub/menu.lst 中kernel参数中增加下面的内容:
kernel /vmlinuz-2.6.32-431.el6.x86_64 ro root=UUID=20c0dfb2-1e62-43d3-a5c1-f7617c157dc3 rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet  
在kernel 行添加下面内容
console=tty0 console=ttyS0,115200n8

#nova-api中的日志
2017-01-18 11:19:56.894 28755 DEBUG nova.api.openstack.wsgi [req-a9bef796-1752-4eb8-ba75-d4ffee92ac81 None] Action: 'action', calling method: <bound method ConsoleOutputController.get_console_output of <nova.api.openstack.compute.contrib.console_output.ConsoleOutputController object at 0x394a2d0>>, body: {"os-getConsoleOutput": {"length": 35}} _process_stack /usr/lib/python2.7/site-packages/nova/api/openstack/wsgi.py:934



#计算节点上的日志
2017-01-18 11:19:56.998 13766 AUDIT nova.compute.manager [req-a9bef796-1752-4eb8-ba75-d4ffee92ac81 None] [instance: 4b5acd5c-6a26-4145-80bb-54bfb42a4d99] Get console output
2017-01-18 11:19:57.003 13766 DEBUG nova.openstack.common.processutils [req-a9bef796-1752-4eb8-ba75-d4ffee92ac81 None] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf chown 162 /var/lib/nova/instances/4b5acd5c-6a26-4145-80bb-54bfb42a4d99/console.log execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:161
2017-01-18 11:19:57.096 13766 DEBUG nova.openstack.common.processutils [req-a9bef796-1752-4eb8-ba75-d4ffee92ac81 None] Result was 0 execute /usr/lib/python2.7/site-packages/nova/openstack/common/processutils.py:195
########################################################################################################################

LLDP协议介绍:

简单说来，LLDP是一种邻近发现协议。它为以太网网络设备，如交换机、路由器和无线局域网接入点定义了一种标准的方法，使其可以向网络中其他节点公告自身的存在，并保存各个邻近设备的发现信息。例如设备配置和设备识别等详细信息都可以用该协议进行公告。

具体来说，LLDP定义了一个通用公告信息集、一个传输公告的协议和一种用来存储所收到的公告信息的方法。要公告自身信息的设备可以将多条公告信息放在一个局域网数据包内传输，传输的形式为类型长度值（TLV）域。

LLDP应用

    服务器上安装lldp之后可以通过lldp确定服务器所连接交换机端口，并且交换机也可以通过lldp确认端口所连接的服务器。

    且在服务器交换机连线较复杂时能很快定位服务器及交换机的连接关系！
	
	
[DEFAULT]
apic_system_id=openstack
[ml2_cisco_apic]
apic_hosts=10.10.0.123
apic_username=admin
apic_password=password
apic_name_mapping=use_name
apic_vpc_pairs=201:202,203:204
root_helper='sudo'
[apic_external_network:ext]
switch=203
port=1/34
cidr_exposed=192.168.0.2/24
gateway_ip=192.168.0.1


#note: optional and needed only for manual configuration
[apic_switch:201]
compute11,compute21=1/10
compute12=1/11
[apic_switch:202]
compute11,compute21=1/20
compute12=1/21	


########################################################################################################################

创建虚拟机时 ,nova最终会调用到类 LibvirtDriver 的 spawn函数
1.获取instance的磁盘映射信息
2.创建客户机的虚拟磁盘镜像，并把需要的信息写入磁盘镜像中，包含网络，磁盘信息。管理员密码和必须的文件
3.将客户机的配置信息生成 xml文件
4.创建一个计时器，每隔0.5秒检查新建的虚拟机是否启动
5.调用一个 _create_domain_and_network函数 通过调用 _create_domain 创建虚拟机

nova/virt/libvirt/driver.py 
    def spawn(self, context, instance, image_meta, injected_files,
              admin_password, network_info=None, block_device_info=None):
		#获取instance的磁盘映射信息	  
        disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
                                            instance,
                                            block_device_info,
                                            image_meta)
		#创建客户机的虚拟磁盘镜像，并把需要的信息写入磁盘镜像中
        #包含网络，磁盘信息。管理员密码和必须的文件		
        self._create_image(context, instance,
                           disk_info['mapping'],
                           network_info=network_info,
                           block_device_info=block_device_info,
                           files=injected_files,
                           admin_pass=admin_password)
		#将客户机的配置信息生成 xml文件				   
        xml = self._get_guest_xml(context, instance, network_info,
                                  disk_info, image_meta,
                                  block_device_info=block_device_info,
                                  write_to_disk=True)
		#创建网络并启动虚拟机						  
        self._create_domain_and_network(context, xml, instance, network_info,
                                        block_device_info, disk_info=disk_info)
        LOG.debug("Instance is running", instance=instance)
        #创建一个计时器，每隔0.5秒检查新建的虚拟机是否启动
        def _wait_for_boot():
            """Called at an interval until the VM is running."""
            state = self.get_info(instance)['state']

            if state == power_state.RUNNING:
                LOG.info(_LI("Instance spawned successfully."),
                         instance=instance)
                raise loopingcall.LoopingCallDone()

        timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
        timer.start(interval=0.5).wait()
 
 
 
    def _create_domain(self, xml=None, domain=None,
                       instance=None, launch_flags=0, power_on=True):
        """Create a domain.

        Either domain or xml must be passed in. If both are passed, then
        the domain definition is overwritten from the xml.
        """
        err = None
        try:
		    #根据xml文件配置 ，定义一个虚拟机 但不启动
            if xml:
                err = _LE('Error defining a domain with XML: %s') % xml
                domain = self._conn.defineXML(xml)
            #启动前面定义的虚拟机，如果成功就从已定义集合移到正运行的集合
            if power_on:
                err = _LE('Error launching a defined domain with XML: %s') \
                          % strutils.safe_decode(domain.XMLDesc(0),
                                                 errors='ignore')
                domain.createWithFlags(launch_flags)
            #获得此虚拟机对应的xml描述，可用于将来重启此虚拟机
            if not utils.is_neutron():
                err = _LE('Error enabling hairpin mode with XML: %s') \
                          % strutils.safe_decode(domain.XMLDesc(0),
                                                 errors='ignore')
                self._enable_hairpin(domain.XMLDesc(0))
        except Exception:
            with excutils.save_and_reraise_exception():
                if err:
                    LOG.error(err)

        return domain 
		
nova/compute/rpcapi.py
	class ComputeAPI(object):
    '''Client side of the compute rpc API.'''
	    def live_migration(self, ctxt, instance, dest, block_migration, host,
                       migration, migrate_data=None):
        args = {'migration': migration}
        version = '4.2'
        if not self.client.can_send_version(version):
            version = '4.0'
            args.pop('migration')
        cctxt = self.client.prepare(server=host, version=version)
		#RPC case主要用于一步形式，比如创建虚拟机，创建过程需要很长时间 
		#live_migration是调用的函数
        cctxt.cast(ctxt, 'live_migration', instance=instance,
                   dest=dest, block_migration=block_migration,
                   migrate_data=migrate_data, **args)
#类ComputeAPI中的函数即为Compute服务提供给RPC调用的接口，

nova/conductor/tasks/live_migrate.py 

class LiveMigrationTask(base.TaskBase):
    def __init__(self, context, instance, destination,
                 block_migration, disk_over_commit, migration, compute_rpcapi,
                 servicegroup_api, scheduler_client):
        super(LiveMigrationTask, self).__init__(context, instance)
        self.destination = destination
        self.block_migration = block_migration
        self.disk_over_commit = disk_over_commit
        self.migration = migration
        self.source = instance.host
        self.migrate_data = None

        self.compute_rpcapi = compute_rpcapi
        self.servicegroup_api = servicegroup_api
        self.scheduler_client = scheduler_client

    def _execute(self):
        self._check_instance_is_active()
        self._check_host_is_up(self.source)

        if not self.destination:
            self.destination = self._find_destination()
            self.migration.dest_compute = self.destination
            self.migration.save()
        else:
            self._check_requested_destination()

        # TODO(johngarbutt) need to move complexity out of compute manager
        # TODO(johngarbutt) disk_over_commit?
		##调用 ComputeAPI类中提供的RPC接口
        return self.compute_rpcapi.live_migration(self.context,
                host=self.source,
                instance=self.instance,
                dest=self.destination,
                block_migration=self.block_migration,
                migration=self.migration,
                migrate_data=self.migrate_data)

#类ComputeAPI只是暴露给其它服务的RPC调用接口 ，Compute服务的RPC Server接受到RPC请求后，真正完成任务的是nova.compute.manager模块
nova/compute/manager.py 
class ComputeManager(manager.Manager):
    """Manages the running instances from creation to destruction."""

    target = messaging.Target(version='4.5')

    # How long to wait in seconds before re-issuing a shutdown
    # signal to an instance during power off.  The overall
    # time to wait is set by CONF.shutdown_timeout.
    SHUTDOWN_RETRY_INTERVAL = 10

    def __init__(self, compute_driver=None, *args, **kwargs):
        """Load configuration options and connect to the hypervisor."""
        self.virtapi = ComputeVirtAPI(self)
        self.network_api = network.API()
        self.volume_api = volume.API()
        self.image_api = image.API()				

从computeAPI到ComputeManager的过程即是RPC调用过程

5.2 nova api		
nova api 作为客户机和nova之间的中间层，nova api扮演了一个桥梁
1. novaclient将用户的命令转换为标准的http请求
2. paste deploy将请求路由到具体的wsgi app  
3. routes将请求路由的具体的函数阶段

4. nova client 默认使用的是v2 api
5. 在nova list 中可以看到 ，共发送了两个http请求 ，第一个请求发送给keystone 获取授权，任何请求都要经过授权才可以使用，因此对nova api的调用都要首先从keystone 拿到一个授权的token
   
6. 这个就是token   "id": "{SHA1}925bfdd940d80f2392a3344d9456499e3e521307"
[root@dzc-o-control01v ~]# nova  --debug list
REQ: curl -i 'http://controller.light.fang.com:35357/v2.0/tokens' -X POST -H "Accept: application/json" -H "Content-Type: application/json" -H "User-Agent: python-novaclient" -d '{"auth": {"tenantName": "admin", "passwordCredentials": {"username": "admin", "password": "{SHA1}2fbf0b4a50c58cfa6d5ba9bd23fa086f2d3c6ff8"}}}'
RESP: [200] {'date': 'Tue, 14 Feb 2017 01:41:53 GMT', 'vary': 'X-Auth-Token', 'content-length': '2722', 'content-type': 'application/json', 'connection': 'keep-alive'}
RESP BODY: {"access": {"token": {"issued_at": "2017-02-14T01:41:53.315204", "expires": "2017-02-14T02:41:53Z", "id": "{SHA1}925bfdd940d80f2392a3344d9456499e3e521307", "tenant": {"enabled": true, "description": "Admin Tenant", "name": "admin", "id": "e599088c985f42e7948b12f601705cd3"}, "audit_ids": ["Ai-rfAVhR1Ozt9RTKJn-6g"]}, "serviceCatalog": [{"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3", "internalURL": "http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3", "id": "05754bb535f04d58b4f2a6819ea31a67"}], "type": "compute", "name": "nova"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:9696", "region": "regionOne", "publicURL": "http://controller.light.fang.com:9696", "internalURL": "http://controller.light.fang.com:9696", "id": "75d7d2b94fd14637a990056bf95ecec6"}], "type": "network", "name": "neutron"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "internalURL": "http://controller.light.fang.com:8776/v2/e599088c985f42e7948b12f601705cd3", "id": "37eb9ae73b72467b8695247873083335"}], "type": "volumev2", "name": "cinderv2"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:9292", "region": "regionOne", "publicURL": "http://controller.light.fang.com:9292", "internalURL": "http://controller.light.fang.com:9292", "id": "01461acd46ea449d93b38a730b721462"}], "type": "image", "name": "glance"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "region": "regionOne", "publicURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "internalURL": "http://controller.light.fang.com:8776/v1/e599088c985f42e7948b12f601705cd3", "id": "30d052b7f62344dabab8c29928d29f99"}], "type": "volume", "name": "cinder"}, {"endpoints_links": [], "endpoints": [{"adminURL": "http://controller.light.fang.com:35357/v2.0", "region": "regionOne", "publicURL": "http://controller.light.fang.com:5000/v2.0", "internalURL": "http://controller.light.fang.com:5000/v2.0", "id": "3e889bea96a34b55ad1ff2061f637851"}], "type": "identity", "name": "keystone"}], "user": {"username": "admin", "roles_links": [], "id": "7d5b5abc30ea463690567e5f8cc794f9", "roles": [{"name": "admin"}], "name": "admin"}, "metadata": {"is_admin": 0, "roles": ["936f0bbe4674464fa2a0416734721317"]}}}

7. 基于这个授权发送第二个请求 http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3/servers/detail来获取虚拟机列表
REQ: curl -i 'http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3/servers/detail' -X GET -H "Accept: application/json" -H "User-Agent: python-novaclient" -H "X-Auth-Project-Id: admin" -H "X-Auth-Token: {SHA1}925bfdd940d80f2392a3344d9456499e3e521307"
RESP: [200] {'date': 'Tue, 14 Feb 2017 01:41:53 GMT', 'connection': 'keep-alive', 'content-type': 'application/json', 'content-length': '5464', 'x-compute-request-id': 'req-e53e7246-edee-4b4f-abb4-e56d0629684a'}
RESP BODY: {"servers": [{"OS-EXT-STS:task_state": null, "addresses": {"DMZ_NET": [{"OS-EXT-IPS-MAC:mac_addr": "fa:16:3e:78:cd:b2", "version": 4, "addr": "192.168.34.172", "OS-EXT-IPS:type": "fixed"}]}, "links": [{"href": "http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3/servers/4d0eae47-9ab3-4ad4-80ee-6c58bdf65de2", "rel": "self"}, {"href": "http://controller.light.fang.com:8774/e599088c985f42e7948b12f601705cd3/servers/4d0eae47-9ab3-4ad4-80ee-6c58bdf65de2", "rel": "bookmark"}], "image": {"id": "17ed0c0a-55a6-4044-8ecf-8af8844b4199", "links": [{"href": "http://controller.light.fang.com:8774/e599088c985f42e7948b12f601705cd3/images/17ed0c0a-55a6-4044-8ecf-8af8844b4199", "rel": "bookmark"}]}, "OS-EXT-STS:vm_state": "active", "OS-EXT-SRV-ATTR:instance_name": "instance-00000141", "OS-SRV-USG:launched_at": "2017-02-13T08:09:19.000000", "flavor": {"id": "621313b5-1e0b-4819-a33b-68990cbb93ca", "links": [{"href": "http://controller.light.fang.com:8774/e599088c985f42e7948b12f601705cd3/flavors/621313b5-1e0b-4819-a33b-68990cbb93ca", "rel": "bookmark"}]}, "id": "4d0eae47-9ab3-4ad4-80ee-6c58bdf65de2", "security_groups": [{"name": "default"}], "user_id": "7d5b5abc30ea463690567e5f8cc794f9", "OS-DCF:diskConfig": "AUTO", "accessIPv4": "", "accessIPv6": "", "progress": 0, "OS-EXT-STS:power_state": 1, "OS-EXT-AZ:availability_zone": "dzc", "config_drive": "", "status": "ACTIVE", "updated": "2017-02-13T08:09:19Z", "hostId": "a4a919f8f6bea8b617181a095c465173cd3bf52994b050fc2381e8a9", "OS-EXT-SRV-ATTR:host": "sjhl-o-compute05", "OS-SRV-USG:terminated_at": null, "key_name": null, "OS-EXT-SRV-ATTR:hypervisor_hostname": "sjhl-o-compute05", "name": "7-test", "created": "2017-02-13T08:08:45Z", "tenant_id": "e599088c985f42e7948b12f601705cd3", "os-extended-volumes:volumes_attached": [], "metadata": {}}, {"OS-EXT-STS:task_state": null, "addresses": {"DMZ_NET": [{"OS-EXT-IPS-MAC:mac_addr": "fa:16:3e:0a:db:6b", "version": 4, "addr": "192.168.34.170", "OS-EXT-IPS:type": "fixed"}]}, "links": [{"href": "http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3/servers/3807cfc7-7ca1-4eaf-ae53-60ef39e4e294", "rel": "self"}, {"href": "http://controller.light.fang.com:8774/e599088c985f42e7948b12f601705cd3/servers/3807cfc7-7ca1-4eaf-ae53-60ef39e4e294", "rel": "bookmark"}], "image": {"id": "17ed0c0a-55a6-4044-8ecf-8af8844b4199", "links": [{"href": "http://controller.light.fang.com:8774/e599088c985f42e7948b12f601705cd3/images/17ed0c0a-55a6-4044-8ecf-8af8844b4199", "rel": "bookmark"}]}, "OS-EXT-STS:vm_state": "active", "OS-EXT-SRV-ATTR:instance_name": "instance-0000013c", "OS-SRV-USG:launched_at": "2017-02-07T08:18:03.000000", "flavor": {"id": "621313b5-1e0b-4819-a33b-68990cbb93ca", "links": [{"href": "http://controller.light.fang.com:8774/e599088c985f42e7948b12f601705cd3/flavors/621313b5-1e0b-4819-a33b-68990cbb93ca", "rel": "bookmark"}]}, "id": "3807cfc7-7ca1-4eaf-ae53-60ef39e4e294", "security_groups": [{"name": "default"}], "user_id": "7d5b5abc30ea463690567e5f8cc794f9", "OS-DCF:diskConfig": "AUTO", "accessIPv4": "", "accessIPv6": "", "progress": 0, "OS-EXT-STS:power_state": 1, "OS-EXT-AZ:availability_zone": "dzc", "config_drive": "", "status": "ACTIVE", "updated": "2017-02-07T08:18:03Z", "hostId": "3b6df4615b808d96d3d2e68f8dc7f1219b1d60316e42c82195be91be", "OS-EXT-SRV-ATTR:host": "sjhl-o-compute06", "OS-SRV-USG:terminated_at": null, "key_name": null, "OS-EXT-SRV-ATTR:hypervisor_hostname": "sjhl-o-compute06", "name": "5-3807cfc7-7ca1-4eaf-ae53-60ef39e4e294", "created": "2017-02-07T08:17:04Z", "tenant_id": "e599088c985f42e7948b12f601705cd3", "os-extended-volumes:volumes_attached": [], "metadata": {}}, {"OS-EXT-STS:task_state": null, "addresses": {"DMZ_NET": [{"OS-EXT-IPS-MAC:mac_addr": "fa:16:3e:5c:b8:b6", "version": 4, "addr": "192.168.34.160", "OS-EXT-IPS:type": "fixed"}]}, "links": [{"href": "http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3/servers/0ceaabf7-9618-4cff-b45b-6ab68a0a0421", "rel": "self"}, {"href": "http://controller.light.fang.com:8774/e599088c985f42e7948b12f601705cd3/servers/0ceaabf7-9618-4cff-b45b-6ab68a0a0421", "rel": "bookmark"}], "image": {"id": "17ed0c0a-55a6-4044-8ecf-8af8844b4199", "links": [{"href": "http://controller.light.fang.com:8774/e599088c985f42e7948b12f601705cd3/images/17ed0c0a-55a6-4044-8ecf-8af8844b4199", "rel": "bookmark"}]}, "OS-EXT-STS:vm_state": "active", "OS-EXT-SRV-ATTR:instance_name": "instance-0000012c", "OS-SRV-USG:launched_at": "2017-02-07T07:12:17.000000", "flavor": {"id": "621313b5-1e0b-4819-a33b-68990cbb93ca", "links": [{"href": "http://controller.light.fang.com:8774/e599088c985f42e7948b12f601705cd3/flavors/621313b5-1e0b-4819-a33b-68990cbb93ca", "rel": "bookmark"}]}, "id": "0ceaabf7-9618-4cff-b45b-6ab68a0a0421", "security_groups": [{"name": "default"}], "user_id": "7d5b5abc30ea463690567e5f8cc794f9", "OS-DCF:diskConfig": "AUTO", "accessIPv4": "", "accessIPv6": "", "progress": 0, "OS-EXT-STS:power_state": 1, "OS-EXT-AZ:availability_zone": "dzc", "config_drive": "", "status": "ACTIVE", "updated": "2017-02-13T06:44:24Z", "hostId": "ce432fd612a3c8ca3eeac139af830c0d07bd764a84dcf2c14f9eced4", "OS-EXT-SRV-ATTR:host": "sjhl-o-compute09", "OS-SRV-USG:terminated_at": null, "key_name": null, "OS-EXT-SRV-ATTR:hypervisor_hostname": "sjhl-o-compute09", "name": "dzc-open-salt", "created": "2017-02-07T07:11:40Z", "tenant_id": "e599088c985f42e7948b12f601705cd3", "os-extended-volumes:volumes_attached": [], "metadata": {}}]}

+--------------------------------------+----------------------------------------+--------+------------+-------------+------------------------+
| ID                                   | Name                                   | Status | Task State | Power State | Networks               |
+--------------------------------------+----------------------------------------+--------+------------+-------------+------------------------+
| 3807cfc7-7ca1-4eaf-ae53-60ef39e4e294 | 5-3807cfc7-7ca1-4eaf-ae53-60ef39e4e294 | ACTIVE | -          | Running     | DMZ_NET=192.168.34.170 |
| 4d0eae47-9ab3-4ad4-80ee-6c58bdf65de2 | 7-test                                 | ACTIVE | -          | Running     | DMZ_NET=192.168.34.172 |
| 0ceaabf7-9618-4cff-b45b-6ab68a0a0421 | dzc-open-salt                          | ACTIVE | -          | Running     | DMZ_NET=192.168.34.160 |
+--------------------------------------+----------------------------------------+--------+------------+-------------+------------------------+

8. http请求到 wsgi application 
   nova api服务 nova-api在启动时，会根据nova配置文件的enabled_api选项内容创建一个或多个wsgi server 
/etc/nova/nova.conf 
# A list of APIs to enable by default (list value)
enabled_apis=ec2,osapi_compute,metadata
    paste deploy 会在各个wsgi server创建时参与进来，基于paste配置文件/etc/nova/api-paste.ini去加载wsgi app 
 nova/service.py 
  class WSGIService(service.Service):
    """Provides ability to launch API from a 'paste' configuration."""

    def __init__(self, name, loader=None, use_ssl=False, max_url_len=None):
        """Initialize, but do not start the WSGI server.

        :param name: The name of the WSGI server given to the loader.
        :param loader: Loads the WSGI application using the given name.
        :returns: None

        """
        self.name = name
        self.manager = self._get_manager()
        self.loader = loader or wsgi.Loader()
		#从paste配置文件加载api对应的wsgi app 
        self.app = self.loader.load_app(name)
        # inherit all compute_api worker counts from osapi_compute
        if name.startswith('openstack_compute_api'):
            wname = 'osapi_compute'
        else:
            wname = name
        self.host = getattr(CONF, '%s_listen' % name, "0.0.0.0")
        self.port = getattr(CONF, '%s_listen_port' % name, 0)
        self.workers = (getattr(CONF, '%s_workers' % wname, None) or
                        processutils.get_worker_count())
        if self.workers and self.workers < 1:
            worker_name = '%s_workers' % name
            msg = (_("%(worker_name)s value of %(workers)s is invalid, "
                     "must be greater than 0") %
                   {'worker_name': worker_name,
                    'workers': str(self.workers)})
            raise exception.InvalidInput(msg)
        self.use_ssl = use_ssl
		#nova 中使用了 evenlet对wsgi 进行了封装 在监听到一个http请求时并不会新建一个独立的线程去处理，而是交给邪成处理
        self.server = wsgi.Server(name,
                                  self.app,
                                  host=self.host,
                                  port=self.port,
                                  use_ssl=self.use_ssl,
                                  max_url_len=max_url_len)
        # Pull back actual port used
        self.port = self.server.port
        self.backdoor_port = None
 
 /etc/nova/api-paste.ini
[composite:osapi_compute]
use = call:nova.api.openstack.urlmap:urlmap_factory
/: oscomputeversions
/v1.1: openstack_compute_api_v2
/v2: openstack_compute_api_v2
/v2.1: openstack_compute_api_v21
/v3: openstack_compute_api_v3
 实例中的http请求为 http://controller.light.fang.com:8774/v2/e599088c985f42e7948b12f601705cd3/servers/detail
 使用了openstack v2API ，wsgi服务器 osapi_compute将监听到的这个http请求，并使用 nova.api.openstack.urlmap模块的urlmap_factory函数进行分发
 
[composite:openstack_compute_api_v2]
use = call:nova.api.auth:pipeline_factory
noauth = compute_req_id faultwrap sizelimit noauth ratelimit osapi_compute_app_v2
keystone = compute_req_id faultwrap sizelimit authtoken keystonecontext ratelimit osapi_compute_app_v2
keystone_nolimit = compute_req_id faultwrap sizelimit authtoken keystonecontext osapi_compute_app_v2

#根据配置文件中认证策略 auth_strategy 选择keystone 或者noauth 
# The strategy to use for auth: noauth or keystone. (string value)
auth_strategy=keystone
参数 noauth 和keystone 都对应了一个pipeline 除了最后一个osapi_compute_app_v2，其它app都是作为filter的角色，这个pipeline里的所有filter
执行完之后，最终需要调用应用 osapi_compute_app_v2 
[app:osapi_compute_app_v2]
paste.app_factory = nova.api.openstack.compute:APIRouter.factory

9.wsgi app到具体的执行函数
 
 
 


 
########################################################################################################################
-rw-r--r-- 1 root root 17795 Feb 12  2015 api.py
-rw-r--r-- 2 root root 16741 Feb 12  2015 api.pyc
-rw-r--r-- 2 root root 16741 Feb 12  2015 api.pyo
-rw-r--r-- 1 root root  1206 Feb 12  2015 __init__.py
-rw-r--r-- 2 root root   980 Feb 12  2015 __init__.pyc
-rw-r--r-- 2 root root   980 Feb 12  2015 __init__.pyo
-rw-r--r-- 1 root root 36839 Feb 12  2015 manager.py
-rw-r--r-- 2 root root 28697 Feb 12  2015 manager.pyc
-rw-r--r-- 2 root root 28697 Feb 12  2015 manager.pyo
-rw-r--r-- 1 root root 19673 Feb 12  2015 rpcapi.py
-rw-r--r-- 2 root root 19244 Feb 12  2015 rpcapi.pyc
-rw-r--r-- 2 root root 19244 Feb 12  2015 rpcapi.pyo
drwxr-xr-x 2 root root   130 Aug 12  2015 tasks
[root@dzc-o-control01v conductor]# pwd
/usr/lib/python2.7/site-packages/nova/conductor

conductor服务
1. nova-conductor还需要承担原本由nova-compute负责的taskapi任务 ，TaskApi主要包含耗时较长的任务，比如创建虚拟机，迁移虚拟机
2. 一般来说rpcapi.py文件与RPC相关，其它服务将这个模块导入就可以使用它提供的接口远程调用 nova-condutor 提供的服务 
3. nova-conductor注册的RPC server接收到RPC请求后 ，再由manager.py文件中的类 CondutorManager真正的完成数据库访问的操作
4. api模块又对RPC的调用做了一层封装，其它模块需要导入的是api模块而不是rpcapi模块
5. api模块中定义了四个类， LocalAPI，API，LocalComputeTaskAPI 和 ComputeTaskAPI，前两个是访问数据库的api ，后面两个是taskapi 

nova/conductor/__init__.py
from nova.conductor import api as conductor_api


def API(*args, **kwargs):
    use_local = kwargs.pop('use_local', False)
	#判断compute和conductor是否部署在同一节点，如果是则使用LocalAPI 否则使用API
    if oslo_config.cfg.CONF.conductor.use_local or use_local:
        api = conductor_api.LocalAPI
    else:
        api = conductor_api.API
    return api(*args, **kwargs)

def ComputeTaskAPI(*args, **kwargs):
    use_local = kwargs.pop('use_local', False)
    if oslo_config.cfg.CONF.conductor.use_local or use_local:
        api = conductor_api.LocalComputeTaskAPI
    else:
        api = conductor_api.ComputeTaskAPI
    return api(*args, **kwargs)
	
5.3.1 object Model 
object  model的引入主要实现了如下功能:
1. nova-compute 和数据库的在线升级，之前数据库的内容有所变动并升级时，必须对nova-compute也做相应的升级， object model引入后
每个对象都会维持一个版本号，RPC请求里会包含这个版本号
    if nova-compute.version is old   conductor会把数据封装成旧的版本返回给 nova-compute,并将新增字段置为None
2. 对象属性类型的声明
3. 减少写入数据库的量
4. object Model代码位于 nova/objects 目录，里面每一个类都对应数据库的一个表，比如类computeNode对应了数据的compute_node表


	
########################################################################################################################  
5.4 scheduler 调度器
1. ChanceScheduler 随机调度器 
2. FilterScheduler 过滤调度器
3. CachingScheduler

nova-compute对数据的更新时周期性的，而nova-scheduler 在选择最佳主机时则要求数据必须是最新的，因此nova-scheduler 中维护了一份数据
里面包含了从上次数据库更新到现在的主机资源变化的情况
nova/scheduler/host_manager.py 
class HostState(object):
    def update_from_compute_node(self, compute):
        """Update information about a host from a ComputeNode object."""
        if (self.updated and compute.updated_at
                and self.updated > compute.updated_at):
            return
        all_ram_mb = compute.memory_mb

        # Assume virtual size is all consumed by instances if use qcow2 disk.
        free_gb = compute.free_disk_gb
        least_gb = compute.disk_available_least
        if least_gb is not None:
            if least_gb > free_gb:
                # can occur when an instance in database is not on host
                LOG.warning(_LW("Host %(hostname)s has more disk space than "
                                "database expected "
                                "(%(physical)sgb > %(database)sgb)"),
                            {'physical': least_gb, 'database': free_gb,
                             'hostname': compute.hypervisor_hostname})
            free_gb = min(least_gb, free_gb)
        free_disk_mb = free_gb * 1024

        self.disk_mb_used = compute.local_gb_used * 1024

        # NOTE(jogo) free_ram_mb can be negative
        self.free_ram_mb = compute.free_ram_mb
        self.total_usable_ram_mb = all_ram_mb
        self.total_usable_disk_gb = compute.local_gb
        self.free_disk_mb = free_disk_mb
        self.vcpus_total = compute.vcpus
        self.vcpus_used = compute.vcpus_used
        self.updated = compute.updated_at
        self.numa_topology = compute.numa_topology
        self.pci_stats = pci_stats.PciDeviceStats(
            compute.pci_device_pools)

        # All virt drivers report host_ip
        self.host_ip = compute.host_ip
        self.hypervisor_type = compute.hypervisor_type
        self.hypervisor_version = compute.hypervisor_version
        self.hypervisor_hostname = compute.hypervisor_hostname
        self.cpu_info = compute.cpu_info
        if compute.supported_hv_specs:
            self.supported_instances = [spec.to_list() for spec
                                        in compute.supported_hv_specs]
        else:
            self.supported_instances = []

########################################################################################################################
nova-compute 
一. 虚拟机状态
Instance 对象中有三个字段与描述状态相关: power_state ,vm_state ,task_state
1.power_state  使用libvirt获取虚拟机的状态 如 running,shutdown,Nostat等
2.vm_state 虚拟机的稳定状态 active , suspending, suspended 
3.task_state  正在执行任务有 task_state 


二.Resource Tracker 
1. nova-compute为每一个主机创建一个ResourceTracker对象，任务就是更新ComputeNode对象在数据库中对应的表compute_nodes
2. 有两种更新数据库中资源数据的方式：一是使用Resource Tracker 的claim机制 二是使用 周期性任务 Periodic Task 
3. claim机制是在创建之前先预测试一下主机的可用资源是否能够满足新建虚拟机的需要 ，如果满足，则更新数据库，将虚拟机申请的资源
  从可用资源中减掉，如果后来创建失败或者将虚拟机删除时，会再通过claim加回来
    def instance_claim(self, context, instance_ref, limits=None):
        """Indicate that some resources are needed for an upcoming compute
        instance build operation.

        This should be called before the compute node is about to perform
        an instance build operation that will consume additional resources.

        :param context: security context
        :param instance_ref: instance to reserve resources for.
        :type instance_ref: nova.objects.instance.Instance object
        :param limits: Dict of oversubscription limits for memory, disk,
                       and CPUs.
        :returns: A Claim ticket representing the reserved resources.  It can
                  be used to revert the resource usage if an error occurs
                  during the instance build.
        """
        if self.disabled:
            # compute_driver doesn't support resource tracking, just
            # set the 'host' and node fields and continue the build:
            self._set_instance_host_and_node(context, instance_ref)
            return claims.NopClaim()

        # sanity checks:
        if instance_ref.host:
            LOG.warning(_LW("Host field should not be set on the instance "
                            "until resources have been claimed."),
                        instance=instance_ref)

        if instance_ref.node:
            LOG.warning(_LW("Node field should not be set on the instance "
                            "until resources have been claimed."),
                        instance=instance_ref)

        # get memory overhead required to build this instance:
        overhead = self.driver.estimate_instance_overhead(instance_ref)
        LOG.debug("Memory overhead for %(flavor)d MB instance; %(overhead)d "
                  "MB", {'flavor': instance_ref.memory_mb,
                          'overhead': overhead['memory_mb']})

        claim = claims.Claim(context, instance_ref, self, self.compute_node,
                             overhead=overhead, limits=limits)
        #如果claim返回none ，即主机的可用资源满足不了新建虚拟机的需求
		
        # self._set_instance_host_and_node() will save instance_ref to the DB
        # so set instance_ref['numa_topology'] first.  We need to make sure
        # that numa_topology is saved while under COMPUTE_RESOURCE_SEMAPHORE
        # so that the resource audit knows about any cpus we've pinned.
        instance_ref.numa_topology = claim.claimed_numa_topology
        self._set_instance_host_and_node(context, instance_ref)

        # Mark resources in-use and update stats
        self._update_usage_from_instance(context, instance_ref)

        elevated = context.elevated()
        # persist changes to the compute node:
        self._update(elevated)

        return claim  
		
4.使用periodic task 
在类nova.compute.manager.ComputeManager中有个周期性任务 update_available_resource
nova/compute/manager.py 
    @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
    def update_available_resource(self, context):
        """See driver.get_available_resource()

        Periodic process that keeps that the compute host's understanding of
        resource availability and usage in sync with the underlying hypervisor.

        :param context: security context
        """
        new_resource_tracker_dict = {}

        compute_nodes_in_db = self._get_compute_nodes_in_db(context,
                                                            use_slave=True)
        nodenames = set(self.driver.get_available_nodes())
        for nodename in nodenames:
            rt = self._get_resource_tracker(nodename)
            try:
                rt.update_available_resource(context)
            except exception.ComputeHostNotFound:
                # NOTE(comstud): We can get to this case if a node was
                # marked 'deleted' in the DB and then re-added with a
                # different auto-increment id. The cached resource
                # tracker tried to update a deleted record and failed.
                # Don't add this resource tracker to the new dict, so
                # that this will resolve itself on the next run.
                LOG.info(_LI("Compute node '%s' not found in "
                             "update_available_resource."), nodename)
                continue
            except Exception as e:
                LOG.error(_LE("Error updating resources for node "
                              "%(node)s: %(e)s"),
                          {'node': nodename, 'e': e})
            new_resource_tracker_dict[nodename] = rt

        # NOTE(comstud): Replace the RT cache before looping through
        # compute nodes to delete below, as we can end up doing greenthread
        # switches there. Best to have everyone using the newest cache
        # ASAP.
        self._resource_tracker_dict = new_resource_tracker_dict

        # Delete orphan compute node not reported by driver but still in db
        for cn in compute_nodes_in_db:
            if cn.hypervisor_hostname not in nodenames:
                LOG.info(_LI("Deleting orphan compute node %s") % cn.id)
                cn.destroy()
两种更新方式并不冲突，claim机制是在数据库当前数据的基础上去计算并更新，能够保证数据库里的可用资源并及时更新
periodic task是为了保证数据库内信息的准确性，他每次通过hypervisor获取主机的信息，并将这些信息更新到数据库中
########################################################################################################################
cinder 根据要求实现卷的创建
cinder/volume/flows/manager/create_volume.py 
CreateVolumeFromSpecTask 区分了五种创建volume的方式，
1. 创建raw格式的新卷
2. 从快照建立新卷
3. 从已有卷建立新卷
4. 从副本建立新卷
5. 从镜像建立新卷
def execute(self, context, volume, volume_spec):
        volume_spec = dict(volume_spec)
        volume_id = volume_spec.pop('volume_id', None)
        if not volume_id:
            volume_id = volume.id

        # we can't do anything if the driver didn't init
        if not self.driver.initialized:
            driver_name = self.driver.__class__.__name__
            LOG.error(_LE("Unable to create volume. "
                          "Volume driver %s not initialized"), driver_name)
            raise exception.DriverNotInitialized()

        # NOTE(xyang): Populate consistencygroup_id and consistencygroup
        # fields before passing to the driver. This is to support backward
        # compatibility of consistencygroup.
        if volume.group_id:
            volume.consistencygroup_id = volume.group_id
            cg = consistencygroup.ConsistencyGroup()
            cg.from_group(volume.group)
            volume.consistencygroup = cg

        create_type = volume_spec.pop('type', None)
        LOG.info(_LI("Volume %(volume_id)s: being created as %(create_type)s "
                     "with specification: %(volume_spec)s"),
                 {'volume_spec': volume_spec, 'volume_id': volume_id,
                  'create_type': create_type})
        if create_type == 'raw':
            model_update = self._create_raw_volume(volume, **volume_spec)
        elif create_type == 'snap':
            model_update = self._create_from_snapshot(context, volume,
                                                      **volume_spec)
        elif create_type == 'source_vol':
            model_update = self._create_from_source_volume(
                context, volume, **volume_spec)
        elif create_type == 'source_replica':
            model_update = self._create_from_source_replica(
                context, volume, **volume_spec)
        elif create_type == 'image':
            model_update = self._create_from_image(context,
                                                   volume,
                                                   **volume_spec)
        else:
            raise exception.VolumeTypeNotFound(volume_type_id=create_type)

        # Persist any model information provided on creation.
        try:
            if model_update:
                with volume.obj_as_admin():
                    volume.update(model_update)
                    volume.save()
        except exception.CinderException:
            # If somehow the update failed we want to ensure that the
            # failure is logged (but not try rescheduling since the volume at
            # this point has been created).
            LOG.exception(_LE("Failed updating model of volume %(volume_id)s "
                              "with creation provided model %(model)s"),
                          {'volume_id': volume_id, 'model': model_update})
            raise

######################################################################################################################## 
将快照转换为镜像
glance image-create --name "CentOS 6.7" --file 0dd799b8-885b-447d-aa72-c3548d8aa4f8 --disk-format qcow2 --container-format bare --is-public true --progress       

########################################################################################################################
让我们真正开始OVS Agent组件启动源码的解析 
 /neutron/plugins/openvswitch/agent/ovs-neutron-agent.py中的main()
def main():
    cfg.CONF.register_opts(ip_lib.OPTS)
    common_config.init(sys.argv[1:])
    common_config.setup_logging()
    q_utils.log_opt_values(LOG)

    try:
        agent_config = create_agent_config_map(cfg.CONF)
    except ValueError as e:
        LOG.error(_('%s Agent terminated!'), e)
        sys.exit(1)

    is_xen_compute_host = 'rootwrap-xen-dom0' in agent_config['root_helper']
    if is_xen_compute_host:
        # Force ip_lib to always use the root helper to ensure that ip
        # commands target xen dom0 rather than domU.
        cfg.CONF.set_default('ip_lib_force_root', True)
    #主要的工作是实例化一个OVSAgent，并完成OVS Agent的一系列初始化工作 
    agent = OVSNeutronAgent(**agent_config)
	
    signal.signal(signal.SIGTERM, agent._handle_sigterm)

    # Start everything.
    LOG.info(_("Agent initialized successfully, now running... "))
	#一直在循环检查一些状态，发现状态发生变化，执行相应的操作。
    agent.daemon_loop()

class OVSNeutronAgent(n_rpc.RpcCallback,
                      sg_rpc.SecurityGroupAgentRpcCallbackMixin,
                      l2population_rpc.L2populationRpcCallBackTunnelMixin,
                      dvr_rpc.DVRAgentRpcCallbackMixin):
		........	
        # setup_integration_br：安装整合网桥——int_br  		
        self.int_br = ovs_lib.OVSBridge(integ_br, self.root_helper)	
		self.updated_ports = set()
		# setup_rpc完成以下任务：  
        # 设置plugin_rpc，这是用来与neutron-server通信的  
        # 设置state_rpc，用于agent状态信息上报  
        # 设置connection，用于接收neutron-server的消息  
        # 启动状态周期上报  
        self.setup_rpc()
		# 创建物理网络网桥，并用veth与br-int连接起来  
        self.bridge_mappings = bridge_mappings
		
		def setup_integration_br(self):
        '''Setup the integration bridge.

        Create patch ports and remove all existing flows.

        :param bridge_name: the name of the integration bridge.
        :returns: the integration bridge
        '''
		""" 
                        安装integration网桥 
                        创建patch ports，并移除所有现有的流规则 
                        添加基本的流规则 
        """  
        # Ensure the integration bridge is created.
        # ovs_lib.OVSBridge.create() will run
        #   ovs-vsctl -- --may-exist add-br BRIDGE_NAME
        # which does nothing if bridge already exists.
		
		# 通过执行ovs-vsctl中add-br创建int_br  
        self.int_br.create()
        self.int_br.set_secure_mode()
        # del-port删除patch    
        self.int_br.delete_port(cfg.CONF.OVS.int_peer_patch_port)
		#创建patch ports，并移除所有现有的流规则  
        self.int_br.remove_all_flows()
        # switch all traffic using L2 learning
		# 增加actions为normal，优先级为1的流规则  
        # 用L2学习来交换所有通信内容  
        self.int_br.add_flow(priority=1, actions="normal")
        # Add a canary flow to int_br to track OVS restarts
		# 添加canary流规则给int_br来跟踪OVS的重启 优先级0级，actions drop  
        self.int_br.add_flow(table=constants.CANARY_TABLE, priority=0,
                             actions="drop")

							 
    def setup_rpc(self):
        self.agent_id = 'ovs-agent-%s' % cfg.CONF.host
        self.topic = topics.AGENT
		# 设置plugin_rpc，用来与neutron-server通信的  
        self.plugin_rpc = OVSPluginApi(topics.PLUGIN)
		# 设置state_rpc，用于agent状态信息上报 
        self.state_rpc = agent_rpc.PluginReportStateAPI(topics.PLUGIN)
		# 设置connection，并添加consumers，用于接收neutron-server的消息  
        # RPC network init
        self.context = context.get_admin_context_without_session()
        # Handle updates from service
        self.endpoints = [self]
        # Define the listening consumers for the agent
        consumers = [[topics.PORT, topics.UPDATE],
                     [topics.NETWORK, topics.DELETE],
                     [constants.TUNNEL, topics.UPDATE],
                     [topics.SECURITY_GROUP, topics.UPDATE],
                     [topics.DVR, topics.UPDATE]]
        if self.l2_pop:
            consumers.append([topics.L2POPULATION,
                              topics.UPDATE, cfg.CONF.host])
        self.connection = agent_rpc.create_consumers(self.endpoints,
                                                     self.topic,
                                                     consumers,
                                                     start_listening=False)
													 
		# 启动心跳周期上报  											 
        report_interval = cfg.CONF.AGENT.report_interval
        if report_interval:
            heartbeat = loopingcall.FixedIntervalLoopingCall(
                self._report_state)
            heartbeat.start(interval=report_interval)
		'''	
		通过代码的分析，我们可以看到这个函数中分别设置用来与neutron-server通信的plugin_rpc，
		设置了用于agent状态信息上报的state_rpc，设置用于接收neutron-server的消息connection，
		并且启动心跳的周期上报，周期默认为30s。Neutron server端启动了rpc_listeners，对agent发过来的消息进行监听，对于心跳的监听，是如果接收到心跳信号，就会对数据库中的时间戳进行更新，
		如果一直不更新时间戳，当前时间减去更新的时间戳，如果超过默认的agent_down_time=75s，则认为agent处于down的状态。					 
		'''
		
		self.bridge_mappings = bridge_mappings
		def setup_physical_bridges(self, bridge_mappings):
        '''Setup the physical network bridges.

        Creates physical network bridges and links them to the
        integration bridge using veths.

        :param bridge_mappings: map physical network names to bridge names.
        '''
		""" 
                        安装物理网络网桥 
                        创建物理网络网桥，并用veth/patchs与br-int连接起来 
        """  
        self.phys_brs = {}
        self.int_ofports = {}
        self.phys_ofports = {}
        ip_wrapper = ip_lib.IPWrapper(self.root_helper)
        ovs_bridges = ovs_lib.get_bridges(self.root_helper)
        for physical_network, bridge in bridge_mappings.iteritems():
            LOG.info(_("Mapping physical network %(physical_network)s to "
                       "bridge %(bridge)s"),
                     {'physical_network': physical_network,
                      'bridge': bridge})
            # setup physical bridge
            if bridge not in ovs_bridges:
                LOG.error(_("Bridge %(bridge)s for physical network "
                            "%(physical_network)s does not exist. Agent "
                            "terminated!"),
                          {'physical_network': physical_network,
                           'bridge': bridge})
                sys.exit(1)
            br = ovs_lib.OVSBridge(bridge, self.root_helper)
            br.remove_all_flows()
            br.add_flow(priority=1, actions="normal")
            self.phys_brs[physical_network] = br
            # 使用veth/patchs使br-eth1与br-int互联  
            # 删除原有的patchs，创建int-br-eth1和phy-br-eth1   
            # interconnect physical and integration bridges using veth/patchs
            int_if_name = self.get_peer_name(constants.PEER_INTEGRATION_PREFIX,
                                             bridge)
            phys_if_name = self.get_peer_name(constants.PEER_PHYSICAL_PREFIX,
                                              bridge)
            self.int_br.delete_port(int_if_name)
            br.delete_port(phys_if_name)
            if self.use_veth_interconnection:
                if ip_lib.device_exists(int_if_name, self.root_helper):
                    ip_lib.IPDevice(int_if_name,
                                    self.root_helper).link.delete()
                    # Give udev a chance to process its rules here, to avoid
                    # race conditions between commands launched by udev rules
                    # and the subsequent call to ip_wrapper.add_veth
                    utils.execute(['/sbin/udevadm', 'settle', '--timeout=10'])
                int_veth, phys_veth = ip_wrapper.add_veth(int_if_name,
                                                          phys_if_name)
                int_ofport = self.int_br.add_port(int_veth)
                phys_ofport = br.add_port(phys_veth)
            else:
                # Create patch ports without associating them in order to block
                # untranslated traffic before association
                int_ofport = self.int_br.add_patch_port(
                    int_if_name, constants.NONEXISTENT_PEER)
                phys_ofport = br.add_patch_port(
                    phys_if_name, constants.NONEXISTENT_PEER)

            self.int_ofports[physical_network] = int_ofport
            self.phys_ofports[physical_network] = phys_ofport

            # block all untranslated traffic between bridges
            self.int_br.add_flow(priority=2, in_port=int_ofport,
                                 actions="drop")
            br.add_flow(priority=2, in_port=phys_ofport, actions="drop")

            if self.use_veth_interconnection:
                # enable veth to pass traffic
                int_veth.link.set_up()
                phys_veth.link.set_up()
                if self.veth_mtu:
                    # set up mtu size for veth interfaces
                    int_veth.link.set_mtu(self.veth_mtu)
                    phys_veth.link.set_mtu(self.veth_mtu)
            else:
                # associate patch ports to pass traffic
                self.int_br.set_db_attribute('Interface', int_if_name,
                                             'options:peer', phys_if_name)
                br.set_db_attribute('Interface', phys_if_name,
                                    'options:peer', int_if_name)

########################################################################################################################  
我们都知道OpenStack项目由很多子项目构成，如负责计算的Nova；负责存储的Swift,Cinder,Glance；负责网络的Neutron；负责安全的keystone等等。
这么多子项目如何入手，确实是一个问题。

快速浏览一下下载下来的每个子项目的源码目录，发现都有一个setup.py和setup.cfg文件。
看一下setup.py的内容,发现都是一样的。都是利用setuptools.setup来安装自己，并且还都传递了prb=True这个选项。




pbr是什么?
pbr -python 合理编译工具
这是一个一致的管理python setuptools 的工具库
pbr模块读入setup.cfg文件的信息，并且给setuptools 中的setup hook 函数填写默认参数，提供更加有意义的行为，然后使用setup.py来调用，因此setuptools工具包依然是必须的。
注意，我们并不支持setuptools包中的easy_install工具集，当我们依赖于安装需求前提软件，我们推荐使用setup.py install方式或者pip方式安装。

pbr能干什么?
PBR包可以做以下事情
版本：可以基于Git版本和标签信息管理版本号
作者:从git的日志信息产生作者信息
更改日志：从git日志中产生软件包日志
manifest:从git以及其他标准文档中产生一个manifest文件
Sphinx Autodoc:自动产生stub files
需求：生成requirements需求文件
详细描述：使用你的README文件作为包的描述
聪明找包：从你的包的根目录下聪明的找到包
########################################################################################################################
hostname      cip               rip      
dzc-open-jo1 10.16.33.52      10.16.200.16   10.16.201.5 
dzc-open-jo2 10.16.33.180     10.16.200.17   10.16.201.5


管理卡ip 10.16.33.180
eth0:  00:e0:81:e9:46:fc      在DZC-ACI-ASSW16 的interface GigabitEthernet1/0/1
eth1:  00:e0:81:e9:46:fd


Using Device: eth0
Waiting for CDP advertisement:
(default config is to transmit CDP packets every 60 seconds)
Device ID
  value:  DZC-ACI-ASSW16.fang.com
Addresses
  value:  192.168.5.159
Port ID
  value:  GigabitEthernet1/0/1


Enter the interface number (1-8):4
Using Device: eth1
Waiting for CDP advertisement:
(default config is to transmit CDP packets every 60 seconds)
Device ID
  value:  dzc-wan-110.212.fang.com
Addresses
  value:  218.30.110.212
Port ID
  value:  FastEthernet0/1





  192.168.5.106   192.168.1.200/21 gw 192.168.1.1

管理卡ip 10.16.33.52                              
eth0:  84:2b:2b:21:51:14     在DZC-ACI-ASSW20的 GigabitEthernet1/0/10
eth1:  84:2b:2b:21:51:16
eth2:  84:2b:2b:21:51:18
eth3:  84:2b:2b:21:51:1a
##################
[root@localhost network-scripts]# cdpr 
cdpr - Cisco Discovery Protocol Reporter
Version 2.4
Copyright (c) 2002-2010 - MonkeyMental.com

1. eth0 (No description available)
2. nflog (Linux netfilter log (NFLOG) interface)
3. nfqueue (Linux netfilter queue (NFQUEUE) interface)
4. eth1 (No description available)
5. usbmon1 (USB bus number 1)
6. eth2 (No description available)
7. usbmon2 (USB bus number 2)
8. eth3 (No description available)
9. usbmon3 (USB bus number 3)
10. usbmon4 (USB bus number 4)
11. usbmon5 (USB bus number 5)
12. usbmon6 (USB bus number 6)
13. any (Pseudo-device that captures on all interfaces)
14. lo (No description available)
Enter the interface number (1-14):6
Using Device: eth2
Warning opening device (eth2: no IPv4 address assigned)
Waiting for CDP advertisement:
(default config is to transmit CDP packets every 60 seconds)
##################
管理卡ip 10.16.33.52                              
eth0:  84:2b:2b:21:51:14     在DZC-ACI-ASSW20的 GigabitEthernet1/0/10
eth1:  84:2b:2b:21:51:16

Using Device: eth0
Waiting for CDP advertisement:
(default config is to transmit CDP packets every 60 seconds)
Device ID
  value:  DZC-ACI-ASSW20.fang.com
Addresses
  value:  192.168.5.10
Port ID
  value:  GigabitEthernet1/0/10


Using Device: eth1
Warning opening device (eth1: no IPv4 address assigned)
Waiting for CDP advertisement:
(default config is to transmit CDP packets every 60 seconds)
Device ID
  value:  dzc-wan-110.67.fang.com
Addresses
  value:  218.30.110.67
Port ID
  value:  FastEthernet0/10
  
#更新源为aliyun 
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
#安装cdpr 查看各网口连接交换机的哪个端口
yum clean all && yum -y install epel  &&  yum clean all &&  yum -y install cdpr





[root@localhost network-scripts]# ifup eth2
Determining IP information for eth2...
 failed; no link present.  Check cable?
#没插线就是down的
 [root@localhost network-scripts]# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000
    link/ether 84:2b:2b:21:51:14 brd ff:ff:ff:ff:ff:ff
    inet 10.16.200.16/24 brd 10.16.200.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::862b:2bff:fe21:5114/64 scope link 
       valid_lft forever preferred_lft forever
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000
    link/ether 84:2b:2b:21:51:16 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::862b:2bff:fe21:5116/64 scope link 
       valid_lft forever preferred_lft forever
4: eth2: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN qlen 1000
    link/ether 84:2b:2b:21:51:18 brd ff:ff:ff:ff:ff:ff
5: eth3: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN qlen 1000
    link/ether 84:2b:2b:21:51:1a brd ff:ff:ff:ff:ff:ff

########################################################################################################################  
hostname      cip               rip      
dzc-open-jo1 10.16.33.52      10.16.200.16   
dzc-open-jo2 10.16.33.180     10.16.200.17  

管理段         10.16.200.0/24  
数据段         10.16.200.0/24
存储数据段     10.16.200.0/24

#同步housts文件
10.16.200.16  dzc-open-jo1       
10.16.200.17  dzc-open-jo2    


#更新源为aliyun 
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
#安装cdpr 查看各网口连接交换机的哪个端口
yum clean all && yum -y install epel*  &&  yum clean all &&  yum -y install cdpr
#同步时间

#准备工作
service NetworkManager stop

chkconfig NetworkManager off
chkconfig network on 
service firewalld stop
service iptables stop
chkconfig firewalld off
chkconfig iptables off

#同步时间
crontab -e  
# Puppet Name: time sync
*/5 1-23 * * * (/usr/sbin/ntpdate time.soufun.com)
# Puppet Name: time sync 40
40 0 * * * (/usr/sbin/ntpdate time.soufun.com)
# Puppet Name: time sync 30
30 0 * * * (/usr/sbin/ntpdate time.soufun.com)
# Puppet Name: time sync 50
50 0 * * * (/usr/sbin/ntpdate time.soufun.com)
########################################################################################################################
#控制节点：
	安装mysql数据库

mysql root  密码 mysql 
yum install mariadb mariadb-server MySQL-python  -y
mv /var/lib/mysql  /www 
ln -sv /www/mysql /var/lib/mysql

vim /etc/my.cnf  
bind-address = 0.0.0.0  
default-storage-engine = innodb
innodb_file_per_table
collation-server = utf8_general_ci
init-connect = 'SET NAMES utf8'
character-set-server = utf8
###生成数据库表
mysql_install_db  
systemctl enable mariadb.service 
systemctl start mariadb.service 
###安全操作，基本上可以直接Yes到底
mysql_secure_installation  

########################################################################################################################
#所有节点
yum install openstack-utils openstack-selinux   -y 

########################################################################################################################
#消息队列服务，在控制节点上安装，并作为消息队列服务的服务器
yum install rabbitmq-server -y 

#启动服务，并设置为开机启动
systemctl enable rabbitmq-server.service
systemctl start rabbitmq-server.service


rabbitmqctl change_password guest RABBIT_PASS

配置rabbit账户和密码【如果使用了非guest账户，需要在每一个使用rabbit服务的openstack节点上都做相应的修改配置】
rabbitmqctl change_password guest RABBIT_PASS
# The RabbitMQ userid. (string value)
#rabbit_userid=guest
#rabbit_password=guest
检测rabbitMQ服务状态
# rabbitmqctl status | grep rabbit
/etc/rabbitmq/rabbitmq.config
[{rabbit, [{loopback_users, []}]}].
systemctl restart rabbitmq-server.service

########################################################################################################################
mysql root  密码 mysql 
3.	keystone认证服务
#安装 
yum install openstack-keystone python-keystoneclient

openstack-config --set /etc/keystone/keystone.conf database connection mysql://keystone:KEYSTONE_DBPASS@controller/keystone


$ mysql -u root -p
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'KEYSTONE_DBPASS';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%'  IDENTIFIED BY 'KEYSTONE_DBPASS';
exit;


根据配置文件，生成数据库表
# su -s /bin/sh -c "keystone-manage db_sync" keystone
【操作完这一步之后，一定记得去数据库中看一下，是否有表生成，如果没有，接下来的操作会失败。关于这一步失败的原因在最后的QA中有分析】

生成对应的认证文件，并写入配置文件中
随机生成密码命令：
openssl rand -base64 12
openssl rand -hex 12

安装相应的组件
# yum install openstack-keystone python-keystoneclient

编辑/etc/keystone/keystone.conf
[DEFAULT]
...
admin_token = ADMIN_TOKEN   ###修改为上面随机生成的密码


连接数据库
[database]
...
connection = mysql://keystone:KEYSTONE_DBPASS@controller/keystone
配置mysql驱动
[token]
...
provider = keystone.token.providers.uuid.Provider
driver = keystone.token.persistence.backends.sql.Token
配置sql  revoke驱动
[revoke]
...
driver = keystone.contrib.revoke.backends.sql.Revoke

[DEFAULT]
...
verbose = True

对认证文件进行授权操作
# keystone-manage pki_setup --keystone-user keystone --keystone-group
keystone
# chown -R keystone:keystone /var/log/keystone
# chown -R keystone:keystone /etc/keystone/ssl
# chown -R keystone:keystone /etc/keystone/ssl
# chmod -R o-rwx /etc/keystone/ssl

生成数据文件
# su -s /bin/sh -c "keystone-manage db_sync" keystone

设置开机启动
# systemctl enable openstack-keystone.service
# systemctl start openstack-keystone.service

这一步，非必须操作，但是推荐。【这一步作用是用于清空日志，因为日志生成太快了，当然可以使用 cat /dev/null>/var/log/keystone/keystone-tokenflush.log】
# (crontab -l -u keystone 2>&1 | grep -q token_flush) || \
echo '@hourly /usr/bin/keystone-manage token_flush >/var/log/keystone/keystone-tokenflush.log 2>&1' >> /var/spool/cron/keystone

*/30 * * * * /usr/bin/keystone-manage token_flush >/var/log/keystone/keystone-tokenflush.log 2>&1


########################################################################################################################  
3.1 定义认证用户，租户，角色
3.1 定义认证用户，租户，角色
$ export OS_SERVICE_TOKEN=ADMIN_TOKEN  ##ADMIN_TOKEN就是上面随机生成的密码
$ export OS_SERVICE_ENDPOINT=http://controller:35357/v2.0
创建admin租户 
[root@dzc-open-jo2 keystone]# keystone tenant-create --name admin --description "Admin Tenant"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |           Admin Tenant           |
|   enabled   |               True               |
|      id     | 9335806ac8204769b6dc262b056eeaab |
|     name    |              admin               |
+-------------+----------------------------------+

创建admin用户
[root@dzc-open-jo2 keystone]# keystone user-create --name admin --pass ADMIN_PASS --email EMAIL_ADDRESS
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |          EMAIL_ADDRESS           |
| enabled  |               True               |
|    id    | 909e99a59830420f94f0b989e7515f9f |
|   name   |              admin               |
| username |              admin               |
+----------+----------------------------------+

创建admin角色
[root@dzc-open-jo2 keystone]#  keystone role-create --name admin
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|    id    | d1fe8fed319c40e98c1559cc30aa8a53 |
|   name   |              admin               |
+----------+----------------------------------+
将admin角色加入admin租户中
keystone user-role-add --user admin --tenant admin --role admin


Link the admin user, _member_ role, and admin tenant:
keystone tenant-create --name demo --description "Demo Tenant"

keystone user-create --name demo --tenant demo --pass DEMO_PASS --email EMAIL_ADDRESS

keystone tenant-create --name service --description "Service Tenant"

keystone service-create --name keystone --type identity --description "OpenStack Identity"

创建一个对应的认证服务
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |        OpenStack Identity        |
|   enabled   |               True               |
|      id     | 209cda92606b47a9bc1a5c4ba753f275 |
|     name    |             keystone             |
|     type    |             identity             |
+-------------+----------------------------------+
创建对应API接口
keystone endpoint-create \
--service-id=$(keystone service-list | awk '/ identity / {print $2}' |head -1) \
--publicurl=http://controller:5000/v2.0 \
--internalurl=http://controller:5000/v2.0 \
--adminurl=http://controller:35357/v2.0 \
--region regionOne

+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
|   adminurl  |   http://controller:35357/v2.0   |
|      id     | 5404978730274b4694d5937d4a47bc49 |
| internalurl |   http://controller:5000/v2.0    |
|  publicurl  |   http://controller:5000/v2.0    |
|    region   |            regionOne             |
|  service_id | 209cda92606b47a9bc1a5c4ba753f275 |
+-------------+----------------------------------+

验证操作
第一步：取消认证
unset OS_SERVICE_TOKEN OS_SERVICE_ENDPOINT


$ keystone --os-tenant-name admin --os-username admin –os-password ADMIN_PASS \
--os-auth-url http://controller:35357/v2.0 token-get

$ keystone --os-tenant-name admin --os-username admin –os-password ADMIN_PASS \
--os-auth-url http://controller:35357/v2.0 tenant-list

$ keystone --os-tenant-name admin --os-username admin –os-password ADMIN_PASS \
--os-auth-url http://controller:35357/v2.0 user-list

$ keystone --os-tenant-name admin --os-username admin –os-password ADMIN_PASS \
--os-auth-url http://controller:35357/v2.0 role-list

$ keystone --os-tenant-name demo --os-username demo –os-password DEMO_PASS \
--os-auth-url http://controller:35357/v2.0 token-get

keystone --os-tenant-name demo --os-username demo –os-password DEMO_PASS \
--os-auth-url http://controller:35357/v2.0 user-list

为了便于以后操作，切换，请创建admin-openrc.sh，作用是切换到admin环境。使用操作：source admin-openrc.sh
export OS_USERNAME=admin
export OS_PASSWORD=ADMIN_PASS
export OS_TENANT_NAME=admin
export OS_AUTH_URL=http://controller:35357/v2.0

创建demo环境，创建demo-openrc.sh,作用是切换到demo环境。使用操作：source demo-openrc.sh
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=DEMO_PASS
export OS_AUTH_URL=http://controller:5000/v2.0

查看现有用户命令：
Keystone user-list
keystone user-role-list --user admin --tenant admin

########################################################################################################################
4.	glance镜像服务
创建数据库并授权
mysql -uroot -pmysql
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'GLANCE_DBPASS';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'GLANCE_DBPASS';

创建keystone用户
[root@dzc-open-jo2 ~]# keystone user-create --name=glance --pass=GLANCE_PASS
WARNING: Bypassing authentication using a token & endpoint (authentication credentials are being ignored).
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |                                  |
| enabled  |               True               |
|    id    | 22fbd2b8ffd240a49c192af442047e0c |
|   name   |              glance              |
| username |              glance              |
+----------+----------------------------------+

[root@dzc-open-jo2 ~]# keystone service-create --name glance --type image --description "OpenStack Image Service"
WARNING: Bypassing authentication using a token & endpoint (authentication credentials are being ignored).
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |     OpenStack Image Service      |
|   enabled   |               True               |
|      id     | ad21d0c9fca04627bf7903d0784f8c11 |
|     name    |              glance              |
|     type    |              image               |
+-------------+----------------------------------+


[root@dzc-open-jo2 ~]# keystone endpoint-create \
> --service-id $(keystone service-list | awk '/ image / {print $2}') \
> --publicurl http://controller:9292 \
> --internalurl http://controller:9292 \
> --adminurl http://controller:9292 \
> --region regionOne

WARNING: Bypassing authentication using a token & endpoint (authentication credentials are being ignored).
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
|   adminurl  |      http://controller:9292      |
|      id     | b5f103781cc94c40bfba92c7a402dcea |
| internalurl |      http://controller:9292      |
|  publicurl  |      http://controller:9292      |
|    region   |            regionOne             |
|  service_id | ad21d0c9fca04627bf7903d0784f8c11 |
+-------------+----------------------------------+

4.2 安装
 yum install openstack-glance python-glanceclient -y 
 
 4.3	配置数据库
 
 Edit the /etc/glance/glance-api.conf

[database]
...
connection = mysql://glance:GLANCE_DBPASS@controller/glance

[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = glance
admin_password = GLANCE_PASS
[paste_deploy]
...
flavor = keystone

Comment out any auth_host, auth_port, and auth_protocol
options because the identity_uri option replaces them.

[glance_store]
...
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

[DEFAULT]
...
notification_driver = noop

[DEFAULT]
...
verbose = True

Edit the /etc/glance/glance-registry.conf
[database]
...
connection = mysql://glance:GLANCE_DBPASS@controller/glance

[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = glance
admin_password = GLANCE_PASS
[paste_deploy]
...
flavor = keystone
[DEFAULT]
...
notification_driver = noop

[DEFAULT]
...
verbose = True
4.4 生成数据库表文件
# su -s /bin/sh -c "glance-manage db_sync" glance

# systemctl enable openstack-glance-api.service openstack-glance-registry.service
# systemctl start openstack-glance-api.service openstack-glance-registry.service

4.8 验证glance服务
4.8.1 下载glance镜像

 
 
 
[glance_store]
# List of which store classes and store class locations are
# currently known to glance at startup.
# Existing but disabled stores:
#      glance.store.rbd.Store,
#      glance.store.s3.Store,
#      glance.store.swift.Store,
#      glance.store.sheepdog.Store,
#      glance.store.cinder.Store,
#      glance.store.gridfs.Store,
#      glance.store.vmware_datastore.Store,.
#配置存储可以从 http 过来
stores=glance.store.filesystem.Store,
         glance.store.http.Store
default_store = file
 
 
 
[root@dzc-open-jo2 www]#  glance image-create --name "cirros-0.3.3-x86_64" --file /tmp/cirros-0.3.3-x86_64-disk.img --disk-format qcow2 --container-format bare --is-public True --progress
[=============================>] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 133eae9fb1c98f45894a4e60d8736619     |
| container_format | bare                                 |
| created_at       | 2017-03-02T10:08:17                  |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | 39ec9eed-c1c7-4ccc-ae7e-1dadb1d15bfc |
| is_public        | True                                 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | cirros-0.3.3-x86_64                  |
| owner            | 9335806ac8204769b6dc262b056eeaab     |
| protected        | False                                |
| size             | 13200896                             |
| status           | active                               |
| updated_at       | 2017-03-02T10:08:18                  |
| virtual_size     | None                                 |
+------------------+--------------------------------------+
 
[root@dzc-open-jo2 www]# glance image-list
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
| ID                                   | Name                | Disk Format | Container Format | Size     | Status |
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
| 39ec9eed-c1c7-4ccc-ae7e-1dadb1d15bfc | cirros-0.3.3-x86_64 | qcow2       | bare             | 13200896 | active |
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
 
########################################################################################################################  
5	Nova计算服务
5.1  Nova控制节点
创建Nova数据库
$ mysql -u root –p
CREATE DATABASE nova;
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \
IDENTIFIED BY 'NOVA_DBPASS';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \
IDENTIFIED BY 'NOVA_DBPASS';

切换到admin环境
$ source admin-openrc.sh
$ keystone user-create --name nova --pass NOVA_PASS
$ keystone user-role-add --user nova --tenant service --role admin
$ keystone service-create --name nova --type compute \
--description "OpenStack Compute"
$ keystone endpoint-create \
--service-id $(keystone service-list | awk '/ compute / {print $2}') \
--publicurl http://controller:8774/v2/%\(tenant_id\)s \
--internalurl http://controller:8774/v2/%\(tenant_id\)s \
--adminurl http://controller:8774/v2/%\(tenant_id\)s \
--region regionOne


5.1.1 安装openstack nova软件包
# yum install openstack-nova-api openstack-nova-cert openstack-nova-conductor \
  openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler \
  python-novaclient

5.1.2 配置数据库配置文件
/etc/nova/nova.conf
[database]
...
connection = mysql://nova:NOVA_DBPASS@controller/nova

[DEFAULT]
...
rpc_backend = rabbit
rabbit_host = controller
rabbit_password = RABBIT_PASS

[DEFAULT]
...
auth_strategy = keystone
[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = nova
admin_password = NOVA_PASS

[DEFAULT]
...
my_ip = 10.0.0.11

[DEFAULT]
...
vncserver_listen = 10.0.0.11
vncserver_proxyclient_address = 10.0.0.11
[glance]
...
host = controller
[DEFAULT]
...
verbose = True
生成数据库文件
# su -s /bin/sh -c "nova-manage db sync" nova

# systemctl enable openstack-nova-api.service openstack-nova-cert.service \
openstack-nova-consoleauth.service openstack-nova-scheduler.service \
openstack-nova-conductor.service openstack-nova-novncproxy.service
# systemctl start openstack-nova-api.service openstack-nova-cert.service \
openstack-nova-consoleauth.service openstack-nova-scheduler.service \
openstack-nova-conductor.service openstack-nova-novncproxy.service

5.1.11 验证nova服务
$ nova image-list

5.2 Nova计算节点【安装配置计算节点】
5.2.1 安装nova软件
# yum install openstack-nova-compute sysfsutils
【如果多安装了其他服务，只要不启动服务，不会影响结果】

Edit the /etc/nova/nova.conf
[DEFAULT]
...
rpc_backend = rabbit
rabbit_host = controller
rabbit_password = RABBIT_PASS

[DEFAULT]
...
auth_strategy = keystone
[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = nova
admin_password = NOVA_PASS

[DEFAULT]
...
my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS
Replace MANAGEMENT_INTERFACE_IP_ADDRESS with the IP address of the
management network interface on your compute node, typically 10.0.0.31 for the
first node in the example architecture.

[DEFAULT]
...
vnc_enabled = True
vncserver_listen = 0.0.0.0
vncserver_proxyclient_address = MANAGEMENT_INTERFACE_IP_ADDRESS
novncproxy_base_url = http://controller:6080/vnc_auto.html

[glance]
...
host = controller

[DEFAULT]
...
verbose = True

$ egrep -c '(vmx|svm)' /proc/cpuinfo

启动相应服务
# systemctl enable libvirtd.service openstack-nova-compute.service
# systemctl start libvirtd.service openstack-nova-compute.service

$ source admin-openrc.sh
$ nova service-list
$ nova image-list


5.2.5.1 查看计算节点是否支持虚拟化技术
$ egrep -c '(vmx|svm)' /proc/cpuinfo
如果返回为0，表示不支持，修虚拟化技术修改为qemu,如果非0，表示支持虚拟化技术。则修改为kvm，当然，如果使用的是其他虚拟化机器，请对应修改。
# openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu



####
yum  install python-eventlet-0.15.2-1.el7 --enablerepo=juno
########################################################################################################################
systemctl restart openstack-keystone.service

systemctl restart httpd.service memcached.service

systemctl restart openstack-nova-api.service openstack-nova-cert.service \
openstack-nova-consoleauth.service openstack-nova-scheduler.service \
openstack-nova-conductor.service openstack-nova-novncproxy.service 




systemctl restart openstack-cinder-api.service        openstack-cinder-scheduler.service  \



systemctl  restart neutron-dhcp-agent.service         neutron-metadata-agent.service    
systemctl  restart  neutron-l3-agent.service                neutron-server.service  neutron-openvswitch-agent.service 
 systemctl  restart neutron-openvswitch-agent.service    openvswitch.service

 
systemctl  restart  mariadb.service  
systemctl   restart rabbitmq-server.service
 

systemctl restart openstack-nova-api.service openstack-nova-cert.service \
openstack-nova-consoleauth.service openstack-nova-scheduler.service \
openstack-nova-conductor.service openstack-nova-novncproxy.service openstack-nova-compute.service  libvirtd.service

glance image-create --name "centos6.5 fang" --file /www/images/1ca0181d-24f4-4c13-938e-e05ade3d1ada --disk-format qcow2 --container-format bare --is-public True --progress
########################################################################################################################  
systemctl restart openstack-nova-compute.service   openstack-cinder-volume.service \
neutron-openvswitch-agent.service    openvswitch.service   libvirtd.service ceph.service 
 systemctl  enable openstack-nova-compute.service   openstack-cinder-volume.service \
neutron-openvswitch-agent.service    openvswitch.service   libvirtd.service
########################################################################################################################

[root@dzc-open-jo2 nova]# ip netns ls
qrouter-e07ff153-e31a-428c-8fae-f3968ea90991
qdhcp-643bc60b-5e9e-4ae7-82c8-97c6ecb58f16
qdhcp-2fefef65-2067-4efc-bf15-fcad0cbeb8c4
You have new mail in /var/spool/mail/root
[root@dzc-open-jo2 nova]# ip netns  exec qrouter-e07ff153-e31a-428c-8fae-f3968ea90991 /bin/bash
[root@dzc-open-jo2 nova]# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
50: qr-dccf32a7-c6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN qlen 1000
    link/ether fa:16:3e:5f:c1:84 brd ff:ff:ff:ff:ff:ff
    inet 10.16.202.1/24 brd 10.16.202.255 scope global qr-dccf32a7-c6
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe5f:c184/64 scope link 
       valid_lft forever preferred_lft forever
51: qr-91de0985-1b: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN qlen 1000
    link/ether fa:16:3e:e8:4a:c5 brd ff:ff:ff:ff:ff:ff
    inet 10.16.201.1/24 brd 10.16.201.255 scope global qr-91de0985-1b
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fee8:4ac5/64 scope link 
       valid_lft forever preferred_lft forever
[root@dzc-open-jo2 nova]# ping 10.16.201.11
PING 10.16.201.11 (10.16.201.11) 56(84) bytes of data.
64 bytes from 10.16.201.11: icmp_seq=1 ttl=64 time=0.715 ms
64 bytes from 10.16.201.11: icmp_seq=2 ttl=64 time=0.381 ms
64 bytes from 10.16.201.11: icmp_seq=3 ttl=64 time=0.324 ms
^C
--- 10.16.201.11 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2000ms
rtt min/avg/max/mdev = 0.324/0.473/0.715/0.173 ms


###########
[root@dzc-open-jo1 network-scripts]# lsmod |grep kvm
kvm                   554609  0 
irqbypass              13503  1 kvm

[root@dzc-open-jo1 network-scripts]# dmesg |grep -i kvm
[ 1346.032162] kvm: disabled by bios
[ 1346.038005] systemd-udevd[521]: Reading rules file: /usr/lib/udev/rules.d/80-kvm.rules
[ 1346.038197] systemd-udevd[521]: Reading rules file: /usr/lib/udev/rules.d/81-kvm-rhel.rules
[ 1346.039467] systemd-udevd[3500]: no db file to read /run/udev/data/+module:kvm_intel: No such file or directory
[ 1346.044171] systemd-udevd[3500]: no db file to read /run/udev/data/+module:kvm_intel: No such file or directory







vif_plugging_is_fatal = False
vif_plugging_timeout = 0



36.110.142.121   255.255.255.128  gw  36.110.142.1    10.16.33.52 
36.110.142.122   255.255.255.128  gw  36.110.142.1    10.16.33.180
########################################################################################################################  
监控rabbitmq
rabbitmq-plugins enable rabbitmq_management

rabbitmqctl add_user  user_admin  passwd_admin

rabbitmqctl set_user_tags user_admin administrator

rabbitmqctl set_permissions -p / user_admin ".*" ".*" ".*"
systemctl  restart rabbitmq-server.service

########################################################################################################################
/etc/openstack-dashboard
systemctl restart httpd.service
locall.setting 
SESSION_TIMEOUT=0
########################################################################################################################  

这样整个链路就打通了：

1. instance 通过 neutron network（Project 网络）将 metadata 请求发送到 neutron-ns-metadata-proxy。

2. neutron-ns-metadata-proxy 通过 unix domain socket 将请求发给 neutron-metadata-agent。

3. neutron-metadata-agent 通过内部管理网络将请求发送给 nova-api-metadata。

可能大家对于 neutron-ns-metadata-proxy 还会有些疑虑：既然 dhcp-agent 和 l3-agent 都可以创建和管理 neutron-ns-metadata-proxy，使用的时候该如何选择呢？

简单的说：各有各的使用场景，并且两种方案可以共存。大家不用担心，后面我们会通过例子详细讨论。

Metadata Service 的架构已经讨论清楚了，下一节将通过实践加深理解。
################################################################################################################################################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
################################################################################################################################################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
################################################################################################################################################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
################################################################################################################################################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
################################################################################################################################################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
################################################################################################################################################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
################################################################################################################################################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
################################################################################################################################################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################
########################################################################################################################  
########################################################################################################################


